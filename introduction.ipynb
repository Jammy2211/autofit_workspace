{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyAutoFit\n",
    "=========\n",
    "\n",
    "**PyAutoFit** is a probabilistic programming language which makes is simple to compose, customize and fit complex\n",
    "models to data.\n",
    "\n",
    "To illustrate the **PyAutoFit** API, we'll use an illustrative toy model of fitting a one-dimensional Gaussian to\n",
    "noisy 1D data.\n",
    "\n",
    "Lets first import autofit and the other libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The 5 lines below set up the notebook working directory and can be ignored for the overview.\n",
    "\n",
    "%matplotlib inline\n",
    "from pyprojroot import here\n",
    "workspace_path = str(here())\n",
    "%cd $workspace_path\n",
    "print(f\"Working Directory has been set to {workspace_path}\")\n",
    "\n",
    "import autofit as af\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Model\n",
    "------------\n",
    "\n",
    "We now load and plot the ``data`` we'll fit:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
    "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
    "noise_map = af.util.numpy_array_from_json(\n",
    "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
    ")\n",
    "\n",
    "xvalues = np.arange(data.shape[0])\n",
    "plt.plot(xvalues, data, color=\"k\")\n",
    "plt.title(\"1D Gaussian dataset.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile Intensity\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our model, a 1D Gaussian, by writing a Python class using the format below:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Gaussian:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        centre=0.0,  # <- PyAutoFit recognises these\n",
    "        intensity=0.1,  # <- constructor arguments are\n",
    "        sigma=0.01,  # <- the Gaussian's parameters.\n",
    "    ):\n",
    "        self.centre = centre\n",
    "        self.intensity = intensity\n",
    "        self.sigma = sigma\n",
    "\n",
    "    \"\"\"\n",
    "    An instance of the Gaussian class will be available during model fitting.\n",
    "\n",
    "    This method will be used to fit the model to data and compute a likelihood.\n",
    "    \"\"\"\n",
    "\n",
    "    def profile_from_xvalues(self, xvalues):\n",
    "\n",
    "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
    "\n",
    "        return np.multiply(\n",
    "            np.divide(self.intensity, self.sigma * np.sqrt(2.0 * np.pi)),\n",
    "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
    "        )\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By writing the `Gaussian` class in this way, PyAutoFit treats it as a *model-component* which can be fitted to data\n",
    "via a `NonLinearSearch`. \n",
    "\n",
    "PyAutoFit calls this a `PriorModel` and it can generate instances of the class from custom *priors*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = af.PriorModel(Gaussian)\n",
    "\n",
    "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
    "model.intensity = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
    "model.sigma = af.GaussianPrior(mean=10.0, sigma=5.0)\n",
    "\n",
    "print(\"PriorModel `Gaussian` object: \\n\")\n",
    "print(model)\n",
    "\n",
    "print(\"\\n PriorModel `Gaussian` Parameters: \\n\")\n",
    "print(model.centre)\n",
    "print(model.intensity)\n",
    "print(model.sigma)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using this `PriorModel` we can create an `instance` of the model, by mapping a list of unit values to physical values\n",
    "via the prior on each parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "instance = model.instance_from_unit_vector(unit_vector=[0.5, 0.5, 0.5])\n",
    "\n",
    "print(\"Model Instance: \\n\")\n",
    "print(instance)\n",
    "\n",
    "print(\"\\nInstance Parameters: \\n\")\n",
    "print(\"x = \", instance.centre)\n",
    "print(\"intensity = \", instance.intensity)\n",
    "print(\"sigma = \", instance.sigma)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *model* fit now only requires that a **PyAutoFit** ``Analysis`` class is written, which combines the data, model and\n",
    "likelihood function and defines how the *model-fit* is performed using a `NonLinearSearch`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Analysis(af.Analysis):\n",
    "\n",
    "    def __init__(self, data, noise_map):\n",
    "\n",
    "        self.data = data\n",
    "        self.noise_map = noise_map\n",
    "\n",
    "    def log_likelihood_function(self, instance):\n",
    "\n",
    "        \"\"\"\n",
    "        The 'instance' that comes into this method is an instance of the Gaussian class\n",
    "        above, with the parameters set to values chosen by the non-linear search.\n",
    "\n",
    "        (The lines below are commented out to text printing later in the Notebook. They\n",
    "        illustrate how model parameters are passed to the `log_likelihood_function`\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"Gaussian Instance:\")\n",
    "        # print(\"Centre = \", instance.centre)\n",
    "        # print(\"Intensity = \", instance.intensity)\n",
    "        # print(\"Sigma = \", instance.sigma)\n",
    "\n",
    "        \"\"\"\n",
    "        We fit the data with the Gaussian instance, using its\n",
    "        \"profile_from_xvalues\" function to create the model data.\n",
    "        \"\"\"\n",
    "\n",
    "        xvalues = np.arange(self.data.shape[0])\n",
    "\n",
    "        model_data = instance.profile_from_xvalues(xvalues=xvalues)\n",
    "        residual_map = self.data - model_data\n",
    "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
    "        log_likelihood = -0.5 * sum(chi_squared_map)\n",
    "\n",
    "        return log_likelihood\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Analysis`` class provides a model specific interface between **PyAutoFit** and the modeling software, allowing\n",
    "it to handle the 'heavy lifting' that comes with writing *model-fitting* software. This includes interfacing with the\n",
    "non-linear search, model-specific visualization during and outputting results in a database.\n",
    "\n",
    "Performing a fit with a non-linear search, for example `dynesty` (https://github.com/joshspeagle/dynesty),\n",
    "is performed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = af.PriorModel(Gaussian)\n",
    "\n",
    "analysis = Analysis(data=data, noise_map=noise_map)\n",
    "\n",
    "print(\"Non-linear search running, this will take a minute or so.... \\n\")\n",
    "\n",
    "dynesty = af.DynestyStatic(name=\"example_simple\", n_live_points=50)\n",
    "result = dynesty.fit(model=model, analysis=analysis)\n",
    "\n",
    "print(\"Non-linear search complete!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Result` object contains information on the model-fit, for example the maximum log likelihood\n",
    "model and marginalized probability density functions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(result.max_log_likelihood_instance)\n",
    "\n",
    "print(\"\\n Model-fit Max Log-likelihood Parameter Estimates: \\n\")\n",
    "print(\"Centre = \", result.max_log_likelihood_instance.centre)\n",
    "print(\"Intensity = \", result.max_log_likelihood_instance.intensity)\n",
    "print(\"Sigma = \", result.max_log_likelihood_instance.sigma)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot this over our data to see the model indeed gives a good fit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_data = result.max_log_likelihood_instance.profile_from_xvalues(\n",
    "    xvalues=np.arange(data.shape[0])\n",
    ")\n",
    "plt.errorbar(\n",
    "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
    ")\n",
    "plt.plot(xvalues, model_data, color=\"r\")\n",
    "plt.title(\"Dynesty model fit to 1D Gaussian dataset.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile intensity\")\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Complex Model\n",
    "-------------\n",
    "\n",
    "It is straight forward to compose and customize more complex models with **PyAutoFit**.  To demonstrate this, we'll\n",
    "fit another 1D dataset that contains a Gaussian's and a symmetric Exponential profile."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1__exponential_x1\")\n",
    "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
    "noise_map = af.util.numpy_array_from_json(\n",
    "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
    ")\n",
    "\n",
    "xvalues = np.arange(data.shape[0])\n",
    "plt.plot(xvalues, data, color=\"k\")\n",
    "plt.title(\"1D Gaussian dataset.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile Intensity\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To fit the Exponential, we define a second Python class which will act as a *model-component* like the Gaussian did\n",
    "previously."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Exponential:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        centre=0.0,     # <- PyAutoFit recognises these constructor arguments are the model\n",
    "        intensity=0.1,  # <- parameters of the Exponential.\n",
    "        rate=0.01,\n",
    "    ):\n",
    "        \"\"\"Represents a 1D Exponential profile symmetric about a centre, which may be treated as a model-component\n",
    "        of PyAutoFit the parameters of which are fitted for by a non-linear search.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        centre : float\n",
    "            The x coordinate of the profile centre.\n",
    "        intensity : float\n",
    "            Overall intensity normalisation of the `Gaussian` profile.\n",
    "        ratw : float\n",
    "            The decay rate controlling has fast the Exponential declines.\n",
    "        \"\"\"\n",
    "        self.centre = centre\n",
    "        self.intensity = intensity\n",
    "        self.rate = rate\n",
    "\n",
    "    def profile_from_xvalues(self, xvalues):\n",
    "        \"\"\"\n",
    "        Calculate the intensity of the profile on a line of Cartesian x coordinates.\n",
    "\n",
    "        The input xvalues are translated to a coordinate system centred on the Exponential, using its centre.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        values : np.ndarray\n",
    "            The x coordinates in the original reference frame of the grid.\n",
    "        \"\"\"\n",
    "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
    "        return self.intensity * np.multiply(\n",
    "            self.rate, np.exp(-1.0 * self.rate * abs(transformed_xvalues))\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compose a model from multiple *model-components* using the `CollectionPriorModel` object."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = af.CollectionPriorModel(gaussian=Gaussian, exponential=Exponential)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An `instance` of the model can be created like we did for a `PriorModel` above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "instance = model.instance_from_vector(vector=[0.1, 0.2, 0.3, 0.4, 0.5, 0.01])\n",
    "\n",
    "print(\"Instance Parameters \\n\")\n",
    "print(\"x (Gaussian) = \", instance.gaussian.centre)\n",
    "print(\"intensity (Gaussian) = \", instance.gaussian.intensity)\n",
    "print(\"sigma (Gaussian) = \", instance.gaussian.sigma)\n",
    "print(\"x (Exponential) = \", instance.exponential.centre)\n",
    "print(\"intensity (Exponential) = \", instance.exponential.intensity)\n",
    "print(\"sigma (Exponential) = \", instance.exponential.rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are many options for customizing a model, below we require that:\n",
    "\n",
    "- The `Gaussian` and `Exponential` share the same centre (reducing the dimensionality of parameter space by 1).\n",
    "- That the `Gaussian`'s `sigma` value is 10.0 (again reducing the dimensionality by 1).\n",
    "- That the `rate` parameter of the `Exponential` is above 0.0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.gaussian.centre = model.exponential.centre\n",
    "model.gaussian.sigma = 10.0\n",
    "model.exponential.add_assertion(model.exponential.rate > 0.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `Analysis` class above was written assuming the input `instance` contained only a single `Gaussian` profile. The\n",
    "`CollectionPriorModel` contains multiple profiles, thus we must update the `Analysis` class to reflect this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Analysis(af.Analysis):\n",
    "\n",
    "    def __init__(self, data, noise_map):\n",
    "\n",
    "        self.data = data\n",
    "        self.noise_map = noise_map\n",
    "\n",
    "    def log_likelihood_function(self, instance):\n",
    "\n",
    "        xvalues = np.arange(self.data.shape[0])\n",
    "\n",
    "        \"\"\"\n",
    "        The instance, which now contains the `gaussian` and `exponential`, can be iterated over\n",
    "        and summed so our model-data is the combination of the two.\n",
    "        \"\"\"\n",
    "\n",
    "        model_data = sum(\n",
    "            [profile.profile_from_xvalues(xvalues=xvalues) for profile in instance]\n",
    "        )\n",
    "\n",
    "        residual_map = self.data - model_data\n",
    "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
    "        log_likelihood = -0.5 * sum(chi_squared_map)\n",
    "\n",
    "        return log_likelihood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now fit our more complex model as we did previously. Lets use the MCMC algorithm\n",
    "`Emcee` (https://github.com/dfm/emcee) this time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analysis = Analysis(data=data, noise_map=noise_map)\n",
    "\n",
    "print(\"Non-linear search running, this will take a minute or so.... \\n\")\n",
    "\n",
    "emcee = af.Emcee(name=\"example_complex_2\", nwalkers=100, nsteps=75)\n",
    "result = emcee.fit(model=model, analysis=analysis)\n",
    "\n",
    "print(\"Non-linear search complete!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because our fit used a `CollectionPriorModel` (as opposed to a `PriorModel`) the `Result` instance returns the\n",
    "results for both the `gaussian` and `exponential`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_log_likelihood_instance = result.max_log_likelihood_instance\n",
    "\n",
    "print(\"Max Log Likelihood `Gaussian` Instance:\")\n",
    "print(\"Centre = \", max_log_likelihood_instance.gaussian.centre)\n",
    "print(\"Intensity = \", max_log_likelihood_instance.gaussian.intensity)\n",
    "print(\"Sigma = \", max_log_likelihood_instance.gaussian.sigma, \"\\n\")\n",
    "print(\"Max Log Likelihood Exponential Instance:\")\n",
    "print(\"Centre = \", max_log_likelihood_instance.exponential.centre)\n",
    "print(\"Intensity = \", max_log_likelihood_instance.exponential.intensity)\n",
    "print(\"Sigma = \", max_log_likelihood_instance.exponential.rate, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can again use this to plot the model fit."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_gaussian = max_log_likelihood_instance.gaussian.profile_from_xvalues(\n",
    "    xvalues=np.arange(data.shape[0])\n",
    ")\n",
    "model_exponential = max_log_likelihood_instance.exponential.profile_from_xvalues(\n",
    "    xvalues=np.arange(data.shape[0])\n",
    ")\n",
    "model_data = model_gaussian + model_exponential\n",
    "\n",
    "plt.errorbar(\n",
    "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
    ")\n",
    "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
    "plt.plot(range(data.shape[0]), model_gaussian, \"--\")\n",
    "plt.plot(range(data.shape[0]), model_exponential, \"--\")\n",
    "plt.title(\"Emcee model fit to 1D Gaussian + Exponential dataset.\")\n",
    "plt.xlabel(\"x values of profile\")\n",
    "plt.ylabel(\"Profile intensity\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you've  completed the **PyAutoFit** introduction!\n",
    "\n",
    "So, What Next?\n",
    "--------------\n",
    "\n",
    "This notebook has given you an overview of the **PyAutoFit** API. This API acts as the foundation for using many of\n",
    "**PyAutoFit**'s advanced features, such as:\n",
    "\n",
    "- [Database tools for loading and manipulating large libraries of results in a Jupyter notebook.](https://pyautofit.readthedocs.io/en/latest/overview/database.html)\n",
    "- [Performing massively paralle grid searches of non-linear searches.](https://pyautofit.readthedocs.io/en/latest/features/search_grid_search.html)\n",
    "- [Chaining searches back-to-back to break a complex model-fitting procedure into sequence of simpler model-fits](https://pyautofit.readthedocs.io/en/latest/features/search_chaining.html)\n",
    "- [Sensitivity mapping over a model or dataset to determine when more complex model features become detectable](https://pyautofit.readthedocs.io/en/latest/features/sensitivity_mapping.html)]\n",
    "\n",
    "We recommend you next start the **HowToFit** Jupyter notebook tutorials, which provide a detailed description of the\n",
    "**PyAutoFit** API, give more details on how to compose and fit models and a more detailed description of the\n",
    "`Result` object and database analysis tools.\n",
    "\n",
    "If you wish to add your own *model-component* to your `autofit_workspace` to perform your own model-fitting task,\n",
    "checkout the script `autofit_workspace/notebooks/overview/new_model_component/new_model_compnent.ipynb`, which explains\n",
    "how to set up the **PyAutoFit** configuration files associated with your model.\n",
    "\n",
    "You can install **PyAutoFit** on your system and clone the `autofit_workspace` and `howtofit` tutorials\n",
    "following the instructions on our readthedocs:\n",
    "\n",
    " https://pyautofit.readthedocs.io/en/latest/installation/overview.html\n",
    "\n",
    "Alternatively, you can begin the tutorials on this Binder by going to the folder `howtofit/chapter_1_introduction`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "pycharm-be979b41",
   "language": "python",
   "display_name": "PyCharm (Results)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}