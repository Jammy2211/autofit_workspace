{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cookbook: Samples\n",
        "=================\n",
        "\n",
        "A non-linear search samples parameter space to find the global maximum log likelihood solution.\n",
        "\n",
        "The `Samples` object contains the history of the non-linear search, including the model parameters and log likelihood\n",
        "of every accepted sample.\n",
        "\n",
        "This cookbook provides an overview of using the samples.\n",
        "\n",
        "__Contents__\n",
        "\n",
        " - Model Fit: Perform a simple model-fit to create a ``Samples`` object.\n",
        " - Samples: The `Samples` object`, containing all non-linear samples (e.g. parameters, log likelihoods, etc.).\n",
        " - Parameters: Accessing the parameters of the model from the samples.\n",
        " - Figures of Merit: The log likelihood, log prior, log posterior and weight_list of every accepted sample.\n",
        " - Instances: Returning instances of the model corresponding to a particular sample (e.g. the maximum log likelihood).\n",
        " - Posterior / PDF: The median PDF model instance and PDF vectors of all model parameters via 1D marginalization.\n",
        " - Errors: The errors on every parameter estimated from the PDF, computed via marginalized 1D PDFs at an input sigma.\n",
        " - Samples Summary: A summary of the samples of the non-linear search (e.g. the maximum log likelihood model) which can\n",
        "   be faster to load than the full set of samples.\n",
        " - Sample Instance: The model instance of any accepted sample.\n",
        " - Search Plots: Plots of the non-linear search, for example a corner plot or 1D PDF of every parameter.\n",
        " - Maximum Likelihood: The maximum log likelihood model value.\n",
        " - Bayesian Evidence: The log evidence estimated via a nested sampling algorithm.\n",
        " - Collection: Results created from models defined via a `Collection` object.\n",
        " - Lists: Extracting results as Python lists instead of instances.\n",
        " - Latex: Producing latex tables of results (e.g. for a paper).\n",
        "\n",
        "The following sections outline how to use advanced features of the results, which you may skip on a first read:\n",
        "\n",
        " - Derived Quantities: Computing quantities and errors for quantities and parameters not included directly in the model.\n",
        " - Result Extension: Extend the `Result` object with new attributes and methods (e.g. `max_log_likelihood_model_data`).\n",
        " - Samples Filtering: Filter the `Samples` object to only contain samples fulfilling certain criteria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt\n",
        "\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "To illustrate results, we need to perform a model-fit in order to create a `Result` object.\n",
        "\n",
        "We do this below using the standard API and noisy 1D signal example, which you should be familiar with from other \n",
        "example scripts.\n",
        "\n",
        "Note that the `Gaussian` and `Analysis` classes come via the `af.ex` module, which contains example model components\n",
        "that are identical to those found throughout the examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "model = af.Model(af.ex.Gaussian)\n",
        "\n",
        "analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee(\n",
        "    name=\"cookbook_result\",\n",
        "    nwalkers=30,\n",
        "    nsteps=1000,\n",
        "    number_of_cores=1,\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples__\n",
        "\n",
        "The result contains a `Samples` object, which contains all samples of the non-linear search.\n",
        "\n",
        "Each sample corresponds to a set of model parameters that were evaluated and accepted by the non linear search, \n",
        "in this example `emcee.` \n",
        "\n",
        "This includes their log likelihoods, which are used for computing additional information about the model-fit,\n",
        "for example the error on every parameter. \n",
        "\n",
        "Our model-fit used the MCMC algorithm Emcee, so the `Samples` object returned is a `SamplesMCMC` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"MCMC Samples: \\n\")\n",
        "print(samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Parameters__\n",
        "\n",
        "The parameters are stored as a list of lists, where:\n",
        "\n",
        " - The outer list is the size of the total number of samples.\n",
        " - The inner list is the size of the number of free parameters in the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Sample 5's second parameter value (Gaussian -> normalization):\")\n",
        "print(samples.parameter_lists[4][1])\n",
        "print(\"Sample 10`s third parameter value (Gaussian -> sigma)\")\n",
        "print(samples.parameter_lists[9][2], \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Figures of Merit__\n",
        "\n",
        "The `Samples` class contains the log likelihood, log prior, log posterior and weight_list of every accepted sample, where:\n",
        "\n",
        "- The `log_likelihood` is the value evaluated in the `log_likelihood_function`.\n",
        "\n",
        "- The `log_prior` encodes information on how parameter priors map log likelihood values to log posterior values.\n",
        "\n",
        "- The `log_posterior` is `log_likelihood + log_prior`.\n",
        "\n",
        "- The `weight` gives information on how samples are combined to estimate the posterior, which depends on type of search\n",
        "  used (for `Emcee` they are all 1's meaning they are weighted equally).\n",
        "\n",
        "Lets inspect these values for the tenth sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"log(likelihood), log(prior), log(posterior) and weight of the tenth sample.\")\n",
        "print(samples.log_likelihood_list[9])\n",
        "print(samples.log_prior_list[9])\n",
        "print(samples.log_posterior_list[9])\n",
        "print(samples.weight_list[9])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Instances__\n",
        "\n",
        "Many results can be returned as an instance of the model, using the Python class structure of the model composition.\n",
        "\n",
        "For example, we can return the model parameters corresponding to the maximum log likelihood sample.\n",
        "\n",
        "The attributes of the `instance` (`centre`, `normalization` and `sigma`) have these names due to how we composed \n",
        "the `Gaussian` class via the `Model` above. They would be named structured and named differently if we hd \n",
        "used a `Collection` and different names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.max_log_likelihood()\n",
        "\n",
        "print(\"Max Log Likelihood `Gaussian` Instance:\")\n",
        "print(\"Centre = \", instance.centre)\n",
        "print(\"Normalization = \", instance.normalization)\n",
        "print(\"Sigma = \", instance.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This makes it straight forward to plot the median PDF model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = instance.model_data_from(xvalues=np.arange(data.shape[0]))\n",
        "\n",
        "plt.plot(range(data.shape[0]), data)\n",
        "plt.plot(range(data.shape[0]), model_data)\n",
        "plt.title(\"Illustrative model fit to 1D `Gaussian` profile data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Posterior / PDF__\n",
        "\n",
        "The result contains the full posterior information of our non-linear search, which can be used for parameter \n",
        "estimation. \n",
        "\n",
        "PDF stands for \"Probability Density Function\" and it quantifies probability of each model parameter having values\n",
        "that are sampled. It therefore enables error estimation via a process called marginalization.\n",
        "\n",
        "The median pdf vector is available, which estimates every parameter via 1D marginalization of their PDFs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.median_pdf()\n",
        "\n",
        "print(\"Median PDF `Gaussian` Instance:\")\n",
        "print(\"Centre = \", instance.centre)\n",
        "print(\"Normalization = \", instance.normalization)\n",
        "print(\"Sigma = \", instance.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Errors__\n",
        "\n",
        "Methods for computing error estimates on all parameters are provided. \n",
        "\n",
        "This again uses 1D marginalization, now at an input sigma confidence limit. \n",
        "\n",
        "By inputting `sigma=3.0` margnialization find the values spanning 99.7% of 1D PDF. Changing this to `sigma=1.0`\n",
        "would give the errors at the 68.3% confidence limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance_upper_sigma = samples.errors_at_upper_sigma(sigma=3.0)\n",
        "instance_lower_sigma = samples.errors_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Upper Error values (at 3.0 sigma confidence):\")\n",
        "print(\"Centre = \", instance_upper_sigma.centre)\n",
        "print(\"Normalization = \", instance_upper_sigma.normalization)\n",
        "print(\"Sigma = \", instance_upper_sigma.sigma, \"\\n\")\n",
        "\n",
        "print(\"lower Error values (at 3.0 sigma confidence):\")\n",
        "print(\"Centre = \", instance_lower_sigma.centre)\n",
        "print(\"Normalization = \", instance_lower_sigma.normalization)\n",
        "print(\"Sigma = \", instance_lower_sigma.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They can also be returned at the values of the parameters at their error values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance_upper_values = samples.values_at_upper_sigma(sigma=3.0)\n",
        "instance_lower_values = samples.values_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Upper Parameter values w/ error (at 3.0 sigma confidence):\")\n",
        "print(\"Centre = \", instance_upper_values.centre)\n",
        "print(\"Normalization = \", instance_upper_values.normalization)\n",
        "print(\"Sigma = \", instance_upper_values.sigma, \"\\n\")\n",
        "\n",
        "print(\"lower Parameter values w/ errors (at 3.0 sigma confidence):\")\n",
        "print(\"Centre = \", instance_lower_values.centre)\n",
        "print(\"Normalization = \", instance_lower_values.normalization)\n",
        "print(\"Sigma = \", instance_lower_values.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples Summary__\n",
        "\n",
        "The samples summary contains a subset of results access via the `Samples`, for example the maximum likelihood model\n",
        "and parameter error estimates.\n",
        "\n",
        "Using the samples method above can be slow, as the quantities have to be computed from all non-linear search samples\n",
        "(e.g. computing errors requires that all samples are marginalized over). This information is stored directly in the\n",
        "samples summary and can therefore be accessed instantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(samples.summary().max_log_likelihood_sample)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Sample Instance__\n",
        "\n",
        "A non-linear search retains every model that is accepted during the model-fit.\n",
        "\n",
        "We can create an instance of any model -- below we create an instance of the last accepted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.from_sample_index(sample_index=-1)\n",
        "\n",
        "print(\"Gaussian Instance of last sample\")\n",
        "print(\"Centre = \", instance.centre)\n",
        "print(\"Normalization = \", instance.normalization)\n",
        "print(\"Sigma = \", instance.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search Plots__\n",
        "\n",
        "The Probability Density Functions (PDF's) of the results can be plotted using the non-linear search in-built \n",
        "visualization tools.\n",
        "\n",
        "This fit used `Emcee` therefore we use the `MCMCPlotter` for visualization, which wraps the Python library `corner.py`.\n",
        "\n",
        "The `autofit_workspace/*/plots` folder illustrates other packages that can be used to make these plots using\n",
        "the standard output results formats (e.g. `GetDist.py`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.MCMCPlotter(samples=result.samples)\n",
        "plotter.corner_cornerpy()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Maximum Likelihood__\n",
        "\n",
        "The maximum log likelihood value of the model-fit can be estimated by simple taking the maximum of all log\n",
        "likelihoods of the samples.\n",
        "\n",
        "If different models are fitted to the same dataset, this value can be compared to determine which model provides\n",
        "the best fit (e.g. which model has the highest maximum likelihood)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Log Likelihood: \\n\")\n",
        "print(max(samples.log_likelihood_list))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Bayesian Evidence__\n",
        "\n",
        "If a nested sampling non-linear search is used, the evidence of the model is also available which enables Bayesian\n",
        "model comparison to be performed (given we are using Emcee, which is not a nested sampling algorithm, the log evidence \n",
        "is None).\n",
        "\n",
        "A full discussion of Bayesian model comparison is given in `autofit_workspace/*/features/bayes_model_comparison.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_evidence = samples.log_evidence\n",
        "print(f\"Log Evidence: {log_evidence}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Collection__\n",
        "\n",
        "The examples correspond to a model where `af.Model(Gaussian)` was used to compose the model.\n",
        "\n",
        "Below, we illustrate how the results API slightly changes if we compose our model using a `Collection`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.ex.Gaussian, exponential=af.ex.Exponential)\n",
        "\n",
        "analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee(\n",
        "    nwalkers=50,\n",
        "    nsteps=1000,\n",
        "    number_of_cores=1,\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `result.info` shows the result for the model with both a `Gaussian` and `Exponential` profile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Result instances again use the Python classes used to compose the model. \n",
        "\n",
        "However, because our fit uses a `Collection` the `instance` has attributes named according to the names given to the\n",
        "`Collection`, which above were `gaussian` and `exponential`.\n",
        "\n",
        "For complex models, with a large number of model components and parameters, this offers a readable API to interpret\n",
        "the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "instance = samples.max_log_likelihood()\n",
        "\n",
        "print(\"Max Log Likelihood `Gaussian` Instance:\")\n",
        "print(\"Centre = \", instance.gaussian.centre)\n",
        "print(\"Normalization = \", instance.gaussian.normalization)\n",
        "print(\"Sigma = \", instance.gaussian.sigma, \"\\n\")\n",
        "\n",
        "print(\"Max Log Likelihood Exponential Instance:\")\n",
        "print(\"Centre = \", instance.exponential.centre)\n",
        "print(\"Normalization = \", instance.exponential.normalization)\n",
        "print(\"Sigma = \", instance.exponential.rate, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Lists__\n",
        "\n",
        "All results can alternatively be returned as a 1D list of values, by passing `as_instance=False`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_lh_list = samples.max_log_likelihood(as_instance=False)\n",
        "print(\"Max Log Likelihood Model Parameters: \\n\")\n",
        "print(max_lh_list, \"\\n\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The list above does not tell us which values correspond to which parameters.\n",
        "\n",
        "The following quantities are available in the `Model`, where the order of their entries correspond to the parameters \n",
        "in the `ml_vector` above:\n",
        "\n",
        " - `paths`: a list of tuples which give the path of every parameter in the `Model`.\n",
        " - `parameter_names`: a list of shorthand parameter names derived from the `paths`.\n",
        " - `parameter_labels`: a list of parameter labels used when visualizing non-linear search results (see below).\n",
        "\n",
        "For simple models like the one fitted in this tutorial, the quantities below are somewhat redundant. For the\n",
        "more complex models they are important for tracking the parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = samples.model\n",
        "\n",
        "print(model.paths)\n",
        "print(model.parameter_names)\n",
        "print(model.parameter_labels)\n",
        "print(model.model_component_and_parameter_names)\n",
        "print(\"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the methods above are available as lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.median_pdf(as_instance=False)\n",
        "values_at_upper_sigma = samples.values_at_upper_sigma(sigma=3.0, as_instance=False)\n",
        "values_at_lower_sigma = samples.values_at_lower_sigma(sigma=3.0, as_instance=False)\n",
        "errors_at_upper_sigma = samples.errors_at_upper_sigma(sigma=3.0, as_instance=False)\n",
        "errors_at_lower_sigma = samples.errors_at_lower_sigma(sigma=3.0, as_instance=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Latex__\n",
        "\n",
        "If you are writing modeling results up in a paper, you can use inbuilt latex tools to create latex table code which \n",
        "you can copy to your .tex document.\n",
        "\n",
        "By combining this with the filtering tools below, specific parameters can be included or removed from the latex.\n",
        "\n",
        "Remember that the superscripts of a parameter are loaded from the config file `notation/label.yaml`, providing high\n",
        "levels of customization for how the parameter names appear in the latex table. This is especially useful if your model\n",
        "uses the same model components with the same parameter, which therefore need to be distinguished via superscripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "latex = af.text.Samples.latex(\n",
        "    samples=result.samples,\n",
        "    median_pdf_model=True,\n",
        "    sigma=3.0,\n",
        "    name_to_label=True,\n",
        "    include_name=True,\n",
        "    include_quickmath=True,\n",
        "    prefix=\"Example Prefix \",\n",
        "    suffix=\" \\\\[-2pt]\",\n",
        ")\n",
        "\n",
        "print(latex)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Derived Quantities__\n",
        "\n",
        "The parameters `centre`, `normalization` and `sigma` are the model parameters of the `Gaussian`. They are sampled\n",
        "directly by the non-linear search and we can therefore use the `Samples` object to easily determine their values and \n",
        "errors.\n",
        "\n",
        "Derived quantities (also called latent variables) are those which are not sampled directly by the non-linear search, \n",
        "but one may still wish to know their values and errors after the fit is complete. For example, what if we want the \n",
        "error on the full width half maximum (FWHM) of the Gaussian? \n",
        "\n",
        "This is achieved by adding them to the `compute_latent_variables` method of the `Analysis` class, which is called\n",
        "after the non-linear search has completed. The analysis cookbook illustrates how to do this.\n",
        "\n",
        "The example analysis used above includes a `compute_latent_variables` method that computes the FWHM of the Gaussian\n",
        "profile. \n",
        "\n",
        "This leads to a number of noteworthy outputs:\n",
        "\n",
        " - A `latent.results` file is output to the results folder, which includes the value and error of all derived quantities \n",
        "   based on the non-linear search samples (in this example only the `fwhm`).\n",
        "   \n",
        " - A `latent/samples.csv` is output which lists every accepted sample's value of every derived quantity, which is again\n",
        "   analogous to the `samples.csv` file (in this example only the `fwhm`). \n",
        "     \n",
        " - A `latent/samples_summary.json` is output which acts analogously to `samples_summary.json` but for the derived \n",
        "   quantities of the model (in this example only the `fwhm`).\n",
        "\n",
        "Derived quantities are also accessible via the `Samples` object, following a similar API to the model parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "latent = analysis.compute_latent_samples(result.samples)\n",
        "\n",
        "instance = latent.max_log_likelihood()\n",
        "\n",
        "print(f\"Max Likelihood FWHM: {instance.gaussian.fwhm}\")\n",
        "\n",
        "instance = latent.median_pdf()\n",
        "\n",
        "print(f\"Median PDF FWHM {instance.gaussian.fwhm}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Derived Errors Manual (Advanced)__\n",
        "\n",
        "The derived quantities decorator above provides a simple interface for computing the errors of a derived quantity and\n",
        "ensuring all results are easily inspected in the output results folder.\n",
        "\n",
        "However, you may wish to compute the errors of a derived quantity manually. For example, if it is a quantity that \n",
        "you did not decorate before performing the fit, or if it is computationally expensive to compute and you only want\n",
        "to compute it specific circumstances.\n",
        "\n",
        "We create the PDF of the derived quantity, the FWHM, manually, which we marginalize over using the same function \n",
        "we use to marginalize model parameters. We compute the FWHM of every accepted model sampled by the non-linear search \n",
        "and use this determine the PDF of the FWHM. \n",
        "\n",
        "When combining the FWHM's we weight each value by its `weight`. For Emcee, an MCMC algorithm, the weight of every \n",
        "sample is 1, but weights may take different values for other non-linear searches.\n",
        "\n",
        "In order to pass these samples to the function `marginalize`, which marginalizes over the PDF of the FWHM to compute \n",
        "its error, we also pass the weight list of the samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fwhm_list = []\n",
        "\n",
        "for sample in samples.sample_list:\n",
        "    instance = sample.instance_for_model(model=samples.model)\n",
        "\n",
        "    sigma = instance.gaussian.sigma\n",
        "\n",
        "    fwhm = 2 * np.sqrt(2 * np.log(2)) * sigma\n",
        "\n",
        "    fwhm_list.append(fwhm)\n",
        "\n",
        "median_fwhm, lower_fwhm, upper_fwhm = af.marginalize(\n",
        "    parameter_list=fwhm_list, sigma=3.0, weight_list=samples.weight_list\n",
        ")\n",
        "\n",
        "print(f\"FWHM = {median_fwhm} ({upper_fwhm} {lower_fwhm}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The calculation above could be computationally expensive, if there are many samples and the derived quantity is\n",
        "slow to compute.\n",
        "\n",
        "An alternative approach, which will provide comparable accuracy provided enough draws are used, is to sample \n",
        "points randomy from the PDF of the model and use these to compute the derived quantity.\n",
        "\n",
        "Draws are from the PDF of the model, so the weights of the samples are accounted for and we therefore do not\n",
        "pass them to the `marginalize` function (it essentially treats all samples as having equal weight).\n",
        "\n",
        "TRY AND EXCEPT INCLUDED TO FIX BUG, NEED TO SOLVE IN FUTURE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    random_draws = 50\n",
        "\n",
        "    fwhm_list = []\n",
        "\n",
        "    for i in range(random_draws):\n",
        "        instance = samples.draw_randomly_via_pdf()\n",
        "\n",
        "        sigma = instance.gaussian.sigma\n",
        "\n",
        "        fwhm = 2 * np.sqrt(2 * np.log(2)) * sigma\n",
        "\n",
        "        fwhm_list.append(fwhm)\n",
        "\n",
        "    median_fwhm, lower_fwhm, upper_fwhm = af.marginalize(\n",
        "        parameter_list=fwhm_list, sigma=3.0, weight_list=samples.weight_list\n",
        "    )\n",
        "\n",
        "    print(f\"fwhm = {median_fwhm} ({upper_fwhm} {lower_fwhm}\")\n",
        "\n",
        "except ValueError:\n",
        "    pass"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples Filtering (Advanced)__\n",
        "\n",
        "Our samples object has the results for all three parameters in our model. However, we might only be interested in the\n",
        "results of a specific parameter.\n",
        "\n",
        "The basic form of filtering specifies parameters via their path, which was printed above via the model and is printed \n",
        "again below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Parameter paths in the model which are used for filtering:\")\n",
        "print(samples.model.paths)\n",
        "\n",
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "samples = samples.with_paths([(\"gaussian\", \"centre\")])\n",
        "\n",
        "print(\"All parameters of the very first sample (containing only the Gaussian centre.\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "print(\"Maximum Log Likelihood Model Instances (containing only the Gaussian centre):\\n\")\n",
        "print(samples.max_log_likelihood(as_instance=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We specified each path as a list of tuples of strings. \n",
        "\n",
        "This is how the source code internally stores the path to different components of the model, but it is not \n",
        "consistent with the API used to compose a model.\n",
        "\n",
        "We can alternatively use the following API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "samples = samples.with_paths([\"gaussian.centre\"])\n",
        "\n",
        "print(\"All parameters of the very first sample (containing only the Gaussian centre).\")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We filtered the `Samples` above by asking for all parameters which included the path (\"gaussian\", \"centre\").\n",
        "\n",
        "We can alternatively filter the `Samples` object by removing all parameters with a certain path. Below, we remove\n",
        "the Gaussian's `centre` to be left with 2 parameters; the `normalization` and `sigma`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Parameter paths in the model which are used for filtering:\")\n",
        "print(samples.model.paths)\n",
        "\n",
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "samples = samples.without_paths([\"gaussian.centre\"])\n",
        "\n",
        "print(\n",
        "    \"All parameters of the very first sample (containing only the Gaussian normalization and sigma).\"\n",
        ")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}