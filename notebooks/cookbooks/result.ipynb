{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cookbook: Result\n",
        "================\n",
        "\n",
        "A non-linear search fits a model to a dataset, returning a `Result` object that contains a lot of information on the\n",
        "model-fit.\n",
        "\n",
        "This cookbook provides a concise reference to the result API.\n",
        "\n",
        "The cookbook then describes how the results of a search can be output to hard-disk and loaded back into Python,\n",
        "either using the `Aggregator` object or by building an sqlite database of results. Result loading supports\n",
        "queries, so that only the results of interest are returned.\n",
        "\n",
        "The samples of the non-linear search, which are used to estimate quantities the maximum likelihood model and\n",
        "parameter errors, are described separately in the `samples.py` cookbook.\n",
        "\n",
        "__Contents__\n",
        "\n",
        "An overview of the `Result` object's functionality is given in the following sections:\n",
        "\n",
        " - Info: Print the `info` attribute of the `Result` object to display a summary of the model-fit.\n",
        " - Max Log Likelihood Instance: Getting the maximum likelihood model instance.\n",
        " - Samples: Getting the samples of the non-linear search from a result.\n",
        " - Custom Result: Extending the `Result` object with custom attributes specific to the model-fit.\n",
        "\n",
        "The cookbook next describes how results can be output to hard-disk and loaded back into Python via the `Aggregator`:\n",
        "\n",
        " - Output To Hard-Disk: Output results to hard-disk so they can be inspected and used to restart a crashed search.\n",
        " - Files: The files that are stored in the `files` folder that is created when results are output to hard-disk.\n",
        " - Loading From Hard-disk: Loading results from hard-disk to Python variables via the aggregator.\n",
        " - Generators: Why loading results uses Python generators to ensure memory efficiency.\n",
        "\n",
        "The cookbook next gives examples of how to load all the following results from the database:\n",
        "\n",
        " - Loading Samples: The samples of the non-linear search (e.g. all parameter values, log likelihoods, etc.).\n",
        " - Loading Model: The model fitted by the non-linear search.\n",
        " - Loading Search: The search used to perform the model-fit.\n",
        " - Loading Samples Info: Additional information on the samples.\n",
        " - Loading Samples Summary: A summary of the samples of the non-linear search (e.g. the maximum log likelihood model).\n",
        " - Loading Info: The `info` dictionary passed to the search.\n",
        "\n",
        "The output of results to hard-disk is customizeable and described in the following section:\n",
        "\n",
        " - Custom Output: Extend `Analysis` classes to output additional information which can be loaded via the aggregator.\n",
        "\n",
        "Using queries to load specific results is described in the following sections:\n",
        "\n",
        " - Querying Datasets: Query based on the name of the dataset.\n",
        " - Querying Searches: Query based on the name of the search.\n",
        " - Querying Models: Query based on the model that is fitted.\n",
        " - Querying Results: Query based on the results of the model-fit.\n",
        " - Querying Logic: Use logic to combine queries to load specific results (e.g. AND, OR, etc.).\n",
        "\n",
        "The final section describes how to use results built in an sqlite database file:\n",
        "\n",
        " - Database: Building a database file from the output folder.\n",
        " - Unique Identifiers: The unique identifier of each model-fit.\n",
        " - Writing Directly To Database: Writing results directly to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import json\n",
        "from os import path\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Simple Fit__\n",
        "\n",
        "To illustrate the API of the result object, we first fit a 1D `Gaussian` profile with a `Gaussian` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "model = af.Model(af.ex.Gaussian)\n",
        "\n",
        "analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee(\n",
        "    nwalkers=30,\n",
        "    nsteps=1000,\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Info__\n",
        "\n",
        "Printing the `info` attribute shows the overall result of the model-fit in a human readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Max Log Likelihood Instance__\n",
        "\n",
        "The `max_log_likelihood_instance` is the model instance of the maximum log likelihood model, which is the model\n",
        "that maximizes the likelihood of the data given the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = result.max_log_likelihood_instance\n",
        "\n",
        "print(\"Max Log Likelihood `Gaussian` Instance:\")\n",
        "print(\"Centre = \", instance.centre)\n",
        "print(\"Normalization = \", instance.normalization)\n",
        "print(\"Sigma = \", instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples__\n",
        "\n",
        "The `Samples` class contains all information on the non-linear search samples, for example the value of every parameter\n",
        "sampled using the fit or an instance of the maximum likelihood model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The samples are described in detail separately in the `samples.py` cookbook.\n",
        "\n",
        "__Custom Result__\n",
        "\n",
        "The result can be can be customized to include additional information about the model-fit that is specific to your \n",
        "model-fitting problem.\n",
        "\n",
        "For example, for fitting 1D profiles, the `Result` could include the maximum log likelihood model 1D data: \n",
        "\n",
        "`print(result.max_log_likelihood_model_data_1d)`\n",
        "\n",
        "In other examples, this quantity has been manually computed after the model-fit has completed.\n",
        "\n",
        "The custom result API allows us to do this. First, we define a custom `Result` class, which includes the property\n",
        "`max_log_likelihood_model_data_1d`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class ResultExample(af.Result):\n",
        "    @property\n",
        "    def max_log_likelihood_model_data_1d(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns the maximum log likelihood model's 1D model data.\n",
        "\n",
        "        This is an example of how we can pass the `Analysis` class a custom `Result` object and extend this result\n",
        "        object with new properties that are specific to the model-fit we are performing.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.analysis.data.shape[0])\n",
        "\n",
        "        return self.instance.model_data_from(xvalues=xvalues)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The custom result has access to the analysis class, meaning that we can use any of its methods or properties to \n",
        "compute custom result properties.\n",
        "\n",
        "To make it so that the `ResultExample` object above is returned by the search we overwrite the `Result` class attribute \n",
        "of the `Analysis` and define a `make_result` object describing what we want it to contain:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.ex.Analysis):\n",
        "    \"\"\"\n",
        "    This overwrite means the `ResultExample` class is returned after the model-fit.\n",
        "    \"\"\"\n",
        "\n",
        "    Result = ResultExample\n",
        "\n",
        "    def make_result(\n",
        "        self,\n",
        "        samples_summary: af.SamplesSummary,\n",
        "        paths: af.AbstractPaths,\n",
        "        samples: Optional[af.SamplesPDF] = None,\n",
        "        search_internal: Optional[object] = None,\n",
        "        analysis: Optional[object] = None,\n",
        "    ) -> Result:\n",
        "        \"\"\"\n",
        "        Returns the `Result` of the non-linear search after it is completed.\n",
        "\n",
        "        The result type is defined as a class variable in the `Analysis` class (see top of code under the python code\n",
        "        `class Analysis(af.Analysis)`.\n",
        "\n",
        "        The result can be manually overwritten by a user to return a user-defined result object, which can be extended\n",
        "        with additional methods and attribute specific to the model-fit.\n",
        "\n",
        "        This example class does example this, whereby the analysis result has been overwritten with the `ResultExample`\n",
        "        class, which contains a property `max_log_likelihood_model_data_1d` that returns the model data of the\n",
        "        best-fit model. This API means you can customize your result object to include whatever attributes you want\n",
        "        and therefore make a result object specific to your model-fit and model-fitting problem.\n",
        "\n",
        "        The `Result` object you return can be customized to include:\n",
        "\n",
        "        - The samples summary, which contains the maximum log likelihood instance and median PDF model.\n",
        "\n",
        "        - The paths of the search, which are used for loading the samples and search internal below when a search\n",
        "        is resumed.\n",
        "\n",
        "        - The samples of the non-linear search (e.g. MCMC chains) also stored in `samples.csv`.\n",
        "\n",
        "        - The non-linear search used for the fit in its internal representation, which is used for resuming a search\n",
        "        and making bespoke visualization using the search's internal results.\n",
        "\n",
        "        - The analysis used to fit the model (default disabled to save memory, but option may be useful for certain\n",
        "        projects).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        samples_summary\n",
        "            The summary of the samples of the non-linear search, which include the maximum log likelihood instance and\n",
        "            median PDF model.\n",
        "        paths\n",
        "            An object describing the paths for saving data (e.g. hard-disk directories or entries in sqlite database).\n",
        "        samples\n",
        "            The samples of the non-linear search, for example the chains of an MCMC run.\n",
        "        search_internal\n",
        "            The internal representation of the non-linear search used to perform the model-fit.\n",
        "        analysis\n",
        "            The analysis used to fit the model.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Result\n",
        "            The result of the non-linear search, which is defined as a class variable in the `Analysis` class.\n",
        "        \"\"\"\n",
        "        return self.Result(\n",
        "            samples_summary=samples_summary,\n",
        "            paths=paths,\n",
        "            samples=samples,\n",
        "            search_internal=search_internal,\n",
        "            analysis=self,\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the `Analysis` class above, the `Result` object returned by the search is now a `ResultExample` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee(\n",
        "    nwalkers=30,\n",
        "    nsteps=1000,\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(result.max_log_likelihood_model_data_1d)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output To Hard-Disk__\n",
        "\n",
        "By default, a non-linear search does not output its results to hard-disk and its results can only be inspected\n",
        "in Python via the `result` object. \n",
        "\n",
        "However, the results of any non-linear search can be output to hard-disk by passing the `name` and / or `path_prefix`\n",
        "attributes, which are used to name files and output the results to a folder on your hard-disk.\n",
        "\n",
        "This cookbook now runs the three searches with output to hard-disk enabled, so you can see how the results are output\n",
        "to hard-disk and to then illustrate how they can be loaded back into Python.\n",
        "\n",
        "Note that an `info` dictionary is also passed to the search, which includes the date of the model-fit and the exposure\n",
        "time of the dataset. This information is stored output to hard-disk and can be loaded to help interpret the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "info = {\"date_of_observation\": \"01-02-18\", \"exposure_time\": 1000.0}\n",
        "\n",
        "dataset_name_list = [\"gaussian_x1_0\", \"gaussian_x1_1\", \"gaussian_x1_2\"]\n",
        "\n",
        "model = af.Collection(gaussian=af.ex.Gaussian)\n",
        "\n",
        "model.gaussian.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.gaussian.normalization = af.LogUniformPrior(lower_limit=1e-2, upper_limit=1e2)\n",
        "model.gaussian.sigma = af.TruncatedGaussianPrior(\n",
        "    mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf\n",
        ")\n",
        "\n",
        "for dataset_name in dataset_name_list:\n",
        "    dataset_path = path.join(\"dataset\", \"example_1d\", dataset_name)\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    search = af.DynestyStatic(\n",
        "        name=\"multi_result_example\",\n",
        "        path_prefix=path.join(\"cookbooks\", \"result\"),\n",
        "        unique_tag=dataset_name,  # This makes the unique identifier use the dataset name\n",
        "        nlive=50,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"\"\"\n",
        "        The non-linear search has begun running. \n",
        "        This Jupyter notebook cell with progress once search has completed, this could take a few minutes!\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    result = search.fit(model=model, analysis=analysis, info=info)\n",
        "\n",
        "print(\"Search has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Files__\n",
        "\n",
        "By outputting results to hard-disk, a `files` folder is created containing .json / .csv files of the model, \n",
        "samples, search, etc, for each fit.\n",
        "\n",
        "You should check it out now for the completed fits on your hard-disk.\n",
        "\n",
        "A description of all files is as follows:\n",
        "\n",
        " - `model`: The `model` defined above and used in the model-fit (`model.json`).\n",
        " - `search`: The non-linear search settings (`search.json`).\n",
        " - `samples`: The non-linear search samples (`samples.csv`).\n",
        " - `samples_info`: Additional information about the samples (`samples_info.json`).\n",
        " - `samples_summary`: A summary of key results of the samples (`samples_summary.json`).\n",
        " - `info`: The info dictionary passed to the search (`info.json`).\n",
        " - `covariance`: The inferred covariance matrix (`covariance.csv`).\n",
        " - `data`: The 1D noisy data used that is fitted (`data.json`).\n",
        " - `noise_map`: The 1D noise-map fitted (`noise_map.json`).\n",
        "\n",
        "The `samples` and `samples_summary` results contain a lot of repeated information. The `samples` result contains\n",
        "the full non-linear search samples, for example every parameter sample and its log likelihood. The `samples_summary`\n",
        "contains a summary of the results, for example the maximum log likelihood model and error estimates on parameters\n",
        "at 1 and 3 sigma confidence.\n",
        "\n",
        "Accessing results via the `samples_summary` is much faster, because as it does not reperform calculations using the full \n",
        "list of samples. Therefore, if the result you want is accessible via the `samples_summary` you should use it\n",
        "but if not you can revert to the `samples.\n",
        "\n",
        "__Loading From Hard-disk__\n",
        "\n",
        "The multi-fits above wrote the results to hard-disk in three distinct folders, one for each dataset.\n",
        "\n",
        "Their results are loaded using the `Aggregator` object, which finds the results in the output directory and can\n",
        "load them into Python objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.aggregator.aggregator import Aggregator\n",
        "\n",
        "agg = Aggregator.from_directory(\n",
        "    directory=path.join(\"multi_result_example\"),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Generators__\n",
        "\n",
        "Before using the aggregator to inspect results, lets discuss Python generators. \n",
        "\n",
        "A generator is an object that iterates over a function when it is called. The aggregator creates all of the objects \n",
        "that it loads from the database as generators (as opposed to a list, or dictionary, or another Python type).\n",
        "\n",
        "This is because generators are memory efficient, as they do not store the entries of the database in memory \n",
        "simultaneously. This contrasts objects like lists and dictionaries, which store all entries in memory all at once. \n",
        "If you fit a large number of datasets, lists and dictionaries will use a lot of memory and could crash your computer!\n",
        "\n",
        "Once we use a generator in the Python code, it cannot be used again. To perform the same task twice, the \n",
        "generator must be remade it. This cookbook therefore rarely stores generators as variables and instead uses the \n",
        "aggregator to create each generator at the point of use.\n",
        "\n",
        "To create a generator of a specific set of results, we use the `values` method. This takes the `name` of the\n",
        "object we want to create a generator of, for example inputting `name=samples` will return the results `Samples`\n",
        "object (which is illustrated in detail below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples__\n",
        "\n",
        "By converting this generator to a list and printing it, it is a list of 3 `SamplesNest` objects, corresponding to \n",
        "the 3 model-fits performed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Samples:\\n\")\n",
        "samples_gen = agg.values(\"samples\")\n",
        "print(samples_gen)\n",
        "print(\"Total Samples Objects = \", len(agg), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Loading Model__\n",
        "\n",
        "The model used to perform the model fit for each of the 3 datasets can be loaded via the aggregator and printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_gen = agg.values(\"model\")\n",
        "\n",
        "for model in model_gen:\n",
        "    print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Loading Search__\n",
        "\n",
        "The non-linear search used to perform the model fit can be loaded via the aggregator and printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_gen = agg.values(\"search\")\n",
        "\n",
        "for search in search_gen:\n",
        "    print(search)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Loading Samples__\n",
        "\n",
        "The `Samples` class contains all information on the non-linear search samples, for example the value of every parameter\n",
        "sampled using the fit or an instance of the maximum likelihood model.\n",
        "\n",
        "The `Samples` class is described fully in the results cookbook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(\"The tenth sample`s third parameter\")\n",
        "    print(samples.parameter_lists[9][2], \"\\n\")\n",
        "\n",
        "    instance = samples.max_log_likelihood()\n",
        "\n",
        "    print(\"Max Log Likelihood `Gaussian` Instance:\")\n",
        "    print(\"Centre = \", instance.gaussian.centre)\n",
        "    print(\"Normalization = \", instance.gaussian.normalization)\n",
        "    print(\"Sigma = \", instance.gaussian.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Loading Samples Info__\n",
        "\n",
        "The samples info contains additional information on the samples, which depends on the non-linear search used. \n",
        "\n",
        "For example, for a nested sampling algorithm it contains information on the number of live points, for a MCMC\n",
        "algorithm it contains information on the number of steps, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples_info in agg.values(\"samples_info\"):\n",
        "    print(samples_info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Loading Samples Summary__\n",
        "\n",
        "The samples summary contains a subset of results access via the `Samples`, for example the maximum likelihood model\n",
        "and parameter error estimates.\n",
        "\n",
        "Using the samples method above can be slow, as the quantities have to be computed from all non-linear search samples\n",
        "(e.g. computing errors requires that all samples are marginalized over). This information is stored directly in the\n",
        "samples summary and can therefore be accessed instantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# for samples_summary in agg.values(\"samples_summary\"):\n",
        "#\n",
        "#     instance = samples_summary.max_log_likelihood()\n",
        "#\n",
        "#     print(\"Max Log Likelihood `Gaussian` Instance:\")\n",
        "#     print(\"Centre = \", instance.centre)\n",
        "#     print(\"Normalization = \", instance.normalization)\n",
        "#     print(\"Sigma = \", instance.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Loading Info__\n",
        "\n",
        "The info dictionary passed to the search, discussed earlier in this cookbook, is accessible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for info in agg.values(\"info\"):\n",
        "    print(info[\"date_of_observation\"])\n",
        "    print(info[\"exposure_time\"])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Custom Output__\n",
        "\n",
        "The results accessible via the database (e.g. `model`, `samples`) are those contained in the `files` folder.\n",
        "\n",
        "By extending an `Analysis` class with the methods `save_attributes` and `save_results`, \n",
        "custom files can be written to the `files` folder and become accessible via the database.\n",
        "\n",
        "To save the objects in a human readable and loaded .json format, the `data` and `noise_map`, which are natively stored\n",
        "as 1D numpy arrays, are converted to a suitable dictionary output format. This uses the **PyAutoConf** method\n",
        "`to_dict`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data: np.ndarray, noise_map: np.ndarray):\n",
        "        \"\"\"\n",
        "        Standard Analysis class example used throughout PyAutoFit examples.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance) -> float:\n",
        "        \"\"\"\n",
        "        Standard log likelihood function used throughout PyAutoFit examples.\n",
        "        \"\"\"\n",
        "\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_from(xvalues=xvalues)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * self.noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def save_attributes(self, paths: af.DirectoryPaths):\n",
        "        \"\"\"\n",
        "        Before the non-linear search begins, this routine saves attributes of the `Analysis` object to the `files`\n",
        "        folder such that they can be loaded after the analysis using PyAutoFit's database and aggregator tools.\n",
        "\n",
        "        For this analysis, it uses the `AnalysisDataset` object's method to output the following:\n",
        "\n",
        "        - The dataset's data as a .json file.\n",
        "        - The dataset's noise-map as a .json file.\n",
        "\n",
        "        These are accessed using the aggregator via `agg.values(\"data\")` and `agg.values(\"noise_map\")`.\n",
        "\n",
        "        They are saved using the paths function `save_json`, noting that this saves outputs appropriate for the\n",
        "        sqlite3 database.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        paths\n",
        "            The paths object which manages all paths, e.g. where the non-linear search outputs are stored,\n",
        "            visualization, and the pickled objects used by the aggregator output by this function.\n",
        "        \"\"\"\n",
        "        from autoconf.dictable import to_dict\n",
        "\n",
        "        paths.save_json(name=\"data\", object_dict=to_dict(self.data))\n",
        "        paths.save_json(name=\"noise_map\", object_dict=to_dict(self.noise_map))\n",
        "\n",
        "    def save_results(self, paths: af.DirectoryPaths, result: af.Result):\n",
        "        \"\"\"\n",
        "        At the end of a model-fit,  this routine saves attributes of the `Analysis` object to the `files`\n",
        "        folder such that they can be loaded after the analysis using PyAutoFit's database and aggregator tools.\n",
        "\n",
        "        For this analysis it outputs the following:\n",
        "\n",
        "        - The maximum log likelihood model data as a .json file.\n",
        "\n",
        "        This is accessed using the aggregator via `agg.values(\"model_data\")`.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        paths\n",
        "            The paths object which manages all paths, e.g. where the non-linear search outputs are stored,\n",
        "            visualization and the pickled objects used by the aggregator output by this function.\n",
        "        result\n",
        "            The result of a model fit, including the non-linear search, samples and maximum likelihood model.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        instance = result.max_log_likelihood_instance\n",
        "\n",
        "        model_data = instance.model_data_from(xvalues=xvalues)\n",
        "\n",
        "        # The path where model_data.json is saved, e.g. output/dataset_name/unique_id/files/model_data.json\n",
        "\n",
        "        file_path = (path.join(paths._json_path, \"model_data.json\"),)\n",
        "\n",
        "        with open(file_path, \"w+\") as f:\n",
        "            json.dump(model_data, f, indent=4)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Querying Datasets__\n",
        "\n",
        "The aggregator can query the database, returning only specific fits of interested. \n",
        "\n",
        "We can query using the `dataset_name` string we input into the model-fit above, in order to get the results\n",
        "of a fit to a specific dataset. \n",
        "\n",
        "For example, querying using the string `gaussian_x1_1` returns results for only the fit using the \n",
        "second `Gaussian` dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "unique_tag = agg.search.unique_tag\n",
        "agg_query = agg.query(unique_tag == \"gaussian_x1_1\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, this list has only 1 `SamplesNest` corresponding to the second dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(agg_query.values(\"samples\"))\n",
        "print(\"Total Samples Objects via dataset_name Query = \", len(agg_query), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we query using an incorrect dataset name we get no results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "unique_tag = agg.search.unique_tag\n",
        "agg_query = agg.query(unique_tag == \"incorrect_name\")\n",
        "samples_gen = agg_query.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Querying Searches__\n",
        "\n",
        "We can query using the `name` of the non-linear search used to fit the model. \n",
        "\n",
        "In this cookbook, all three fits used the same search, named `database_example`. Query based on search name in this \n",
        "example is therefore somewhat pointless. \n",
        "\n",
        "However, querying based on the search name is useful for model-fits which use a range of searches, for example\n",
        "if different non-linear searches are used multiple times.\n",
        "\n",
        "As expected, the query using search name below contains all 3 results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "name = agg.search.name\n",
        "agg_query = agg.query(name == \"database_example\")\n",
        "\n",
        "print(agg_query.values(\"samples\"))\n",
        "print(\"Total Samples Objects via name Query = \", len(agg_query), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Querying Models__\n",
        "\n",
        "We can query based on the model fitted. \n",
        "\n",
        "For example, we can load all results which fitted a `Gaussian` model-component, which in this simple example is all\n",
        "3 model-fits.\n",
        "\n",
        "Querying via the model is useful for loading results after performing many model-fits with many different model \n",
        "parameterizations to large (e.g. Bayesian model comparison).  \n",
        "\n",
        "[Note: the code `agg.model.gaussian` corresponds to the fact that in the `Collection` above, we named the model\n",
        "component `gaussian`. If this `Collection` had used a different name the code below would change \n",
        "correspondingly. Models with multiple model components (e.g., `gaussian` and `exponential`) are therefore also easily \n",
        "accessed via the database.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian = agg.model.gaussian\n",
        "agg_query = agg.query(gaussian == af.ex.Gaussian)\n",
        "print(\"Total Samples Objects via `Gaussian` model query = \", len(agg_query), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Querying Results__\n",
        "\n",
        "We can query based on the results of the model-fit.\n",
        "\n",
        "Below, we query the database to find all fits where the inferred value of `sigma` for the `Gaussian` is less \n",
        "than 3.0 (which returns only the first of the three model-fits)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian = agg.model.gaussian\n",
        "agg_query = agg.query(gaussian.sigma < 3.0)\n",
        "print(\"Total Samples Objects In Query `gaussian.sigma < 3.0` = \", len(agg_query), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Querying with Logic__\n",
        "\n",
        "Advanced queries can be constructed using logic. \n",
        "\n",
        "Below, we combine the two queries above to find all results which fitted a `Gaussian` AND (using the & symbol) \n",
        "inferred a value of sigma less than 3.0. \n",
        "\n",
        "The OR logical clause is also supported via the symbol |."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian = agg.model.gaussian\n",
        "agg_query = agg.query((gaussian == af.ex.Gaussian) & (gaussian.sigma < 3.0))\n",
        "print(\n",
        "    \"Total Samples Objects In Query `Gaussian & sigma < 3.0` = \", len(agg_query), \"\\n\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Database__\n",
        "\n",
        "The default behaviour of model-fitting results output is to be written to hard-disc in folders. These are simple to \n",
        "navigate and manually check. \n",
        "\n",
        "For small model-fitting tasks this is sufficient, however it does not scale well when performing many model fits to \n",
        "large datasets, because manual inspection of results becomes time consuming.\n",
        "\n",
        "All results can therefore be output to an sqlite3 (https://docs.python.org/3/library/sqlite3.html) relational database,\n",
        "meaning that results can be loaded into a Jupyter notebook or Python script for inspection, analysis and interpretation. \n",
        "This database supports advanced querying, so that specific model-fits (e.g., which fit a certain model or dataset) can \n",
        "be loaded.\n",
        "\n",
        "__Unique Identifiers__\n",
        "\n",
        "We have discussed how every model-fit is given a unique identifier, which is used to ensure that the results of the\n",
        "model-fit are output to a separate folder on hard-disk.\n",
        "\n",
        "Each unique identifier is also used to define every entry of the database as it is built. Unique identifiers \n",
        "therefore play the same vital role for the database of ensuring that every set of results written to it are unique.\n",
        "\n",
        "__Building From Output Folder__\n",
        "\n",
        "The fits above wrote the results to hard-disk in folders, not as an .sqlite database file. \n",
        "\n",
        "We build the database below, where the `database_name` corresponds to the name of your output folder and is also the \n",
        "name of the `.sqlite` database file that is created.\n",
        "\n",
        "If you are fitting a relatively small number of datasets (e.g. 10-100) having all results written to hard-disk (e.g. \n",
        "for quick visual inspection) and using the database for sample wide analysis is beneficial.\n",
        "\n",
        "We can optionally only include completed model-fits but setting `completed_only=True`.\n",
        "\n",
        "If you inspect the `output` folder, you will see a `database.sqlite` file which contains the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "database_name = \"database\"\n",
        "\n",
        "agg = af.Aggregator.from_database(\n",
        "    filename=f\"{database_name}.sqlite\", completed_only=False\n",
        ")\n",
        "\n",
        "agg.add_directory(directory=path.join(\"output\", \"cookbooks\", database_name))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Writing Directly To Database__\n",
        "\n",
        "Results can be written directly to the .sqlite database file, skipping output to hard-disk entirely, by creating\n",
        "a session and passing this to the non-linear search.\n",
        "\n",
        "The code below shows how to do this, but it is commented out to avoid rerunning the non-linear searches.\n",
        "\n",
        "This is ideal for tasks where model-fits to hundreds or thousands of datasets are performed, as it becomes unfeasible\n",
        "to inspect the results of all fits on the hard-disk. \n",
        "\n",
        "Our recommended workflow is to set up database analysis scripts using ~10 model-fits, and then scaling these up\n",
        "to large samples by writing directly to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "session = af.db.open_database(\"database.sqlite\")\n",
        "\n",
        "search = af.DynestyStatic(\n",
        "    name=\"multi_result_example\",\n",
        "    path_prefix=path.join(\"cookbooks\", \"result\"),\n",
        "    unique_tag=dataset_name,  # This makes the unique identifier use the dataset name\n",
        "    session=session,  # This can instruct the search to write to the .sqlite database.\n",
        "    nlive=50,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you run the above code and inspect the `output` folder, you will see a `database.sqlite` file which contains \n",
        "the results.\n",
        "\n",
        "The API for loading a database and creating an aggregator to query is as follows:\n",
        "\n",
        "# agg = af.Aggregator.from_database(\"database.sqlite\")\n",
        "\n",
        "Once we have the Aggregator, we can use it to query the database and load results as we did before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}