{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cookbook: Results\n",
        "=================\n",
        "\n",
        "After a non-linear search has completed, it returns a `Result` object that contains information on fit, such as\n",
        "the maximum likelihood model instance, the errors on each parameter and the Bayesian evidence.\n",
        "\n",
        "This cookbook provides an overview of using the results.\n",
        "\n",
        "__Contents__\n",
        "\n",
        " - Model Fit: Perform a simple model-fit to create a `Result` object.\n",
        " - Info: Print the `info` attribute of the `Result` object to display a summary of the model-fit.\n",
        " - Loading From Hard-disk: Loading results from hard-disk to Python variables via the aggregator.\n",
        " - Samples: The `Samples` object contained in the `Result`, containing all non-linear samples (e.g. parameters,\n",
        "   log likelihoods, etc.).\n",
        " - Instances: Returning instances of the model corresponding to a particular sample (e.g. the maximum log likelihood).\n",
        " - Posterior / PDF: The median PDF model instance and PDF vectors of all model parameters via 1D marginalization.\n",
        " - Errors: The errors on every parameter estimated from the PDF, computed via marginalized 1D PDFs at an input sigma.\n",
        " - Samples Summary: A summary of the samples of the non-linear search (e.g. the maximum log likelihood model) which can\n",
        "   be faster to load than the full set of samples.\n",
        " - Sample Instance: The model instance of any accepted sample.\n",
        " - Search Plots: Plots of the non-linear search, for example a corner plot or 1D PDF of every parameter.\n",
        " - Maximum Likelihood: The maximum log likelihood model value.\n",
        " - Bayesian Evidence: The log evidence estimated via a nested sampling algorithm.\n",
        " - Collection: Results created from models defined via a `Collection` object.\n",
        " - Lists: Extracting results as Python lists instead of instances.\n",
        " - Latex: Producing latex tables of results (e.g. for a paper).\n",
        "\n",
        "The following sections outline how to use advanced features of the results, which you may skip on a first read:\n",
        "\n",
        " - Derived Quantities: Computing quantities and errors for quantities and parameters not included directly in the model.\n",
        " - Result Extension: Extend the `Result` object with new attributes and methods (e.g. `max_log_likelihood_model_data`).\n",
        " - Samples Filtering: Filter the `Samples` object to only contain samples fulfilling certain criteria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt\n",
        "\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "To illustrate results, we need to perform a model-fit in order to create a `Result` object.\n",
        "\n",
        "We do this below using the standard API and noisy 1D signal example, which you should be familiar with from other \n",
        "example scripts.\n",
        "\n",
        "Note that the `Gaussian` and `Analysis` classes come via the `af.ex` module, which contains example model components\n",
        "that are identical to those found throughout the examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "model = af.Model(af.ex.Gaussian)\n",
        "\n",
        "analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee(\n",
        "    name=\"cookbook_result\",\n",
        "    nwalkers=30,\n",
        "    nsteps=1000,\n",
        "    number_of_cores=1,\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Info__\n",
        "\n",
        "Printing the `info` attribute shows the overall result of the model-fit in a human readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Loading From Hard-disk__\n",
        "\n",
        "When performing fits which output results to hard-disk, a `files` folder is created containing .json / .csv files of \n",
        "the model, samples, search, etc. You should check it out now for a completed fit on your hard-disk if you have\n",
        "not already!\n",
        "\n",
        "These files can be loaded from hard-disk to Python variables via the aggregator, making them accessible in a \n",
        "Python script or Jupyter notebook. They are loaded as the internal **PyAutoFit** objects we are familiar with,\n",
        "for example the `model` is loaded as the `Model` object we passed to the search above.\n",
        "\n",
        "Below, we will access these results using the aggregator's `values` method. A full list of what can be loaded is\n",
        "as follows:\n",
        "\n",
        " - `model`: The `model` defined above and used in the model-fit (`model.json`).\n",
        " - `search`: The non-linear search settings (`search.json`).\n",
        " - `samples`: The non-linear search samples (`samples.csv`).\n",
        " - `samples_info`: Additional information about the samples (`samples_info.json`).\n",
        " - `samples_summary`: A summary of key results of the samples (`samples_summary.json`).\n",
        " - `info`: The info dictionary passed to the search (`info.json`).\n",
        " - `covariance`: The inferred covariance matrix (`covariance.csv`).\n",
        " - `data`: The 1D noisy data used that is fitted (`data.json`).\n",
        " - `noise_map`: The 1D noise-map fitted (`noise_map.json`).\n",
        " \n",
        "The `samples` and `samples_summary` results contain a lot of repeated information. The `samples` result contains\n",
        "the full non-linear search samples, for example every parameter sample and its log likelihood. The `samples_summary`\n",
        "contains a summary of the results, for example the maximum log likelihood model and error estimates on parameters\n",
        "at 1 and 3 sigma confidence.\n",
        "\n",
        "Accessing results via the `samples_summary` is much faster, because as it does not reperform calculations using the full \n",
        "list of samples. Therefore, if the result you want is accessible via the `samples_summary` you should use it\n",
        "but if not you can revert to the `samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.aggregator.aggregator import Aggregator\n",
        "\n",
        "agg = Aggregator.from_directory(\n",
        "    directory=path.join(\"output\", \"cookbook_result\"),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Generators__\n",
        "\n",
        "Before using the aggregator to inspect results, lets discuss Python generators. \n",
        "\n",
        "A generator is an object that iterates over a function when it is called. The aggregator creates all of the objects \n",
        "that it loads from the database as generators (as opposed to a list, or dictionary, or another Python type).\n",
        "\n",
        "This is because generators are memory efficient, as they do not store the entries of the database in memory \n",
        "simultaneously. This contrasts objects like lists and dictionaries, which store all entries in memory all at once. \n",
        "If you fit a large number of datasets, lists and dictionaries will use a lot of memory and could crash your computer!\n",
        "\n",
        "Once we use a generator in the Python code, it cannot be used again. To perform the same task twice, the \n",
        "generator must be remade it. This cookbook therefore rarely stores generators as variables and instead uses the \n",
        "aggregator to create each generator at the point of use.\n",
        "\n",
        "To create a generator of a specific set of results, we use the `values` method. This takes the `name` of the\n",
        "object we want to create a generator of, for example inputting `name=samples` will return the results `Samples`\n",
        "object (which is illustrated in detail below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples__\n",
        "\n",
        "The result contains a `Samples` object, which contains all samples of the non-linear search.\n",
        "\n",
        "Each sample corresponds to a set of model parameters that were evaluated and accepted by the non linear search, \n",
        "in this example `emcee.` \n",
        "\n",
        "This includes their log likelihoods, which are used for computing additional information about the model-fit,\n",
        "for example the error on every parameter. \n",
        "\n",
        "Our model-fit used the MCMC algorithm Emcee, so the `Samples` object returned is a `SamplesMCMC` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"MCMC Samples: \\n\")\n",
        "print(samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Parameters__\n",
        "\n",
        "The parameters are stored as a list of lists, where:\n",
        "\n",
        " - The outer list is the size of the total number of samples.\n",
        " - The inner list is the size of the number of free parameters in the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Sample 5's second parameter value (Gaussian -> normalization):\")\n",
        "print(samples.parameter_lists[4][1])\n",
        "print(\"Sample 10`s third parameter value (Gaussian -> sigma)\")\n",
        "print(samples.parameter_lists[9][2], \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Figures of Merit__\n",
        "\n",
        "The `Samples` class contains the log likelihood, log prior, log posterior and weight_list of every accepted sample, where:\n",
        "\n",
        "- The `log_likelihood` is the value evaluated in the `log_likelihood_function`.\n",
        "\n",
        "- The `log_prior` encodes information on how parameter priors map log likelihood values to log posterior values.\n",
        "\n",
        "- The `log_posterior` is `log_likelihood + log_prior`.\n",
        "\n",
        "- The `weight` gives information on how samples are combined to estimate the posterior, which depends on type of search\n",
        "  used (for `Emcee` they are all 1's meaning they are weighted equally).\n",
        "\n",
        "Lets inspect these values for the tenth sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"log(likelihood), log(prior), log(posterior) and weight of the tenth sample.\")\n",
        "print(samples.log_likelihood_list[9])\n",
        "print(samples.log_prior_list[9])\n",
        "print(samples.log_posterior_list[9])\n",
        "print(samples.weight_list[9])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Instances__\n",
        "\n",
        "Many results can be returned as an instance of the model, using the Python class structure of the model composition.\n",
        "\n",
        "For example, we can return the model parameters corresponding to the maximum log likelihood sample.\n",
        "\n",
        "The attributes of the `instance` (`centre`, `normalization` and `sigma`) have these names due to how we composed \n",
        "the `Gaussian` class via the `Model` above. They would be named structured and named differently if we hd \n",
        "used a `Collection` and different names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.max_log_likelihood()\n",
        "\n",
        "print(\"Max Log Likelihood `Gaussian` Instance:\")\n",
        "print(\"Centre = \", instance.centre)\n",
        "print(\"Normalization = \", instance.normalization)\n",
        "print(\"Sigma = \", instance.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This makes it straight forward to plot the median PDF model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = instance.model_data_1d_via_xvalues_from(xvalues=np.arange(data.shape[0]))\n",
        "\n",
        "plt.plot(range(data.shape[0]), data)\n",
        "plt.plot(range(data.shape[0]), model_data)\n",
        "plt.title(\"Illustrative model fit to 1D `Gaussian` profile data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Posterior / PDF__\n",
        "\n",
        "The result contains the full posterior information of our non-linear search, which can be used for parameter \n",
        "estimation. \n",
        "\n",
        "PDF stands for \"Probability Density Function\" and it quantifies probability of each model parameter having values\n",
        "that are sampled. It therefore enables error estimation via a process called marginalization.\n",
        "\n",
        "The median pdf vector is available, which estimates every parameter via 1D marginalization of their PDFs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.median_pdf()\n",
        "\n",
        "print(\"Median PDF `Gaussian` Instance:\")\n",
        "print(\"Centre = \", instance.centre)\n",
        "print(\"Normalization = \", instance.normalization)\n",
        "print(\"Sigma = \", instance.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Errors__\n",
        "\n",
        "Methods for computing error estimates on all parameters are provided. \n",
        "\n",
        "This again uses 1D marginalization, now at an input sigma confidence limit. \n",
        "\n",
        "By inputting `sigma=3.0` margnialization find the values spanning 99.7% of 1D PDF. Changing this to `sigma=1.0`\n",
        "would give the errors at the 68.3% confidence limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance_upper_sigma = samples.errors_at_upper_sigma(sigma=3.0)\n",
        "instance_lower_sigma = samples.errors_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Upper Error values (at 3.0 sigma confidence):\")\n",
        "print(\"Centre = \", instance_upper_sigma.centre)\n",
        "print(\"Normalization = \", instance_upper_sigma.normalization)\n",
        "print(\"Sigma = \", instance_upper_sigma.sigma, \"\\n\")\n",
        "\n",
        "print(\"lower Error values (at 3.0 sigma confidence):\")\n",
        "print(\"Centre = \", instance_lower_sigma.centre)\n",
        "print(\"Normalization = \", instance_lower_sigma.normalization)\n",
        "print(\"Sigma = \", instance_lower_sigma.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They can also be returned at the values of the parameters at their error values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance_upper_values = samples.values_at_upper_sigma(sigma=3.0)\n",
        "instance_lower_values = samples.values_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Upper Parameter values w/ error (at 3.0 sigma confidence):\")\n",
        "print(\"Centre = \", instance_upper_values.centre)\n",
        "print(\"Normalization = \", instance_upper_values.normalization)\n",
        "print(\"Sigma = \", instance_upper_values.sigma, \"\\n\")\n",
        "\n",
        "print(\"lower Parameter values w/ errors (at 3.0 sigma confidence):\")\n",
        "print(\"Centre = \", instance_lower_values.centre)\n",
        "print(\"Normalization = \", instance_lower_values.normalization)\n",
        "print(\"Sigma = \", instance_lower_values.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples Summary__\n",
        "\n",
        "The samples summary contains a subset of results access via the `Samples`, for example the maximum likelihood model\n",
        "and parameter error estimates.\n",
        "\n",
        "Using the samples method above can be slow, as the quantities have to be computed from all non-linear search samples\n",
        "(e.g. computing errors requires that all samples are marginalized over). This information is stored directly in the\n",
        "samples summary and can therefore be accessed instantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(samples.summary().max_log_likelihood_sample)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Sample Instance__\n",
        "\n",
        "A non-linear search retains every model that is accepted during the model-fit.\n",
        "\n",
        "We can create an instance of any model -- below we create an instance of the last accepted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.from_sample_index(sample_index=-1)\n",
        "\n",
        "print(\"Gaussian Instance of last sample\")\n",
        "print(\"Centre = \", instance.centre)\n",
        "print(\"Normalization = \", instance.normalization)\n",
        "print(\"Sigma = \", instance.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search Plots__\n",
        "\n",
        "The Probability Density Functions (PDF's) of the results can be plotted using the non-linear search in-built \n",
        "visualization tools.\n",
        "\n",
        "This fit used `Emcee` therefore we use the `MCMCPlotter` for visualization, which wraps the Python library `corner.py`.\n",
        "\n",
        "The `autofit_workspace/*/plots` folder illustrates other packages that can be used to make these plots using\n",
        "the standard output results formats (e.g. `GetDist.py`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.MCMCPlotter(samples=result.samples)\n",
        "plotter.corner_cornerpy()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Maximum Likelihood__\n",
        "\n",
        "The maximum log likelihood value of the model-fit can be estimated by simple taking the maximum of all log\n",
        "likelihoods of the samples.\n",
        "\n",
        "If different models are fitted to the same dataset, this value can be compared to determine which model provides\n",
        "the best fit (e.g. which model has the highest maximum likelihood)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Log Likelihood: \\n\")\n",
        "print(max(samples.log_likelihood_list))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Bayesian Evidence__\n",
        "\n",
        "If a nested sampling non-linear search is used, the evidence of the model is also available which enables Bayesian\n",
        "model comparison to be performed (given we are using Emcee, which is not a nested sampling algorithm, the log evidence \n",
        "is None).\n",
        "\n",
        "A full discussion of Bayesian model comparison is given in `autofit_workspace/*/features/bayes_model_comparison.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_evidence = samples.log_evidence\n",
        "print(f\"Log Evidence: {log_evidence}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Collection__\n",
        "\n",
        "The examples correspond to a model where `af.Model(Gaussian)` was used to compose the model.\n",
        "\n",
        "Below, we illustrate how the results API slightly changes if we compose our model using a `Collection`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.ex.Gaussian, exponential=af.ex.Exponential)\n",
        "\n",
        "analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee(\n",
        "    nwalkers=50,\n",
        "    nsteps=1000,\n",
        "    number_of_cores=1,\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `result.info` shows the result for the model with both a `Gaussian` and `Exponential` profile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Result instances again use the Python classes used to compose the model. \n",
        "\n",
        "However, because our fit uses a `Collection` the `instance` has attributes named according to the names given to the\n",
        "`Collection`, which above were `gaussian` and `exponential`.\n",
        "\n",
        "For complex models, with a large number of model components and parameters, this offers a readable API to interpret\n",
        "the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "instance = samples.max_log_likelihood()\n",
        "\n",
        "print(\"Max Log Likelihood `Gaussian` Instance:\")\n",
        "print(\"Centre = \", instance.gaussian.centre)\n",
        "print(\"Normalization = \", instance.gaussian.normalization)\n",
        "print(\"Sigma = \", instance.gaussian.sigma, \"\\n\")\n",
        "\n",
        "print(\"Max Log Likelihood Exponential Instance:\")\n",
        "print(\"Centre = \", instance.exponential.centre)\n",
        "print(\"Normalization = \", instance.exponential.normalization)\n",
        "print(\"Sigma = \", instance.exponential.rate, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Lists__\n",
        "\n",
        "All results can alternatively be returned as a 1D list of values, by passing `as_instance=False`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_lh_list = samples.max_log_likelihood(as_instance=False)\n",
        "print(\"Max Log Likelihood Model Parameters: \\n\")\n",
        "print(max_lh_list, \"\\n\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The list above does not tell us which values correspond to which parameters.\n",
        "\n",
        "The following quantities are available in the `Model`, where the order of their entries correspond to the parameters \n",
        "in the `ml_vector` above:\n",
        "\n",
        " - `paths`: a list of tuples which give the path of every parameter in the `Model`.\n",
        " - `parameter_names`: a list of shorthand parameter names derived from the `paths`.\n",
        " - `parameter_labels`: a list of parameter labels used when visualizing non-linear search results (see below).\n",
        "\n",
        "For simple models like the one fitted in this tutorial, the quantities below are somewhat redundant. For the\n",
        "more complex models they are important for tracking the parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = samples.model\n",
        "\n",
        "print(model.paths)\n",
        "print(model.parameter_names)\n",
        "print(model.parameter_labels)\n",
        "print(model.model_component_and_parameter_names)\n",
        "print(\"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the methods above are available as lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.median_pdf(as_instance=False)\n",
        "values_at_upper_sigma = samples.values_at_upper_sigma(sigma=3.0, as_instance=False)\n",
        "values_at_lower_sigma = samples.values_at_lower_sigma(sigma=3.0, as_instance=False)\n",
        "errors_at_upper_sigma = samples.errors_at_upper_sigma(sigma=3.0, as_instance=False)\n",
        "errors_at_lower_sigma = samples.errors_at_lower_sigma(sigma=3.0, as_instance=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Latex__\n",
        "\n",
        "If you are writing modeling results up in a paper, you can use inbuilt latex tools to create latex table code which \n",
        "you can copy to your .tex document.\n",
        "\n",
        "By combining this with the filtering tools below, specific parameters can be included or removed from the latex.\n",
        "\n",
        "Remember that the superscripts of a parameter are loaded from the config file `notation/label.yaml`, providing high\n",
        "levels of customization for how the parameter names appear in the latex table. This is especially useful if your model\n",
        "uses the same model components with the same parameter, which therefore need to be distinguished via superscripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "latex = af.text.Samples.latex(\n",
        "    samples=result.samples,\n",
        "    median_pdf_model=True,\n",
        "    sigma=3.0,\n",
        "    name_to_label=True,\n",
        "    include_name=True,\n",
        "    include_quickmath=True,\n",
        "    prefix=\"Example Prefix \",\n",
        "    suffix=\" \\\\[-2pt]\",\n",
        ")\n",
        "\n",
        "print(latex)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Derived Quantities__\n",
        "\n",
        "The parameters `centre`, `normalization` and `sigma` are the model parameters of the `Gaussian`. They are sampled\n",
        "directly by the non-linear search and we can therefore use the `Samples` object to easily determine their values and \n",
        "errors.\n",
        "\n",
        "Derived quantities (also called latent variables) are those which are not sampled directly by the non-linear search, \n",
        "but one may still wish to know their values and errors after the fit is complete. For example, what if we want the \n",
        "error on the full width half maximum (FWHM) of the Gaussian? \n",
        "\n",
        "This is achieved by adding each derived quantity to the `Model` objects with the `@derived_quantity` decorator, with\n",
        "an example for the `Gaussian` model component given below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre: float = 0.0,  # <- PyAutoFit recognises these constructor arguments\n",
        "        normalization: float = 0.1,  # <- are the Gaussian`s model parameters.\n",
        "        sigma: float = 0.01,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D `Gaussian` profile, which may be treated as a model-component of PyAutoFit the\n",
        "        parameters of which are fitted for by a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization normalisation of the `Gaussian` profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    @af.derived_quantity\n",
        "    def fwhm(self):\n",
        "        return 2.0 * np.sqrt(2.0 * np.log(2.0)) * self.sigma\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fit performed above used a `Gaussian` object a `fwhm` `derived_property`. \n",
        "\n",
        "This leads to a number of noteworthy outputs:\n",
        "\n",
        " - A `model.derived` file is output to the results folder, which includes the value and error of all derived quantities \n",
        "   based on the non-linear search samples (in this example only the `fwhm`).\n",
        "   \n",
        " - A `derived_quantities.csv` is output which lists every accepted sample's value of every derived quantity, which is again\n",
        "   analogous to the `samples.csv` file (in this example only the `fwhm`). \n",
        "     \n",
        " - A `derived_summary.json` is output which acts analogously to `samples_summary.json` but for the derived quantities \n",
        "   of the model (in this example only the `fwhm`).\n",
        "\n",
        "Derived quantities are also accessible via the `Samples` object, following a similar API to the model parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.max_log_likelihood()\n",
        "\n",
        "print(f\"Max Likelihood FWHM: {instance.fwhm}\")\n",
        "\n",
        "instance = samples.median_pdf()\n",
        "\n",
        "print(f\"Median PDF FWHM {instance.fwhm}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The summary of the derived quantities can be output using the `derived_quantities_summary_dict` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(samples.derived_quantities_summary_dict())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Derived Errors Manual (Advanced)__\n",
        "\n",
        "The derived quantities decorator above provides a simple interface for computing the errors of a derived quantity and\n",
        "ensuring all results are easily inspected in the output results folder.\n",
        "\n",
        "However, you may wish to compute the errors of a derived quantity manually. For example, if it is a quantity that \n",
        "you did not decorate before performing the fit, or if it is computationally expensive to compute and you only want\n",
        "to compute it specific circumstances.\n",
        "\n",
        "Below, we create the PDF of the derived quantity, the FWHM, manually, which we marginalize over using the same function \n",
        "we use to marginalize model parameters. We compute the FWHM of every accepted model sampled by the non-linear search \n",
        "and use this determine the PDF of the FWHM. \n",
        "\n",
        "When combining the FWHM's we weight each value by its `weight`. For Emcee, an MCMC algorithm, the weight of every \n",
        "sample is 1, but weights may take different values for other non-linear searches.\n",
        "\n",
        "In order to pass these samples to the function `marginalize`, which marginalizes over the PDF of the FWHM to compute \n",
        "its error, we also pass the weight list of the samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fwhm_list = []\n",
        "\n",
        "for sample in samples.sample_list:\n",
        "    instance = sample.instance_for_model(model=samples.model)\n",
        "\n",
        "    sigma = instance.gaussian.sigma\n",
        "\n",
        "    fwhm = 2 * np.sqrt(2 * np.log(2)) * sigma\n",
        "\n",
        "    fwhm_list.append(fwhm)\n",
        "\n",
        "median_fwhm, lower_fwhm, upper_fwhm = af.marginalize(\n",
        "    parameter_list=fwhm_list, sigma=3.0, weight_list=samples.weight_list\n",
        ")\n",
        "\n",
        "print(f\"FWHM = {median_fwhm} ({upper_fwhm} {lower_fwhm}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The calculation above could be computationally expensive, if there are many samples and the derived quantity is\n",
        "slow to compute.\n",
        "\n",
        "An alternative approach, which will provide comparable accuracy provided enough draws are used, is to sample \n",
        "points randomy from the PDF of the model and use these to compute the derived quantity.\n",
        "\n",
        "Draws are from the PDF of the model, so the weights of the samples are accounted for and we therefore do not\n",
        "pass them to the `marginalize` function (it essentially treats all samples as having equal weight)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "random_draws = 50\n",
        "\n",
        "fwhm_list = []\n",
        "\n",
        "for i in range(random_draws):\n",
        "    instance = samples.draw_randomly_via_pdf()\n",
        "\n",
        "    sigma = instance.gaussian.sigma\n",
        "\n",
        "    fwhm = 2 * np.sqrt(2 * np.log(2)) * sigma\n",
        "\n",
        "    fwhm_list.append(fwhm)\n",
        "\n",
        "median_fwhm, lower_fwhm, upper_fwhm = af.marginalize(\n",
        "    parameter_list=fwhm_list, sigma=3.0, weight_list=samples.weight_list\n",
        ")\n",
        "\n",
        "print(f\"fwhm = {median_fwhm} ({upper_fwhm} {lower_fwhm}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result Extensions (Advanced)__\n",
        "\n",
        "You might be wondering what else the results contains, as nearly everything we discussed above was a part of its \n",
        "`samples` property! The answer is, not much, however the result can be extended to include  model-specific results for \n",
        "your project. \n",
        "\n",
        "We detail how to do this in analysis cookbook, but for the example of fitting a 1D Gaussian we could extend\n",
        "the result to include the maximum log likelihood profile:\n",
        "\n",
        "(The commented out functions below are llustrative of the API we can create by extending a result)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# max_log_likelihood_profile = results.max_log_likelihood_profile"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples Filtering (Advanced)__\n",
        "\n",
        "Our samples object has the results for all three parameters in our model. However, we might only be interested in the\n",
        "results of a specific parameter.\n",
        "\n",
        "The basic form of filtering specifies parameters via their path, which was printed above via the model and is printed \n",
        "again below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Parameter paths in the model which are used for filtering:\")\n",
        "print(samples.model.paths)\n",
        "\n",
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "samples = samples.with_paths([(\"gaussian\", \"centre\")])\n",
        "\n",
        "print(\"All parameters of the very first sample (containing only the Gaussian centre.\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "print(\"Maximum Log Likelihood Model Instances (containing only the Gaussian centre):\\n\")\n",
        "print(samples.max_log_likelihood(as_instance=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We specified each path as a list of tuples of strings. \n",
        "\n",
        "This is how the source code internally stores the path to different components of the model, but it is not \n",
        "consistent with the API used to compose a model.\n",
        "\n",
        "We can alternatively use the following API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "samples = samples.with_paths([\"gaussian.centre\"])\n",
        "\n",
        "print(\"All parameters of the very first sample (containing only the Gaussian centre).\")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We filtered the `Samples` above by asking for all parameters which included the path (\"gaussian\", \"centre\").\n",
        "\n",
        "We can alternatively filter the `Samples` object by removing all parameters with a certain path. Below, we remove\n",
        "the Gaussian's `centre` to be left with 2 parameters; the `normalization` and `sigma`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Parameter paths in the model which are used for filtering:\")\n",
        "print(samples.model.paths)\n",
        "\n",
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "samples = samples.without_paths([\"gaussian.centre\"])\n",
        "\n",
        "print(\n",
        "    \"All parameters of the very first sample (containing only the Gaussian normalization and sigma).\"\n",
        ")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "Adding model complexity does not change the behaviour of the Result object, other than the switch\n",
        "to Collections meaning that our instances now have named entries.\n",
        "\n",
        "When you name your model components, you should make sure to give them descriptive and information names that make \n",
        "the use of a result object clear and intuitive!\n",
        "\n",
        "__Database__\n",
        "\n",
        "For large-scaling model-fitting problems to large datasets, the results of the many model-fits performed can be output\n",
        "and stored in a queryable sqlite3 database. The `Result` and `Samples` objects have been designed to streamline the \n",
        "analysis and interpretation of model-fits to large datasets using the database.\n",
        "\n",
        "Checkout the database cookbook for more details on how to use the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}