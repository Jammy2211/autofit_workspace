{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cookbook: Multiple Datasets\n",
        "===========================\n",
        "\n",
        "This cookbook illustrates how to fit multiple datasets simultaneously, where each dataset is fitted by a different\n",
        "`Analysis` class.\n",
        "\n",
        "The `Analysis` classes are combined to give an overall log likelihood function that is the sum of the\n",
        "individual log likelihood functions, which a single model is fitted to via non-linear search.\n",
        "\n",
        "If one has multiple observations of the same signal, it is often desirable to fit them simultaneously. This ensures\n",
        "that better constraints are placed on the model, as the full amount of information in the datasets is used.\n",
        "\n",
        "In some scenarios, the signal may vary across the datasets in a way that requires that the model is updated\n",
        "accordingly. **PyAutoFit** provides tools to customize the model composition such that specific parameters of the model\n",
        "vary across the datasets.\n",
        "\n",
        "This cookbook illustrates using observations of 3 1D Gaussians, which have the same `centre` (which is the same\n",
        "for the model fitted to each dataset) but different `normalization and `sigma` values (which vary for the model\n",
        "fitted to each dataset).\n",
        "\n",
        "It is common for each individual dataset to only constrain specific aspects of a model. The high level of model\n",
        "customization provided by **PyAutoFit** ensures that composing a model that is appropriate for fitting large and diverse\n",
        "datasets is straight forward. This is because different `Analysis` classes can be written for each dataset and combined.\n",
        "\n",
        "__Contents__\n",
        "\n",
        " - Model-Fit: Setup a model-fit to 3 datasets to illustrate multi-dataset fitting.\n",
        " - Analysis List: Create a list of `Analysis` objects, one for each dataset, which are fitted simultaneously.\n",
        " - Analysis Factor: Wrap each `Analysis` object in an `AnalysisFactor`, which pairs it with the model and prepares it for model fitting.\n",
        " - Factor Graph: Combine all `AnalysisFactor` objects into a `FactorGraphModel`, which represents a global model fit to multiple datasets.\n",
        " - Result List: Use the output of fits to multiple datasets which are a list of `Result` objects.\n",
        " - Variable Model Across Datasets: Fit a model where certain parameters vary across the datasets whereas others\n",
        "   stay fixed.\n",
        " - Relational Model: Fit models where certain parameters vary across the dataset as a user\n",
        "   defined relation (e.g. `y = mx + c`).\n",
        " - Different Analysis Classes: Fit multiple datasets where each dataset is fitted by a different `Analysis` class,\n",
        "   meaning that datasets with different formats can be fitted simultaneously.\n",
        " - Interpolation: Fit multiple datasets with a model one-by-one and interpolation over a smoothly varying parameter\n",
        "   (e.g. time) to infer the model between datasets.\n",
        " - Hierarchical / Graphical Models: Use hierarchical / graphical models to fit multiple datasets simultaneously,\n",
        "   which fit for global trends in the model across the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "Load 3 1D Gaussian datasets from .json files in the directory `autofit_workspace/dataset/`.\n",
        "\n",
        "All three datasets contain an identical signal, therefore fitting the same model to all three datasets simultaneously\n",
        "is appropriate.\n",
        "\n",
        "Each dataset has a different noise realization, therefore fitting them simultaneously will offer improved constraints \n",
        "over individual fits.\n",
        "\n",
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and\n",
        " `visualize` functions.\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        "These are functionally identical to the `Analysis` and `Gaussian` objects you have seen elsewhere in the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_size = 3\n",
        "\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "\n",
        "for dataset_index in range(dataset_size):\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", f\"gaussian_x1_identical_{dataset_index}\"\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    data_list.append(data)\n",
        "\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "    noise_map_list.append(noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot all 3 datasets, including their error bars. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    xvalues = range(data.shape[0])\n",
        "\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=data,\n",
        "        yerr=noise_map,\n",
        "        color=\"k\",\n",
        "        ecolor=\"k\",\n",
        "        linestyle=\"\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create our model corresponding to a single 1D Gaussian that is fitted to all 3 datasets simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(af.ex.Gaussian)\n",
        "\n",
        "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.normalization = af.LogUniformPrior(lower_limit=1e-2, upper_limit=1e2)\n",
        "model.sigma = af.GaussianPrior(\n",
        "    mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis List__\n",
        "\n",
        "Set up three instances of the `Analysis` class which fit 1D Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factor__\n",
        "\n",
        "Each analysis object is wrapped in an `AnalysisFactor`, which pairs it with the model and prepares it for use in a \n",
        "factor graph. This step allows us to flexibly define how each dataset relates to the model.\n",
        "\n",
        "The term \"Factor\" comes from factor graphs, a type of probabilistic graphical model. In this context, each factor \n",
        "represents the connection between one dataset and the shared model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for analysis in analysis_list:\n",
        "\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "All `AnalysisFactor` objects are combined into a `FactorGraphModel`, which represents a global model fit to \n",
        "multiple datasets using a graphical model structure.\n",
        "\n",
        "The key outcomes of this setup are:\n",
        "\n",
        " - The individual log likelihoods from each `Analysis` object are summed to form the total log likelihood \n",
        "   evaluated during the model-fitting process.\n",
        "   \n",
        " - Results from all datasets are output to a unified directory, with subdirectories for visualizations \n",
        "   from each analysis object, as defined by their `visualize` methods.\n",
        "\n",
        "This is a basic use of **PyAutoFit**'s graphical modeling capabilities, which support advanced hierarchical \n",
        "and probabilistic modeling for large, multi-dataset analyses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To inspect the model, we print `factor_graph.global_prior_model.info`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To fit multiple datasets, we pass the `FactorGraphModel` to a non-linear search.\n",
        "\n",
        "Unlike single-dataset fitting, we now pass the `factor_graph.global_prior_model` as the model and \n",
        "the `factor_graph` itself as the analysis object.\n",
        "\n",
        "This structure enables simultaneous fitting of multiple datasets in a consistent and scalable way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=\"features\", sample=\"rwalk\", name=\"multiple_datasets_simple\"\n",
        ")\n",
        "\n",
        "result_list = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result List__\n",
        "\n",
        "The result object returned by the fit is a list of the `Result` objects, which is described in the result cookbook.\n",
        "\n",
        "Each `Result` in the list corresponds to each `Analysis` object in the `analysis_list` we passed to the fit.\n",
        "\n",
        "The same model was fitted across all analyses, thus every `Result` in the `result_list` contains the same information \n",
        "on the samples and the same `max_log_likelihood_instance`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_list[0].max_log_likelihood_instance.centre)\n",
        "print(result_list[0].max_log_likelihood_instance.normalization)\n",
        "print(result_list[0].max_log_likelihood_instance.sigma)\n",
        "\n",
        "print(result_list[1].max_log_likelihood_instance.centre)\n",
        "print(result_list[1].max_log_likelihood_instance.normalization)\n",
        "print(result_list[1].max_log_likelihood_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the model-fit to each dataset by iterating over the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for data, result in zip(data_list, result_list):\n",
        "    instance = result.max_log_likelihood_instance\n",
        "\n",
        "    model_data = instance.model_data_from(xvalues=np.arange(data.shape[0]))\n",
        "\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=data,\n",
        "        yerr=noise_map,\n",
        "        linestyle=\"\",\n",
        "        color=\"k\",\n",
        "        ecolor=\"k\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.plot(xvalues, model_data, color=\"r\")\n",
        "    plt.title(\"Dynesty model fit to 1D Gaussian dataset.\")\n",
        "    plt.xlabel(\"x values of profile\")\n",
        "    plt.ylabel(\"Profile normalization\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Variable Model Across Datasets__\n",
        "\n",
        "The same model was fitted to every dataset simultaneously because all 3 datasets contained an identical signal with \n",
        "only the noise varying across the datasets.\n",
        "\n",
        "If the signal varied across the datasets, we would instead want to fit a different model to each dataset. The model\n",
        "composition can be updated by changing the model passed to each `AnalysisFactor`.\n",
        "\n",
        "We will use an example of 3 1D Gaussians which have the same `centre` but the `normalization` and `sigma` vary across \n",
        "datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1_variable\")\n",
        "\n",
        "dataset_name_list = [\"sigma_0\", \"sigma_1\", \"sigma_2\"]\n",
        "\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "\n",
        "for dataset_name in dataset_name_list:\n",
        "    dataset_time_path = path.join(dataset_path, dataset_name)\n",
        "\n",
        "    data = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_time_path, \"data.json\")\n",
        "    )\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_time_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    data_list.append(data)\n",
        "    noise_map_list.append(noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting these datasets shows that the `normalization` and` `sigma` of each Gaussian vary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    xvalues = range(data.shape[0])\n",
        "\n",
        "    af.ex.plot_profile_1d(xvalues=xvalues, profile_1d=data)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `centre` of all three 1D Gaussians are the same in each dataset, but their `normalization` and `sigma` values \n",
        "are decreasing.\n",
        "\n",
        "We will therefore fit a model to all three datasets simultaneously, whose `centre` is the same for all 3 datasets but\n",
        "the `normalization` and `sigma` vary.\n",
        "\n",
        "To do that, we use a summed list of `Analysis` objects, where each `Analysis` object contains a different dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now update the model passed to each `AnalysisFactor `object to compose a model where: \n",
        "\n",
        " - The `centre` values of the Gaussian fitted to every dataset in every `Analysis` object are identical. \n",
        "\n",
        " - The`normalization` and `sigma` value of the every Gaussian fitted to every dataset in every `Analysis` object \n",
        "   are different.\n",
        "\n",
        "The model has 7 free parameters in total, x1 shared `centre`, x3 unique `normalization`'s and x3 unique `sigma`'s.\n",
        "\n",
        "We do this by overwriting the `normalization` and `sigma` variables of the model passed to each `AnalysisFactor` object\n",
        "with new priors, that make them free parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for analysis in analysis_list:\n",
        "\n",
        "    model_analysis = model.copy()\n",
        "\n",
        "    model_analysis.normalization = af.LogUniformPrior(lower_limit=1e-2, upper_limit=1e2)\n",
        "    model_analysis.sigma = af.GaussianPrior(\n",
        "        mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf\n",
        "    )\n",
        "\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model_analysis, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To inspect this model, with extra parameters for each dataset created, we print `factor_graph.global_prior_model.info`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)\n",
        "\n",
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit this model to the data using dynesty."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=\"features\", sample=\"rwalk\", name=\"multiple_datasets_free_sigma\"\n",
        ")\n",
        "\n",
        "result_list = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `normalization` and `sigma` values of the maximum log likelihood models fitted to each dataset are different, \n",
        "which is shown by printing the `sigma` values of the maximum log likelihood instances of each result.\n",
        "\n",
        "The `centre` values of the maximum log likelihood models fitted to each dataset are the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for result in result_list:\n",
        "    instance = result.max_log_likelihood_instance\n",
        "\n",
        "    print(\"Max Log Likelihood Model:\")\n",
        "    print(\"Centre = \", instance.centre)\n",
        "    print(\"Normalization = \", instance.normalization)\n",
        "    print(\"Sigma = \", instance.sigma)\n",
        "    print()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Relational Model__\n",
        "\n",
        "In the model above, two extra free parameters (`normalization and `sigma`) were added for every dataset. \n",
        "\n",
        "For just 3 datasets the model stays low dimensional and this is not a problem. However, for 30+ datasets the model\n",
        "will become complex and difficult to fit.\n",
        "\n",
        "In these circumstances, one can instead compose a model where the parameters vary smoothly across the datasets\n",
        "via a user defined relation.\n",
        "\n",
        "Below, we compose a model where the `sigma` value fitted to each dataset is computed according to:\n",
        "\n",
        " `y = m * x + c` : `sigma` = sigma_m * x + sigma_c`\n",
        "\n",
        "Where x is an integer number specifying the index of the dataset (e.g. 1, 2 and 3).\n",
        "\n",
        "By defining a relation of this form, `sigma_m` and `sigma_c` are the only free parameters of the model which vary\n",
        "across the datasets. \n",
        "\n",
        "Of more datasets are added the number of model parameters therefore does not increase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.Model(af.ex.Gaussian))\n",
        "\n",
        "sigma_m = af.UniformPrior(lower_limit=-10.0, upper_limit=10.0)\n",
        "sigma_c = af.UniformPrior(lower_limit=-10.0, upper_limit=10.0)\n",
        "\n",
        "x_list = [1.0, 2.0, 3.0]\n",
        "\n",
        "analysis_factor_list = []\n",
        "\n",
        "for x, analysis in zip(x_list, analysis_list):\n",
        "    sigma_relation = (sigma_m * x) + sigma_c\n",
        "\n",
        "    model_analysis = model.copy()\n",
        "    model_analysis.gaussian.sigma = sigma_relation\n",
        "\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model_analysis, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The factor graph is created and its info can be printed after the relational model has been defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)\n",
        "\n",
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can fit the model as per usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=\"features\", sample=\"rwalk\", name=\"multiple_datasets_relation\"\n",
        ")\n",
        "\n",
        "result_list = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `centre` and `sigma` values of the maximum log likelihood models fitted to each dataset are different, \n",
        "which is shown by printing the `sigma` values of the maximum log likelihood instances of each result.\n",
        "\n",
        "They now follow the relation we defined above.\n",
        "\n",
        "The `centre` normalization of the maximum log likelihood models fitted to each dataset are the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for result in result_list:\n",
        "    instance = result.max_log_likelihood_instance\n",
        "\n",
        "    print(\"Max Log Likelihood Model:\")\n",
        "    print(\"Centre = \", instance.gaussian.centre)\n",
        "    print(\"Normalization = \", instance.gaussian.normalization)\n",
        "    print(\"Sigma = \", instance.gaussian.sigma)\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Different Analysis Objects__\n",
        "\n",
        "For simplicity, this example used a single `Analysis` class which fitted 1D Gaussian's to 1D data.\n",
        "\n",
        "For many problems one may have multiple datasets which are quite different in their format and structure. In this \n",
        "situation, one can simply define unique `Analysis` objects for each type of dataset, which will contain a \n",
        "unique `log_likelihood_function` and methods for visualization.\n",
        "\n",
        "__Hierarchical / Graphical Models__\n",
        "\n",
        "The analysis factor API illustrated here can then be used to fit this large variety of datasets, noting that the \n",
        "the model can also be customized as necessary for fitting models to multiple datasets that are different in their \n",
        "format and structure. \n",
        "\n",
        "This allows us to fit large heterogeneous datasets simultaneously, but also forms the basis of the graphical\n",
        "modeling API which can be used to fit complex models, such as hierarchical models, to extract more information\n",
        "from large datasets.\n",
        "\n",
        "**PyAutoFit** has a dedicated feature set for fitting hierarchical and graphical models and interested readers should\n",
        "checkout the hierarchical and graphical modeling \n",
        "chapter of **HowToFit** (https://pyautofit.readthedocs.io/en/latest/howtofit/chapter_graphical_models.html)\n",
        "\n",
        "__Interpolation__\n",
        "\n",
        "One may have many datasets which vary according to a smooth function, for example a dataset taken over time where\n",
        "the signal varies smoothly as a function of time.\n",
        "\n",
        "This could be fitted using the tools above, all at once. However, in many use cases this is not possible due to the\n",
        "model complexity, number of datasets or computational time.\n",
        "\n",
        "An alternative approach is to fit each dataset individually, and then interpolate the results over the smoothly\n",
        "varying parameter (e.g. time) to estimate the model parameters at any point.\n",
        "\n",
        "**PyAutoFit** has interpolation tools to do exactly this, which are described in the `features/interpolation.ipynb`\n",
        "example.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "We have shown how **PyAutoFit** can fit large datasets simultaneously, using custom models that vary specific\n",
        "parameters across the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}