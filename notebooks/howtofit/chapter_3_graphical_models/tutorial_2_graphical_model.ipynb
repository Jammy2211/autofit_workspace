{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 2: Graphical Models\n",
        "============================\n",
        "\n",
        "We have fitted a dataset containing 5 noisy 1D Gaussian which had a shared `centre` value. We estimated\n",
        "the `centre` by fitting each dataset individually and combining the value of the `centre` inferred by each fit into\n",
        "an overall estimate, using a weighted average.\n",
        "\n",
        "Graphical models use a different approach. They are a single model that is fitted to the entire dataset simultaneously. \n",
        "The model includes specific model component for every individual 1D Gaussian in the sample. However, the graphical \n",
        "model also has shared parameters between these individual model components.\n",
        "\n",
        "This example fits a graphical model using the same sample fitted in the previous tutorial, consisting of many 1D \n",
        "Gaussians. However, whereas previously the `centre` of each Gaussian was a free parameter in each fit, in the graphical \n",
        "model there is only a single parameter for the `centre` shared by all 1D Gaussians.\n",
        "\n",
        "This graphical model creates a non-linear parameter space with parameters for every Gaussian in our sample. For 5\n",
        "Gaussians each with their own model parameters but a single shared centre:\n",
        "\n",
        " - Each Gaussian has 2 free parameters from the components that are not shared (`normalization`, `sigma`).\n",
        " - There is one additional free parameter, which is the `centre` shared by all 5 Gaussians."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and \n",
        " `visualize` functions.\n",
        " \n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        " - `plot_profile_1d`: a function for plotting 1D profile datasets including their noise.\n",
        "\n",
        "These are functionally identical to the `Analysis`, `Gaussian` and `plot_profile_1d` objects and functions you \n",
        "have seen and used elsewhere throughout the workspace.\n",
        "\n",
        "__Dataset__\n",
        "\n",
        "For each dataset we now set up the correct path and load it. \n",
        "\n",
        "Whereas in the previous tutorial we fitted each dataset one-by-one, in this tutorial we instead store each dataset \n",
        "in a list so that we can set up a single model-fit that fits the 5 datasets simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_datasets = 5\n",
        "\n",
        "dataset_name_list = []\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", \"gaussian_x1__low_snr\", dataset_name\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    dataset_name_list.append(dataset_name)\n",
        "    data_list.append(data)\n",
        "    noise_map_list.append(noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting the Gaussians we can remind ourselves that determining their centres by eye is difficult."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name, data in zip(dataset_name_list, data_list):\n",
        "    af.ex.plot_profile_1d(\n",
        "        xvalues=np.arange(data.shape[0]),\n",
        "        profile_1d=data,\n",
        "        title=dataset_name,\n",
        "        ylabel=\"Data Values\",\n",
        "        color=\"k\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "For each dataset we now create a corresponding `Analysis` class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We now compose the graphical model that we fit, using the `Model` object you are now familiar with.\n",
        "\n",
        "We begin by setting up a shared prior for `centre`. \n",
        "\n",
        "We set up this up as a single `GaussianPrior` which is passed to separate `Model`'s for each `Gaussian` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "centre_shared_prior = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now set up a list of `Model`'s, each of which contain a `Gaussian` that is used to fit each of the datasets \n",
        "loaded above.\n",
        "\n",
        "All of these models use the `centre_shared_prior`, meaning that all model-components use the same value of `centre` \n",
        "for every individual model component. \n",
        "\n",
        "For a fit using five Gaussians, this reduces the dimensionality of parameter space from N=15 (e.g. 3 parameters per \n",
        "Gaussian) to N=11 (e.g. 5 `sigma`'s 5 `normalizations` and 1 `centre`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_list = []\n",
        "\n",
        "for model_index in range(len(data_list)):\n",
        "    gaussian = af.Model(af.ex.Gaussian)\n",
        "\n",
        "    gaussian.centre = centre_shared_prior  # This prior is used by all 3 Gaussians!\n",
        "    gaussian.normalization = af.LogUniformPrior(lower_limit=1e-6, upper_limit=1e6)\n",
        "    gaussian.sigma = af.UniformPrior(lower_limit=0.0, upper_limit=25.0)\n",
        "\n",
        "    model_list.append(gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factors__\n",
        "\n",
        "Above, we composed a model consisting of three `Gaussian`'s with a shared `centre` prior. We also loaded three datasets\n",
        "which we intend to fit with each of these `Gaussians`, setting up each in an `Analysis` class that defines how the \n",
        "model is used to fit the data.\n",
        "\n",
        "We now simply pair each model-component to each `Analysis` class, so that:\n",
        "\n",
        "- `gaussian_0` fits `data_0` via `analysis_0`.\n",
        "- `gaussian_1` fits `data_1` via `analysis_1`.\n",
        "- `gaussian_2` fits `data_2` via `analysis_2`.\n",
        "\n",
        "The point where a `Model` and `Analysis` class meet is called an `AnalysisFactor`. \n",
        "\n",
        "This term denotes that we are composing a graphical model, which is commonly called a 'factor graph'. A  factor \n",
        "defines a node on this graph where we have some data, a model, and we fit the two together. The 'links' between these \n",
        "different nodes then define the global model we are fitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "We now combine our `AnalysisFactor`'s to compose a factor graph.\n",
        "\n",
        "What is a factor graph? A factor graph defines the graphical model's graph. For example, it defines the different \n",
        "model components that make up our model (e.g. the individual `Gaussian` classes) and how their parameters are linked or \n",
        "shared (e.g. that each `Gaussian` has its own unique `normalization` and `sigma`, but a shared `centre` parameter).\n",
        "\n",
        "This is what our factor graph looks like (visualization of graphs not implemented yet): \n",
        "\n",
        "The factor graph above is made up of two components:\n",
        "\n",
        "- Nodes: these are points on the graph where we have a unique set of data and a model that is made up of a subset of \n",
        "our overall graphical model. This is effectively the `AnalysisFactor` objects we created above. \n",
        "\n",
        "- Links: these define the model components and parameters that are shared across different nodes and thus retain the \n",
        "same values when fitting different datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fit will use the factor graph's `global_prior_model`, which uses the models contained in every analysis factor \n",
        "to contrast the overall global model that is fitted.\n",
        "\n",
        "Printing the `info` attribute of this model reveals the overall structure of the model, which is grouped in terms\n",
        "of the analysis factors and therefore datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "We can now create a non-linear search and use it to the fit the factor graph, using its `global_prior_model` property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"howtofit\", \"chapter_graphical_models\"),\n",
        "    name=\"tutorial_2_graphical_model\",\n",
        "    nlive=200,\n",
        "    dlogz=1e-4,\n",
        "    sample=\"rwalk\",\n",
        "    walks=10,\n",
        ")\n",
        "\n",
        "result = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result's `info` attribute shows that the result is expressed following the same struture of analysis factors\n",
        "that the `global_prior_model.info` attribute revealed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now inspect the inferred value of `centre`, and compare this to the value we estimated in the previous tutorial\n",
        "via a weighted average or posterior multiplicaition using KDE.(feature missing currently). \n",
        "\n",
        "(The errors of the weighted average and KDE below is what was estimated for a run on my PC, yours may be slightly \n",
        "different!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    f\"Weighted Average Centre Estimate = 48.535531422571886 (4.139907734505303) [1.0 sigma confidence intervals] \\n\"\n",
        ")\n",
        "\n",
        "centre = result.samples.median_pdf()[0].centre\n",
        "\n",
        "u1_error = result.samples.values_at_upper_sigma(sigma=1.0)[0].centre\n",
        "l1_error = result.samples.values_at_lower_sigma(sigma=1.0)[0].centre\n",
        "\n",
        "u3_error = result.samples.values_at_upper_sigma(sigma=3.0)[0].centre\n",
        "l3_error = result.samples.values_at_lower_sigma(sigma=3.0)[0].centre\n",
        "\n",
        "print(\"Inferred value of the shared centre via a graphical model fit: \\n\")\n",
        "print(f\"{centre} ({l1_error} {u1_error}) [1.0 sigma confidence intervals]\")\n",
        "print(f\"{centre} ({l3_error} {u3_error}) [3.0 sigma confidence intervals]\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graphical model's centre estimate and errors are pretty much exactly the same as the weighted average or KDE!\n",
        "\n",
        "Whats the point of fitting a graphical model if the much simpler approach of the previous tutorial gives the\n",
        "same answer? \n",
        "\n",
        "The answer, is model complexity. Graphical models become more powerful as we make our model more complex,\n",
        "our non-linear parameter space higher dimensionality and the degeneracies between different parameters on the graph\n",
        "more significant. \n",
        "\n",
        "We will demonstrate this in the next tutorial.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "In this tutorial, we showed that for our extremely simple model the graphical model gives pretty much the\n",
        "same estimate of the 1D Gaussian centre's as simpler approaches followed in the previous tutorial. \n",
        "\n",
        "We will next show the strengths of graphical models by fitting more complex models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}