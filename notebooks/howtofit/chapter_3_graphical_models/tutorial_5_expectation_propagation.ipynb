{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 5: Expectation Propagation\n",
        "===================================\n",
        "\n",
        "In the previous tutorials, we fitted graphical models to dataset comprising many noisy 1D Gaussians. These had a shared\n",
        "and global value of their `centre`, or assumed their centres were hierarchically drawn from a parent Gaussian\n",
        "distribution. This provides the basis of composing and fitting complex graphical models to large datasets.\n",
        "\n",
        "We concluded by discussing that there is a ceiling scaling these graphical models up to extremely large datasets. One\n",
        "would soon find that the parameter space is too complex to sample, and computational limits would ultimately cap how\n",
        "many datasets one could feasibly fit.\n",
        "\n",
        "This tutorial introduces expectation propagation (EP), the solution to this problem, which inspects a factor graph\n",
        "and partitions the model-fit into many simpler fits of sub-components of the graph to individual datasets. This\n",
        "overcomes the challenge of model complexity, and mitigates computational restrictions that may occur if one tries to\n",
        "fit every dataset simultaneously.\n",
        "\n",
        "This tutorial fits a global model with a shared parameter and does not use a hierarchical model. The optional tutorial\n",
        "`tutorial_optional_hierarchical_ep` shows an example fit of a hierarchical model with EP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and \n",
        " `visualize` functions.\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        " - `plot_profile_1d`: a function for plotting 1D profile datasets including their noise.\n",
        "\n",
        "These are functionally identical to the `Analysis`, `Gaussian` and `plot_profile_1d` objects and functions you have seen \n",
        "and used elsewhere throughout the workspace.\n",
        "\n",
        "__Dataset__\n",
        "\n",
        "For each dataset we now set up the correct path and load it. \n",
        "\n",
        "We first fit the 1D Gaussians which all share the same centre, thus not requiring a hierarchical model. \n",
        "\n",
        "An example for fitting the hierarchical model with EP is given at the end of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_datasets = 5\n",
        "\n",
        "dataset_name_list = []\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", \"gaussian_x1__low_snr\", dataset_name\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    dataset_name_list.append(dataset_name)\n",
        "    data_list.append(data)\n",
        "    noise_map_list.append(noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting the Gaussians we can remind ourselves that determining their centres by eye is difficult."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name, data in zip(dataset_name_list, data_list):\n",
        "    af.ex.plot_profile_1d(\n",
        "        xvalues=np.arange(data.shape[0]),\n",
        "        profile_1d=data,\n",
        "        title=dataset_name,\n",
        "        ylabel=\"Data Values\",\n",
        "        color=\"k\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "For each dataset we now create a corresponding `Analysis` class, like in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We now compose the graphical model that we fit, using the `Model` and `Collection` objects you are now familiar with.\n",
        "\n",
        "We will assume all Gaussians share the same centre, therefore we set up a shared prior for `centre`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "centre_shared_prior = af.GaussianPrior(mean=50.0, sigma=30.0)\n",
        "\n",
        "model_list = []\n",
        "\n",
        "for model_index in range(len(data_list)):\n",
        "    gaussian = af.Model(af.ex.Gaussian)\n",
        "\n",
        "    gaussian.centre = centre_shared_prior  # This prior is used by all 3 Gaussians!\n",
        "\n",
        "    gaussian.normalization = af.GaussianPrior(mean=3.0, sigma=5.0, lower_limit=0.0)\n",
        "    gaussian.sigma = af.GaussianPrior(mean=10.0, sigma=10.0, lower_limit=0.0)\n",
        "\n",
        "    model = af.Collection(gaussian=gaussian)\n",
        "\n",
        "    model_list.append(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factors__\n",
        "\n",
        "Now we have our `Analysis` classes and graphical model, we can compose our `AnalysisFactor`'s.\n",
        "\n",
        "However, unlike the previous tutorials, each `AnalysisFactor` is now assigned its own `search`. This is because the EP \n",
        "framework performs a model-fit to each node on the factor graph (e.g. each `AnalysisFactor`). Therefore, each node \n",
        "requires its own non-linear search, and in this tutorial we use `dynesty`. For complex graphs consisting of many \n",
        "nodes, one could easily use different searches for different nodes on the factor graph.\n",
        "\n",
        "Each `AnalysisFactor` is also given a `name`, corresponding to the name of the dataset it fits. These names are used\n",
        "to name the folders containing the results in the output directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "paths = af.DirectoryPaths(\n",
        "    name=path.join(\n",
        "        \"howtofit\", \"chapter_graphical_models\", \"tutorial_5_expectation_propagation\"\n",
        "    )\n",
        ")\n",
        "\n",
        "search = af.DynestyStatic(paths=paths, nlive=100, sample=\"rwalk\")\n",
        "\n",
        "analysis_factor_list = []\n",
        "\n",
        "dataset_index = 0\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "    dataset_index += 1\n",
        "\n",
        "    analysis_factor = af.AnalysisFactor(\n",
        "        prior_model=model, analysis=analysis, optimiser=search, name=dataset_name\n",
        "    )\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "We combine our `AnalysisFactors` into one, to compose the factor graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The factor graph model `info` attribute shows the model which we fit via expectaton propagation (note that we do\n",
        "not use `global_prior_model` below when performing the fit)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Expectation Propagation__\n",
        "\n",
        "In the previous tutorials, we used the `global_prior_model` of the `factor_graph` to fit the global model. In this \n",
        "tutorial, we instead fit the `factor_graph` using the EP framework, which fits the graphical model composed in this \n",
        "tutorial as follows:\n",
        "\n",
        "1) Go to the first node on the factor graph (e.g. `analysis_factor_list[0]`) and fit its model to its dataset. This is \n",
        "simply a fit of the `Gaussian` model to the first 1D Gaussian dataset, the model-fit we are used to performing by now.\n",
        "\n",
        "2) Once the model-fit is complete, inspect the model for parameters that are shared with other nodes on the factor\n",
        "graph. In this example, the `centre` of the `Gaussian` fitted to the first dataset is global, and therefore connects\n",
        "to the other nodes on the factor graph (the `AnalysisFactor`'s) of the second and first `Gaussian` datasets.\n",
        "\n",
        "3) The EP framework now creates a 'message' that is to be passed to the connecting nodes on the factor graph. This\n",
        "message informs them of the results of the model-fit, so they can update their priors on the `Gaussian`'s centre \n",
        "accordingly and, more importantly, update their posterior inference and therefore estimate of the global centre.\n",
        "\n",
        "For example, the model fitted to the first Gaussian dataset includes the global centre. Therefore, after the model is \n",
        "fitted, the EP framework creates a 'message' informing the factor graph about its inference on that Gaussians's centre,\n",
        "thereby updating our overall inference on this shared parameter. This is termed 'message passing'.\n",
        "\n",
        "__Cyclic Fitting__\n",
        "\n",
        "After every `AnalysisFactor` has been fitted (e.g. after each fit to each of the 5 datasets in this example), we have a \n",
        "new estimate of the shared parameter `centre`. This updates our priors on the shared parameter `centre`, which needs \n",
        "to be reflected in each model-fit we perform on each `AnalysisFactor`. \n",
        "\n",
        "The EP framework therefore performs a second iteration of model-fits. It again cycles through each `AnalysisFactor` \n",
        "and refits the model, using updated priors on shared parameters like the `centre`. At the end of each fit, we again \n",
        "create messages that update our knowledge about other parameters on the graph.\n",
        "\n",
        "This process is repeated multiple times, until a convergence criteria is met whereby continued cycles are expected to\n",
        "produce the same estimate of the shared parameter `centre`. \n",
        "\n",
        "When we fit the factor graph a `name` is passed, which determines the folder all results of the factor graph are\n",
        "stored in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "laplace = af.LaplaceOptimiser()\n",
        "\n",
        "factor_graph_result = factor_graph.optimise(\n",
        "    optimiser=laplace, paths=paths, ep_history=af.EPHistory(kl_tol=0.05), max_steps=5\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "An `info` attribute for the result of a factor graph fitted via EP does not exist yet, its on the to do list!\n",
        "\n",
        "The result can be seen in the `graph.result` file output to hard-disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "##print(factor_graph_result.info)##"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output__\n",
        "\n",
        "The results of the factor graph, using the EP framework and message passing, are contained in the folder \n",
        "`output/howtofit/chapter_graphical_models/tutorial_5_expectation_propagation`. \n",
        "\n",
        "The following folders and files are worth of note:\n",
        "\n",
        " - `graph.info`: this provides an overall summary of the graphical model that is fitted, including every parameter, \n",
        " how parameters are shared across `AnalysisFactor`'s and the priors associated to each individual parameter.\n",
        " \n",
        " - The 3 folders titled `gaussian_x1_#__low_snr` correspond to the three `AnalysisFactor`'s and therefore signify \n",
        " repeated non-linear searches that are performed to fit each dataset.\n",
        " \n",
        " - Inside each of these folders are `optimization_#` folders, corresponding to each model-fit performed over cycles of\n",
        " the EP fit. A careful inspection of the `model.info` files inside each folder reveals how the priors are updated\n",
        " over each cycle, whereas the `model.results` file should indicate the improved estimate of model parameters over each\n",
        " cycle.\n",
        "\n",
        "__Results__\n",
        "\n",
        "The `MeanField` object represent the posterior of the entire factor graph and is used to infer estimates of the \n",
        "values and error of each parameter in the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mean_field = factor_graph_result.updated_ep_mean_field.mean_field\n",
        "print(mean_field)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The object has a `variables` property which lists every variable in the factor graph, which is essentially all of the \n",
        "free parameters on the graph.\n",
        "\n",
        "This includes the parameters specific to each data (E.g. each node on the graph) as well as the shared centre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(mean_field.variables)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variables above use the priors on each parameter as their key. \n",
        "\n",
        "Therefore to estimate mean-field quantities of the shared centre, we can simply use the `centre_shared_prior` defined\n",
        "above.\n",
        "\n",
        "Each parameter estimate is given by the mean of its value in the `MeanField`. Below, we use the `centred_shared_prior` \n",
        "as a key to the `MeanField.mean` dictionary to print the estimated value of the shared centre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Centre Mean Parameter Estimate = {mean_field.mean[centre_shared_prior]}\")\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want the parameter estimate of another parameter in the model, we can use the `model_list` that we composed \n",
        "above to pass a parameter prior to the mean field dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    f\"Normalization Gaussian Dataset 0 Mean = {mean_field.mean[model_list[0].gaussian.normalization]}\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mean-field mean dictionary contains the estimate value of every parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"All Parameter Estimates = {mean_field.mean}\")\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mean-field also contains a `variance` dictionary, which has the same keys as the `mean` dictionary above. \n",
        "\n",
        "This is the easier way to estimate the error on every parameter, for example that of the shared centre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Centre Variance = {mean_field.variance[centre_shared_prior]}\")\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The standard deviation (or error at one sigma confidence interval) is given by the square root of the variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Centre 1 Sigma = {np.sqrt(mean_field.variance[centre_shared_prior])}\")\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mean field object also contains a dictionary of the s.d./variance**0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Centre SD/sqrt(variance) = {mean_field.scale[centre_shared_prior]}\")\n",
        "print()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}