{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial Optional: Hierarchical Individual\n",
        "==========================================\n",
        "\n",
        "In tutorial 4, we fit a hierarchical model using a graphical model, whereby all datasets are fitted simultaneously\n",
        "and the hierarchical parameters are fitted for simultaneously with the model parameters of each 1D Gaussian in each\n",
        "dataset.\n",
        "\n",
        "This script illustrates how the hierarchical parameters can be estimated using a simpler approach, which fits\n",
        "each dataset one-by-one and estimates the hierarchical parameters afterwards by fitting the inferred `centres`\n",
        "with a Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and \n",
        " `visualize` functions.\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        " - `plot_profile_1d`: a function for plotting 1D profile datasets including their noise.\n",
        "\n",
        "These are functionally identical to the `Analysis`, `Gaussian` and `plot_profile_1d` objects and functions you \n",
        "have seen and used elsewhere throughout the workspace.\n",
        "\n",
        "__Dataset__\n",
        "\n",
        "For each dataset we now set up the correct path and load it. \n",
        "\n",
        "We are loading a new Gaussian dataset, where the Gaussians have different centres which were drawn from a parent\n",
        "Gaussian distribution with a mean centre value of 50.0 and sigma of 10.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_datasets = 5\n",
        "\n",
        "dataset_name_list = []\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", \"gaussian_x1__hierarchical\", dataset_name\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    dataset_name_list.append(dataset_name)\n",
        "    data_list.append(data)\n",
        "    noise_map_list.append(noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting the Gaussians we can just about make out that their centres are not all at pix 50, and are spread out\n",
        "around it (albeit its difficult to be sure, due to the low signal-to-noise of the data). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name, data in zip(dataset_name_list, data_list):\n",
        "    af.ex.plot_profile_1d(\n",
        "        xvalues=np.arange(data.shape[0]),\n",
        "        profile_1d=data,\n",
        "        title=dataset_name,\n",
        "        ylabel=\"Data Values\",\n",
        "        color=\"k\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "For each dataset we now create a corresponding `Analysis` class, like in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    analysis_list.append(analysis)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "The model we fit to each dataset, which is a simple 1D Gaussian with all 3 parameters free."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian = af.Model(af.ex.Gaussian)\n",
        "\n",
        "gaussian.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "gaussian.normalization = af.UniformPrior(lower_limit=0.0, upper_limit=10.0)\n",
        "gaussian.sigma = af.UniformPrior(lower_limit=0.0, upper_limit=50.0)\n",
        "\n",
        "model = af.Collection(gaussian=gaussian)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fits (one-by-one)__\n",
        "\n",
        "For every dataset we now create an `Analysis` class using it and use `Dynesty` to fit it with a `Gaussian`.\n",
        "\n",
        "The `Result` is stored in the list `results`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_list = []\n",
        "\n",
        "for dataset_name, analysis in zip(dataset_name_list, analysis_list):\n",
        "    \"\"\"\n",
        "    Create the `DynestyStatic` non-linear search and use it to fit the data.\n",
        "    \"\"\"\n",
        "    dynesty = af.DynestyStatic(\n",
        "        name=\"tutorial_optional_hierarchical_individual\",\n",
        "        unique_tag=dataset_name,\n",
        "        nlive=200,\n",
        "        dlogz=1e-4,\n",
        "        sample=\"rwalk\",\n",
        "        walks=10,\n",
        "    )\n",
        "\n",
        "    result_list.append(dynesty.fit(model=model, analysis=analysis))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Results__\n",
        "\n",
        "Checkout the output folder, you should see three new sets of results corresponding to our 3 `Gaussian` datasets.\n",
        "\n",
        "The `result_list` allows us to plot the median PDF value and 3.0 confidence intervals of the `centre` estimate from\n",
        "the model-fit to each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_list = [result.samples for result in result_list]\n",
        "\n",
        "mp_instances = [samps.median_pdf() for samps in samples_list]\n",
        "ue3_instances = [samp.errors_at_upper_sigma(sigma=3.0) for samp in samples_list]\n",
        "le3_instances = [samp.errors_at_lower_sigma(sigma=3.0) for samp in samples_list]\n",
        "\n",
        "mp_centres = [instance.gaussian.centre for instance in mp_instances]\n",
        "ue3_centres = [instance.gaussian.centre for instance in ue3_instances]\n",
        "le3_centres = [instance.gaussian.centre for instance in le3_instances]\n",
        "\n",
        "print(f\"Median PDF inferred centre values\")\n",
        "print(mp_centres)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Overall Gaussian Parent Distribution__\n",
        "\n",
        "Fit the inferred `centre`'s from the fits performed above with a Gaussian distribution, in order to \n",
        "estimate the mean and scatter of the Gaussian from which the centres were drawn.\n",
        "\n",
        "We first extract the inferred median PDF centre values and their 1 sigma errors below, which will be the inputs\n",
        "to our fit for the parent Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ue1_instances = [samp.values_at_upper_sigma(sigma=1.0) for samp in samples_list]\n",
        "le1_instances = [samp.values_at_lower_sigma(sigma=1.0) for samp in samples_list]\n",
        "\n",
        "ue1_centres = [instance.gaussian.centre for instance in ue1_instances]\n",
        "le1_centres = [instance.gaussian.centre for instance in le1_instances]\n",
        "\n",
        "error_list = [ue1 - le1 for ue1, le1 in zip(ue1_centres, le1_centres)]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Analysis` class below fits a Gaussian distribution to the inferred `centre` values from each of the fits above,\n",
        "where the inferred error values are used as the errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data: np.ndarray, errors: np.ndarray):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = np.array(data)\n",
        "        self.errors = np.array(errors)\n",
        "\n",
        "    def log_likelihood_function(self, instance: af.ModelInstance) -> float:\n",
        "        \"\"\"\n",
        "        Fits a set of 1D data points with a 1D Gaussian distribution, in order to determine from what Gaussian\n",
        "        distribution the analysis classes `data` were drawn.\n",
        "\n",
        "        In this example, this function determines from what parent Gaussian disrtribution the inferred centres\n",
        "        of each 1D Gaussian were drawn.\n",
        "        \"\"\"\n",
        "        log_likelihood_term_1 = np.sum(\n",
        "            -np.divide(\n",
        "                (self.data - instance.median) ** 2,\n",
        "                2 * (instance.scatter**2 + self.errors**2),\n",
        "            )\n",
        "        )\n",
        "        log_likelihood_term_2 = -np.sum(\n",
        "            0.5 * np.log(instance.scatter**2 + self.errors**2)\n",
        "        )\n",
        "\n",
        "        return log_likelihood_term_1 + log_likelihood_term_2\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `ParentGaussian` class is the model-component which used to fit the parent Gaussian to the inferred `centre` values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class ParentGaussian:\n",
        "    def __init__(self, median: float = 0.0, scatter: float = 0.01):\n",
        "        \"\"\"\n",
        "        A model component which represents a parent Gaussian distribution, which can be fitted to a 1D set of\n",
        "        measurments with errors in order to determine the probabilty they were drawn from this Gaussian.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        median\n",
        "            The median value of the parent Gaussian distribution.\n",
        "        scatter\n",
        "            The scatter (E.g. the sigma value) of the Gaussian.\n",
        "        \"\"\"\n",
        "\n",
        "        self.median = median\n",
        "        self.scatter = scatter\n",
        "\n",
        "    def probability_from_values(self, values: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        For a set of 1D values, determine the probability that they were random drawn from this parent Gaussian\n",
        "        based on its `median` and `scatter` attributes.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        values\n",
        "            A set of 1D values from which we will determine the probability they were drawn from the parent Gaussian.\n",
        "        \"\"\"\n",
        "        values = np.sort(np.array(values))\n",
        "        transformed_values = np.subtract(values, self.median)\n",
        "\n",
        "        return np.multiply(\n",
        "            np.divide(1, self.scatter * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_values, self.scatter))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "The `ParentGaussian` is the model component we fit in order to determine the probability the inferred centres were\n",
        "drawn from the distribution.\n",
        "\n",
        "This will be fitted via a non-linear search and therefore is created as a model component using `af.Model()` as per \n",
        "usual in **PyAutoFit**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(ParentGaussian)\n",
        "\n",
        "model.median = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.scatter = af.UniformPrior(lower_limit=0.0, upper_limit=50.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis + Search__\n",
        "\n",
        "We now create the Analysis class above which fits a parent 1D gaussian and create a dynesty search in order to fit\n",
        "it to the 1D inferred list of `centres`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=mp_centres, errors=error_list)\n",
        "search = af.DynestyStatic(nlive=100)\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results of this fit tell us the most probably values for the `median` and `scatter` of the 1D parent Gaussian fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "median = samples.median_pdf().median\n",
        "\n",
        "u1_error = samples.values_at_upper_sigma(sigma=1.0).median\n",
        "l1_error = samples.values_at_lower_sigma(sigma=1.0).median\n",
        "\n",
        "u3_error = samples.values_at_upper_sigma(sigma=3.0).median\n",
        "l3_error = samples.values_at_lower_sigma(sigma=3.0).median\n",
        "\n",
        "print(\n",
        "    f\"Inferred value of the hierarchical median via simple fit to {total_datasets} datasets: \\n \"\n",
        ")\n",
        "print(f\"{median} ({l1_error} {u1_error}) [1.0 sigma confidence intervals]\")\n",
        "print(f\"{median} ({l3_error} {u3_error}) [3.0 sigma confidence intervals]\")\n",
        "print()\n",
        "\n",
        "scatter = samples.median_pdf().scatter\n",
        "\n",
        "u1_error = samples.values_at_upper_sigma(sigma=1.0).scatter\n",
        "l1_error = samples.values_at_lower_sigma(sigma=1.0).scatter\n",
        "\n",
        "u3_error = samples.values_at_upper_sigma(sigma=3.0).scatter\n",
        "l3_error = samples.values_at_lower_sigma(sigma=3.0).scatter\n",
        "\n",
        "print(\n",
        "    f\"Inferred value of the hierarchical scatter via simple fit to {total_datasets} datasets: \\n \"\n",
        ")\n",
        "print(f\"{scatter} ({l1_error} {u1_error}) [1.0 sigma confidence intervals]\")\n",
        "print(f\"{scatter} ({l3_error} {u3_error}) [3.0 sigma confidence intervals]\")\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compare these values to those inferred in `tutorial_4_hierarchical_model`, which fits all datasets and the\n",
        "hierarchical values of the parent Gaussian simultaneously.,\n",
        " \n",
        "The errors for the fit performed in this tutorial are much larger. This is because of how in a graphical model\n",
        "the \"datasets talk to one another\", which is described fully in that tutorials subsection \"Benefits of Graphical Model\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}