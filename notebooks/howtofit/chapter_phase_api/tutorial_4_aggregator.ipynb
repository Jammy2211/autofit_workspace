{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 4: Aggregator\n",
        "===========================\n",
        "In chapter 1. we used used the `Aggregator` to load and inspect the `Samples` and fits of a model-fit.\n",
        "\n",
        "In this tutorial, we'll look at how the phase API and the template source code makes it easy to use the `Aggregator`\n",
        "to inspect, interpret and plot the results of the model-fit in ways that were not possible in chapter 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "from autoconf import conf\n",
        "\n",
        "\n",
        "conf.instance.push(\n",
        "    new_path=path.join(workspace_path, \"howtofit\", \"chapter_phase_api\", \"src\", \"config\")\n",
        ")\n",
        "\n",
        "import autofit as af\n",
        "import src as htf"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To load these results with the `Aggregator`, we again point it to the path of the results we want it to inspect, with\n",
        "our path straight to the `Aggregator` results ensuring we don't need to filter our `Aggregator` in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg = af.Aggregator(\n",
        "    directory=path.join(\"output\", \"howtofit\", \"chapter_phase_api\", \"phase_t1\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can again use the `Aggregator` to load a generator of every fit`s dataset, by changing the `output` attribute to \n",
        "the `dataset` attribute at the end of the aggregator.\n",
        "\n",
        "Note how in chapter 1 this attribute corresponded to that `data` and `noise_map` seprately, where the chapter 2 template \n",
        "projects simply outputs the `Dataset` object. For a realistic model-fitting project the object containing the data\n",
        "could well have many attributes, highlighting the benefit for the `Aggregator` there is by containing it all in one \n",
        "class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_gen = agg.values(\"dataset\")\n",
        "print(\"Datasets:\")\n",
        "print(list(dataset_gen), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is here the object-oriented design of our `plot.py` module comes into its own. We have the `Dataset` objects loaded, \n",
        "meaning we can easily plot each `Dataset` using the `dataset_plot.py` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset in agg.values(\"dataset\"):\n",
        "    htf.plot.Dataset.data(dataset=dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` dictionary we input into the `Phase` is also available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for info in agg.values(\"info\"):\n",
        "    print(info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're going to refit each `Dataset` with the `max_log_likelihood_instance` of each model-fit. To do this, we'll need \n",
        "each `Phase`'s `Dataset`.\n",
        "\n",
        "(If you are unsure what the `zip` is doing below, it essentially combines the `dataset_gen` and `settings_gen` into \n",
        "one list such that we can iterate over all three simultaneously to create each trimmed `Dataset`).\n",
        "\n",
        "The `Dataset` may have been altered by the `data_trim_left` and `data_trim_right` `SettingsPhase`. We can \n",
        "load the `SettingsPhase` via the `Aggregator` to use these settings when we create the `Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_gen = agg.values(\"dataset\")\n",
        "settings_gen = agg.values(\"settings\")\n",
        "\n",
        "datasets = [\n",
        "    dataset.trimmed_dataset_from_settings(settings=settings.settings_dataset)\n",
        "    for dataset, settings in zip(dataset_gen, settings_gen)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of course, we should actually be setting up the `Dataset`'s using generators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def dataset_from_agg_obj(agg_obj):\n",
        "\n",
        "    dataset = agg_obj.dataset\n",
        "    settings = agg_obj.settings\n",
        "\n",
        "    return dataset.trimmed_dataset_from_settings(settings=settings.settings_dataset)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use this function as a generator using the `Aggregator`, we again apply the `Aggregator`'s `map` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_gen = agg.map(func=dataset_from_agg_obj)\n",
        "print(list(dataset_gen))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets get the `max_log_likelihood_instance`s, as we did in chapter 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instances = [samps.max_log_likelihood_instance for samps in agg.values(\"samples\")]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, we want to inspect the fit of each `max_log_likelihood_instance`. To do this, we reperform each fit.\n",
        "\n",
        "First, we need to create the `model_data` of every `max_log_likelihood_instance`. Lets begin by creating a list \n",
        "of profiles of every phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "profiles = [instance.profiles for instance in instances]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use these to create the `model_data` of each set of profiles (which in this case is just 1 `Gaussian`, but had\n",
        "we included more profiles in the model would consist of multiple `Gaussian`s / `Exponential`s.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_datas = [\n",
        "    profile.gaussian.profile_from_xvalues(xvalues=dataset.xvalues)\n",
        "    for profile, dataset in zip(profiles, agg.values(\"dataset\"))\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can combine the `Dataset`s and `model_data`s in a `Fit` object to create the maximum likelihood fit of each phase!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fits = [\n",
        "    htf.FitDataset(dataset=dataset, model_data=model_data)\n",
        "    for dataset, model_data in zip(datasets, model_datas)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now plot different components of the `Fit` (again benefiting from how we set up the `fit_plots.py` module)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for fit in fits:\n",
        "    htf.plot.FitDataset.residual_map(fit=fit)\n",
        "    htf.plot.FitDataset.normalized_residual_map(fit=fit)\n",
        "    htf.plot.FitDataset.chi_squared_map(fit=fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, the code above does not use generators and could prove memory intensive for large datasets. Below is how we \n",
        "would perform the above task with generator functions, using the `dataset_gen` above for the `Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def model_data_from_agg_obj(agg_obj):\n",
        "\n",
        "    xvalues = agg_obj.dataset.xvalues\n",
        "    instance = agg_obj.samples.max_log_likelihood_instance\n",
        "    profiles = instance.profiles\n",
        "\n",
        "    return sum([profile.profile_from_xvalues(xvalues=xvalues) for profile in profiles])\n",
        "\n",
        "\n",
        "def fit_from_agg_obj(agg_obj):\n",
        "\n",
        "    dataset = dataset_from_agg_obj(agg_obj=agg_obj)\n",
        "    model_data = model_data_from_agg_obj(agg_obj=agg_obj)\n",
        "\n",
        "    return htf.FitDataset(dataset=dataset, model_data=model_data)\n",
        "\n",
        "\n",
        "fit_gen = agg.map(func=fit_from_agg_obj)\n",
        "\n",
        "for fit in fit_gen:\n",
        "    htf.plot.FitDataset.residual_map(fit=fit)\n",
        "    htf.plot.FitDataset.normalized_residual_map(fit=fit)\n",
        "    htf.plot.FitDataset.chi_squared_map(fit=fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting up the above objects (the `dataset's, `model data`s, `fit`s) was a bit of work. It wasn`t too many \n",
        "lines of code, but for something our users will want to do many times it`d be nice to have a short cut to setting them \n",
        "up, right?\n",
        "\n",
        "In the source code module `aggregator.py` we've set up exactly such a short-cut. This module simply contains the \n",
        "generator functions above such that the generator can be created by passing the `Aggregator`. This provides us with \n",
        "convenience methods for quickly creating the `Dataset`, `model_data` and `Fit`'s using a single line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_gen = htf.agg.dataset_generator_from_aggregator(aggregator=agg)\n",
        "model_data_gen = htf.agg.model_data_generator_from_aggregator(aggregator=agg)\n",
        "fit_gen = htf.agg.fit_generator_from_aggregator(aggregator=agg)\n",
        "\n",
        "for fit in fit_gen:\n",
        "\n",
        "    htf.plot.FitDataset.residual_map(fit=fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The methods in `aggregator.py` actually allow us to go one step further: they all us to create the `Dataset` and\n",
        "`Fit` objects using an input `SettingsDataset`. This means we can fit a `Dataset` with a `Phase` and then see how\n",
        "the model-fits change if we customize the `Dataset` in different ways.\n",
        "\n",
        "Below, we create and plot a `Fit` where the `Dataset` is trimmed from the left and right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "settings_dataset = htf.SettingsDataset(data_trim_left=20, data_trim_right=20)\n",
        "\n",
        "fit_gen = htf.agg.fit_generator_from_aggregator(\n",
        "    aggregator=agg, settings_dataset=settings_dataset\n",
        ")\n",
        "\n",
        "htf.plot.FitDataset.residual_map(fit=list(fit_gen)[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For your model-fitting project, you'll need to update the `aggregator.py` module in the same way. This is why we have \n",
        "emphasised the object-oriented design of our model-fitting project throughout. This design makes it very easy to \n",
        "inspect results via the `Aggregator` later on!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}