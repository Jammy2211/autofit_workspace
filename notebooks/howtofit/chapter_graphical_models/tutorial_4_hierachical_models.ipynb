{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 4: Hierarchical\n",
        "========================\n",
        "\n",
        "In the previous tutorial, we fitted a graphical model with the aim of determining an estimate of shared parameters,\n",
        "the `centre`'s of a dataset of 1D Gaussians. We did this by fitting all datasets simultaneously. When there are shared\n",
        "parameters in a model, this is a powerful and effective tool, but things may not always be so simple.\n",
        "\n",
        "A common extension to the problem is one where we expect that the shared parameter(s) of the model do not have exactly\n",
        "the same value in every dataset. Instead, our expectation is that the parameter(s) are drawn from a common\n",
        "parent distribution (e.g. a Gaussian distribution). It is the parameters of this distribution that we consider shared\n",
        "across the dataset (e.g. the means and scatter of the Gaussian distribution). These are the parameters we ultimately\n",
        "wish to infer to understand the global behaviour of our model.\n",
        "\n",
        "This is called a hierarchical model, whihc we fit in this tutorial. The `centre` of each 1D Gaussian is now no\n",
        "longer the same in each dataset and they are instead drawn from a shared parent Gaussian distribution\n",
        "(with `mean=50.0` and `sigma=10.0`). The hierarchical model will recover the `mean` and `sigma` values of the parent\n",
        "distribution'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and \n",
        " `visualize` functions.\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        " - `plot_profile_1d`: a function for plotting 1D profile datasets including their noise.\n",
        "\n",
        "These are functionally identical to the `Analysis`, `Gaussian` and `plot_profile_1d` objects and functions you \n",
        "have seen and used elsewhere throughout the workspace.\n",
        "\n",
        "__Dataset__\n",
        "\n",
        "For each dataset we now set up the correct path and load it. \n",
        "\n",
        "We are loading a new Gaussian dataset, where the Gaussians have different centres which were drawn from a parent\n",
        "Gaussian distribution with a mean centre value of 50.0 and sigma of 10.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_datasets = 5\n",
        "\n",
        "dataset_name_list = []\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", \"gaussian_x1__hierarchical\", dataset_name\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    dataset_name_list.append(dataset_name)\n",
        "    data_list.append(data)\n",
        "    noise_map_list.append(noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting the Gaussians we can just about make out that their centres are not all at pixel 50, and are spread out\n",
        "around it (albeit its difficult to be sure, due to the low signal-to-noise of the data). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name, data in zip(dataset_name_list, data_list):\n",
        "    af.ex.plot_profile_1d(\n",
        "        xvalues=np.arange(data.shape[0]),\n",
        "        profile_1d=data,\n",
        "        title=dataset_name,\n",
        "        ylabel=\"Data Values\",\n",
        "        color=\"k\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "For each dataset we now create a corresponding `Analysis` class, like in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    analysis_list.append(analysis)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Individual Factors__\n",
        "\n",
        "We first set up a model for each `Gaussian` which is individually fitted to each 1D dataset, which forms the\n",
        "factors on the factor graph we compose. \n",
        "\n",
        "This uses a nearly identical for loop to the previous tutorials, however a shared `centre` is no longer used and each \n",
        "`Gaussian` is given its own prior for the `centre`. \n",
        "\n",
        "We will see next how this `centre` is used to construct the hierarchical model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_list = []\n",
        "\n",
        "for model_index in range(len(data_list)):\n",
        "    gaussian = af.Model(af.ex.Gaussian)\n",
        "\n",
        "    gaussian.centre = af.GaussianPrior(\n",
        "        mean=50.0, sigma=20.0, lower_limit=0.0, upper_limit=100.0\n",
        "    )\n",
        "    gaussian.normalization = af.GaussianPrior(mean=3.0, sigma=5.0, lower_limit=0.0)\n",
        "    gaussian.sigma = af.GaussianPrior(mean=10.0, sigma=10.0, lower_limit=0.0)\n",
        "\n",
        "    model_list.append(gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factors__\n",
        "\n",
        "Now we have our `Analysis` classes and model components, we can compose our `AnalysisFactor`'s.\n",
        "\n",
        "These are composed in the same way as for the graphical model in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We now compose the hierarchical model that we fit, using the individual Gaussian model components created above.\n",
        "\n",
        "We first create a `HierarchicalFactor`, which represents the parent Gaussian distribution from which we will assume \n",
        "that the `centre` of each individual `Gaussian` dataset is drawn. \n",
        "\n",
        "For this parent `Gaussian`, we have to place priors on its `mean` and `sigma`, given that they are parameters in our\n",
        "model we are ultimately fitting for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "hierarchical_factor = af.HierarchicalFactor(\n",
        "    af.GaussianPrior,\n",
        "    mean=af.GaussianPrior(mean=50.0, sigma=10, lower_limit=0.0, upper_limit=100.0),\n",
        "    sigma=af.GaussianPrior(mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=100.0),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now add each of the individual model `Gaussian`'s `centre` parameters to the `hierarchical_factor`.\n",
        "\n",
        "This composes the hierarchical model whereby the individual `centre` of every `Gaussian` in our dataset is now assumed \n",
        "to be drawn from a shared parent distribution. It is the `mean` and `sigma` of this distribution we are hoping to \n",
        "estimate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "for model in model_list:\n",
        "    hierarchical_factor.add_drawn_variable(model.centre)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "We now create the factor graph for this model, using the list of `AnalysisFactor`'s and the hierarchical factor.\n",
        "\n",
        "Note that the `hierarchical_factor` is passed in below, which was not the case in previous tutorials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list, hierarchical_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The factor graph model `info` attribute shows that the hierarchical factor's parameters are included in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "We can now create a non-linear search and used it to the fit the hierarchical model, again using \n",
        "its `global_prior_model` property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"howtofit\", \"chapter_graphical_models\"),\n",
        "    name=\"tutorial_4_hierarchical\",\n",
        "    sample=\"rwalk\",\n",
        ")\n",
        "\n",
        "result = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result's `info` attribute shows the result, including the hierarchical factor's parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now inspect the inferred value of hierarchical factor's mean and sigma.\n",
        "\n",
        "We see that they are consistent with the input values of `mean=50.0` and `sigma=10.0`.\n",
        "\n",
        "The hierarchical factor results are at the end of the samples list, hence why we extract them using `[-1]` and [-2]`\n",
        "below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "mean = samples.median_pdf(as_instance=False)[-2]\n",
        "\n",
        "u1_error = samples.values_at_upper_sigma(sigma=1.0)[-2]\n",
        "l1_error = samples.values_at_lower_sigma(sigma=1.0)[-2]\n",
        "\n",
        "u3_error = samples.values_at_upper_sigma(sigma=3.0)[-2]\n",
        "l3_error = samples.values_at_lower_sigma(sigma=3.0)[-2]\n",
        "\n",
        "print(\n",
        "    \"Inferred value of the mean of the parent hierarchical distribution for the centre: \\n\"\n",
        ")\n",
        "print(f\"{mean} ({l1_error} {u1_error}) [1.0 sigma confidence intervals]\")\n",
        "print(f\"{mean} ({l3_error} {u3_error}) [3.0 sigma confidence intervals]\")\n",
        "\n",
        "scatter = samples.median_pdf(as_instance=False)[-1]\n",
        "\n",
        "u1_error = samples.values_at_upper_sigma(sigma=1.0)[-1]\n",
        "l1_error = samples.values_at_lower_sigma(sigma=1.0)[-1]\n",
        "\n",
        "u3_error = samples.values_at_upper_sigma(sigma=3.0)[-1]\n",
        "l3_error = samples.values_at_lower_sigma(sigma=3.0)[-1]\n",
        "\n",
        "print(\n",
        "    \"Inferred value of the scatter (the sigma value of the Gassuain) of the parent hierarchical distribution for the centre: \\n\"\n",
        ")\n",
        "print(f\"{scatter} ({l1_error} {u1_error}) [1.0 sigma confidence intervals]\")\n",
        "print(f\"{scatter} ({l3_error} {u3_error}) [3.0 sigma confidence intervals]\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Comparison to One-by-One Fits__\n",
        "\n",
        "We can compare the inferred values above to the values inferred for individual fits in the \n",
        "tutorial `tutorial_optional_hierarchical_individual.py`.\n",
        "\n",
        "This fits the hierarchical model is a much simpler way -- fitting each dataset one-by-one and then fitting the \n",
        "parent Gaussian distribution to those results.\n",
        "\n",
        "For the results below, inferred on my laptop, we can see that the correct mean and scatter of the parent Gaussian is \n",
        "inferred but the errors are much larger than the graphical model fit above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"Inferred value of the mean of the parent hierarchical distribution for one-by-one fits: \\n\"\n",
        ")\n",
        "print(\n",
        "    \"50.00519854538594 (35.825675441265815 65.56274024242403) [1.0 sigma confidence intervals]\"\n",
        ")\n",
        "print(\n",
        "    \"50.00519854538594 (1.3226539914914734 96.92151898283811) [3.0 sigma confidence intervals]\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Inferred value of the scatter of the parent hierarchical distribution for one-by-one fits: \\n\"\n",
        ")\n",
        "print(\n",
        "    \"15.094393493747617 (4.608862348173649 31.346751522582483) [1.0 sigma confidence intervals]\"\n",
        ")\n",
        "print(\n",
        "    \"15.094393493747617 (0.060533647989089806 49.05537884440667) [3.0 sigma confidence intervals]\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Benefits of Graphical Model__\n",
        "\n",
        "We compared the results inferred in this script via a graphical model to a simpler approach which fits each dataset \n",
        "one-by-one and infers the hierarchical parent distribution's parameters afterwards.\n",
        "\n",
        "The graphical model provides a more accurate and precise estimate of the parent distribution's parameters. This is \n",
        "because the fit to each dataset informs the hierarchical distribution's parameters, which in turn improves\n",
        "constraints on the other datasets. In a hierarchical fit, we describe this as \"the datasets talking to one another\". \n",
        "\n",
        "For example, by itself, dataset_0 may give weak constraints on the centre spanning the range 20 -> 85 at 1 sigma \n",
        "confidence. Now, consider if simultaneously all of the other datasets provide strong constraints on the \n",
        "hierarchical's distribution's parameters, such that its `mean = 50 +- 5.0` and `sigma = 10.0 +- 2.0` at 1 sigma \n",
        "confidence. \n",
        "\n",
        "This will significantly change our inferred parameters for dataset 0, as the other datasets inform us\n",
        "that solutions where the centre is well below approximately 40 are less likely, because they are inconsistent with\n",
        "the parent hierarchical distribution's parameters!\n",
        "\n",
        "For complex graphical models with many hierarchical factors, this phenomena of the \"datasets talking to one another\" \n",
        "is crucial in breaking degeneracies between parameters and maximally extracting information from extremely large\n",
        "datasets.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "By composing and fitting hierarchical models in the graphical modeling framework we can fit for global trends\n",
        "within large datasets. The tools applied in this tutorial and the previous tutorial can be easily extended to \n",
        "compose complex graphical models, with multiple shared parameters and hierarchical factors.\n",
        "\n",
        "However, there is a clear challenge scaling the graphical modeling framework up in this way: model complexity. As the \n",
        "model becomes more complex, an inadequate sampling of parameter space will lead one to infer local maxima. Furthermore,\n",
        "one will soon hit computational limits on how many datasets can feasibly be fitted simultaneously, both in terms of\n",
        "CPU time and memory limitations. \n",
        "\n",
        "Therefore, the next tutorial introduces expectation propagation, a framework that inspects the factor graph of a \n",
        "graphical model and partitions the model-fit into many separate fits on each graph node. When a fit is complete, \n",
        "it passes the information learned about the model to neighboring nodes. \n",
        "\n",
        "Therefore, graphs comprising hundreds of model components (and tens of thousands of parameters) can be fitted as \n",
        "many bite-sized model fits, where the model fitted at each node consists of just tens of parameters. This makes \n",
        "graphical models scalable to largest datasets and most complex models!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}