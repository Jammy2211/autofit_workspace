{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial 2: Graphical Models\n",
    "============================\n",
    "\n",
    "In the previous tutorial, we fitted a dataset containing 3 noisy 1D Gaussian which had a shared and global value of\n",
    "`centre`. We attempted to estimate this global `centre` value by fitting each dataset individually with a 1D `Gaussian`. \n",
    "We then combined the inferred `centre` values of each fit to estimate the global `centre`, by either taking the mean \n",
    "values of each `centre` or combining the fits into a joint PDF.\n",
    "\n",
    "We concluded that estimating the global `centre` in these ways was suboptimal, and that we were better of fitting for\n",
    "the global `centre` in our model by fitting all 3 datasets simultaneously. In this tutorial we will do this by \n",
    "composing a graphical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T14:10:13.482354Z",
     "iopub.status.busy": "2021-03-06T14:10:13.482005Z",
     "iopub.status.idle": "2021-03-06T14:10:14.235917Z",
     "shell.execute_reply": "2021-03-06T14:10:14.235556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Jammy/Code/PyAuto/autofit_workspace\n",
      "Working Directory has been set to `/mnt/c/Users/Jammy/Code/PyAuto/autofit_workspace`\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pyprojroot import here\n",
    "workspace_path = str(here())\n",
    "%cd $workspace_path\n",
    "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
    "\n",
    "import autofit as af\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `Analysis` class of chapter 1, which includes `visualize` and `save_attributes_for_aggregator` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T14:10:14.238551Z",
     "iopub.status.busy": "2021-03-06T14:10:14.238212Z",
     "iopub.status.idle": "2021-03-06T14:10:14.240311Z",
     "shell.execute_reply": "2021-03-06T14:10:14.239967Z"
    }
   },
   "outputs": [],
   "source": [
    "from analysis import Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset we now set up the correct path and load it. Whereas in the previous tutorial we fitted each dataset \n",
    "one-by-one, in this tutorial we will instead store each dataset in a list so that we can set up a single model-fit \n",
    "that fits the 3 datasets simultaneously.\n",
    "\n",
    "NOTE: In this tutorial we explicitly write code for loading each dataset and store each dataset as their own Python\n",
    "variable (e.g. data_0, data_1, data_2, etc.). We do not use a for loop or a list to do this (like we did in previous \n",
    "tutorials), even though this would be syntactically cleaner code. This is to make the API for setting up a graphical \n",
    "model in this tutorial clear and explicit; in the next tutorial we will introduce  the **PyAutoFit** API for setting \n",
    "up a graphical model for large datasets concisely.\n",
    "\n",
    "[FOR RICH: For graphical models, the use of the `Analysis` class will cause memory issues for large datasets. This is\n",
    "because every instance of the `Analysis` class is made before we run the graphical model. For large datasets this will\n",
    "cripple memory. We should discussing how we can avoid this.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T14:10:14.243764Z",
     "iopub.status.busy": "2021-03-06T14:10:14.243438Z",
     "iopub.status.idle": "2021-03-06T14:10:14.246754Z",
     "shell.execute_reply": "2021-03-06T14:10:14.246472Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = path.join(\"dataset\", \"example_1d\")\n",
    "\n",
    "dataset_0_path = path.join(dataset_path, \"gaussian_x1_0__low_snr\")\n",
    "data_0 = af.util.numpy_array_from_json(file_path=path.join(dataset_0_path, \"data.json\"))\n",
    "noise_map_0 = af.util.numpy_array_from_json(\n",
    "    file_path=path.join(dataset_0_path, \"noise_map.json\")\n",
    ")\n",
    "\n",
    "dataset_1_path = path.join(dataset_path, \"gaussian_x1_1__low_snr\")\n",
    "data_1 = af.util.numpy_array_from_json(file_path=path.join(dataset_1_path, \"data.json\"))\n",
    "noise_map_1 = af.util.numpy_array_from_json(\n",
    "    file_path=path.join(dataset_1_path, \"noise_map.json\")\n",
    ")\n",
    "\n",
    "dataset_2_path = path.join(dataset_path, \"gaussian_x1_2__low_snr\")\n",
    "data_2 = af.util.numpy_array_from_json(file_path=path.join(dataset_2_path, \"data.json\"))\n",
    "noise_map_2 = af.util.numpy_array_from_json(\n",
    "    file_path=path.join(dataset_2_path, \"noise_map.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset we now create a corresponding `Analysis` class. \n",
    "\n",
    "By associating each dataset with an `Analysis` class we are therefore also setting up the `log_likelihood_function` \n",
    "used to fit it. In the next tutorial we will introduce a graphical model that fits shared model components to datasets \n",
    "that are of different formats, such that each fit is performed using a different `log_likelihood_function` and \n",
    "therefore `Analysis` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T14:10:14.249142Z",
     "iopub.status.busy": "2021-03-06T14:10:14.248821Z",
     "iopub.status.idle": "2021-03-06T14:10:14.250821Z",
     "shell.execute_reply": "2021-03-06T14:10:14.250549Z"
    }
   },
   "outputs": [],
   "source": [
    "analysis_0 = Analysis(data=data_0, noise_map=noise_map_0)\n",
    "analysis_1 = Analysis(data=data_1, noise_map=noise_map_1)\n",
    "analysis_2 = Analysis(data=data_2, noise_map=noise_map_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compose the graphical model we will fit. This uses the `PriorModel` and `CollectionPriorModel` objects you \n",
    "are now familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T14:10:14.252867Z",
     "iopub.status.busy": "2021-03-06T14:10:14.252559Z",
     "iopub.status.idle": "2021-03-06T14:10:14.271696Z",
     "shell.execute_reply": "2021-03-06T14:10:14.271401Z"
    }
   },
   "outputs": [],
   "source": [
    "from autofit import graphical as g\n",
    "import profiles as p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by setting up a shared prior for `centre`. \n",
    "\n",
    "We set up this up as a single `GaussianPrior` which will be passed to separate `PriorModel`'s for each `Gaussian` used \n",
    "to fit each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T14:10:14.274215Z",
     "iopub.status.busy": "2021-03-06T14:10:14.273900Z",
     "iopub.status.idle": "2021-03-06T14:10:14.275610Z",
     "shell.execute_reply": "2021-03-06T14:10:14.275286Z"
    }
   },
   "outputs": [],
   "source": [
    "centre_shared_prior = af.GaussianPrior(mean=10.0, sigma=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up three `CollectionPriorModel`'s, each of which contain a `Gaussian` that is used to fit each of the \n",
    "datasets we loaded above.\n",
    "\n",
    "All three of these `CollectionPriorModel`'s use the `centre_shared_prior`. This means all three model-components use \n",
    "the same value of `centre` for every model composed and fitted by the `NonLinearSearch`, reducing the dimensionality \n",
    "of parameter space from N=9 (e.g. 3 parameters per Gaussian) to N=7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T14:10:14.279174Z",
     "iopub.status.busy": "2021-03-06T14:10:14.278837Z",
     "iopub.status.idle": "2021-03-06T14:10:14.309656Z",
     "shell.execute_reply": "2021-03-06T14:10:14.308624Z"
    }
   },
   "outputs": [],
   "source": [
    "gaussian_0 = af.PriorModel(p.Gaussian)\n",
    "gaussian_0.centre = af.GaussianPrior(mean=50, sigma=20)\n",
    "gaussian_0.intensity = af.GaussianPrior(mean=10.0, sigma=10.0)\n",
    "gaussian_0.sigma = centre_shared_prior  # This prior is used by all 3 Gaussians!\n",
    "\n",
    "prior_model_0 = af.CollectionPriorModel(gaussian=gaussian_0)\n",
    "\n",
    "gaussian_1 = af.PriorModel(p.Gaussian)\n",
    "gaussian_1.centre = af.GaussianPrior(mean=50, sigma=20)\n",
    "gaussian_1.intensity = af.GaussianPrior(mean=10.0, sigma=10.0)\n",
    "gaussian_1.sigma = centre_shared_prior  # This prior is used by all 3 Gaussians!\n",
    "\n",
    "prior_model_1 = af.CollectionPriorModel(gaussian=gaussian_1)\n",
    "\n",
    "gaussian_2 = af.PriorModel(p.Gaussian)\n",
    "gaussian_2.centre = af.GaussianPrior(mean=50, sigma=20)\n",
    "gaussian_2.intensity = af.GaussianPrior(mean=10.0, sigma=10.0)\n",
    "gaussian_2.sigma = centre_shared_prior  # This prior is used by all 3 Gaussians!\n",
    "\n",
    "prior_model_2 = af.CollectionPriorModel(gaussian=gaussian_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we composed a model consisting of three `Gaussian`'s with a shared `centre` prior. We also loaded three datasets\n",
    "which we intend to fit with each of these `Gaussians`, setting up each in an `Analysis` class that defines how the \n",
    "model is used to fit the data.\n",
    "\n",
    "We now simply need to pair each model-component to each `Analysis` class, so that **PyAutoFit** knows that: \n",
    "\n",
    "- `prior_model_0` fits `data_0` via `analysis_0`.\n",
    "- `prior_model_1` fits `data_1` via `analysis_1`.\n",
    "- `prior_model_2` fits `data_2` via `analysis_2`.\n",
    "\n",
    "The point where a `PriorModel` and `Analysis` class meet is called a `ModelFactor`. \n",
    "\n",
    "This term is used to denote that we are composing a graphical model, which is commonly termed a 'factor graph'. A \n",
    "factor defines a node on this graph where we have some data, a model, and we fit the two together. The 'links' between \n",
    "these different nodes then define the global model we are fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T14:10:14.316184Z",
     "iopub.status.busy": "2021-03-06T14:10:14.315416Z",
     "iopub.status.idle": "2021-03-06T14:10:14.318565Z",
     "shell.execute_reply": "2021-03-06T14:10:14.317952Z"
    }
   },
   "outputs": [],
   "source": [
    "model_factor_0 = g.ModelFactor(prior_model=prior_model_0, analysis=analysis_0)\n",
    "model_factor_1 = g.ModelFactor(prior_model=prior_model_1, analysis=analysis_1)\n",
    "model_factor_2 = g.ModelFactor(prior_model=prior_model_2, analysis=analysis_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine our `ModelFactors` into one, to compose the factor graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T14:10:14.322172Z",
     "iopub.status.busy": "2021-03-06T14:10:14.321644Z",
     "iopub.status.idle": "2021-03-06T14:10:14.324331Z",
     "shell.execute_reply": "2021-03-06T14:10:14.323830Z"
    }
   },
   "outputs": [],
   "source": [
    "factor_graph = g.FactorGraphModel(model_factor_0, model_factor_1, model_factor_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is a factor graph?\n",
    "\n",
    "A factor graph defines the graphical model we have composed. For example, it defines the different model components \n",
    "that make up our model (e.g. the three `Gaussian` classes) and how their parameters are linked or shared (e.g. that\n",
    "each `Gaussian` has its own unique `intensity` and `centre`, but a shared `sigma` parameter.\n",
    "\n",
    "This is what our factor graph looks like: \n",
    "\n",
    "The factor graph above is made up of two components:\n",
    "\n",
    "- Nodes: these are points on the graph where we have a unique set of data and a model that is made up of a subset of \n",
    "our overall graphical model. This is effectively the `ModelFactor` objects we created above. \n",
    "\n",
    "- Links: these define the model components and parameters that are shared across different nodes and thus retain the \n",
    "same values when fitting different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T14:10:14.392863Z",
     "iopub.status.busy": "2021-03-06T14:10:14.392506Z",
     "iopub.status.idle": "2021-03-06T14:10:15.027809Z",
     "shell.execute_reply": "2021-03-06T14:10:15.027502Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Jammy/Code/PyAuto/PyAutoFit/autofit/graphical/messages/normal.py:63: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sigma = np.sqrt(- 0.5 / eta2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian (centre, GaussianPrior, mean = 51.36788695185436, sigma = 1.491876753344831), (intensity, GaussianPrior, mean = 1.638259340042963, sigma = 0.2676303900288858), (sigma, GaussianPrior, mean = 12.047793805946828, sigma = 0.9654215841721682), Gaussian (centre, GaussianPrior, mean = 52.09365043227566, sigma = 1.6886098320333665), (intensity, GaussianPrior, mean = 2.283845286916797, sigma = 0.26699823243789417), (sigma, GaussianPrior, mean = 12.047793805946828, sigma = 0.9654215841721682), Gaussian (centre, GaussianPrior, mean = 49.95317630589345, sigma = 1.0848707141394476), (intensity, GaussianPrior, mean = 4.471052478954877, sigma = 0.31451013944776024), (sigma, GaussianPrior, mean = 12.047793805946828, sigma = 0.9654215841721682)\n"
     ]
    }
   ],
   "source": [
    "from autofit.graphical import optimise\n",
    "\n",
    "laplace = optimise.LaplaceFactorOptimiser()\n",
    "collection = factor_graph.optimise(laplace)\n",
    "\n",
    "print(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
