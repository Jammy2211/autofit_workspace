{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 1: Individual Models\n",
        "=============================\n",
        "\n",
        "In many examples, we fit one model to one dataset. For many problems, we may have a large dataset and are not\n",
        "interested in how well the model fits each individual dataset. Instead, we want to know how the model fits the full\n",
        "dataset, so that we can determine \"global\" trends of how the model fits the data.\n",
        "\n",
        "These tutorials show you how to compose and fit hierarchical models to large datasets, which fit many individual\n",
        "models to each dataset. However, all parameters in the model are linked together, enabling global inference of the\n",
        "model over the full dataset. This can extract a significant amount of extra information from large datasets, which\n",
        "fitting each dataset individually does not.\n",
        "\n",
        "Fitting a hierarchical model uses a \"graphical model\", which is a model that is simultaneously fitted to every\n",
        "dataset simultaneously. The graph expresses how the parameters of every individual model is paired with each dataset\n",
        "and how they are linked to every other model parameter. Complex graphical models fitting a diversity of different\n",
        "datasets and non-trivial model parameter linking is possible and common.\n",
        "\n",
        "This chapter will start by fitting a simple graphical model to a dataset of noisy 1D Gaussians. The Gaussians all\n",
        "share the same `centre`, meaning that a graphical model can be composed where there is only a single global `centre`\n",
        "shared by all Gaussians.\n",
        "\n",
        "However, before fitting a graphical model, we will first fit each Gaussian individually and combine the inference\n",
        "on the `centre` after every fit is complete. This will give us an estimate of the `centre` that we can compare to\n",
        "the result of the graphical model in tutorial 2.\n",
        "\n",
        "__Real World Example__\n",
        "\n",
        "Hierarchical models are often used to determine effective drug treatments across a sample of patients distributed over\n",
        "many hospitals. Trying to do this on each individual hospital dataset is not ideal, as the number of patients in each\n",
        "hospital is small and the treatment may be more or less effective in some hospitals than others. Hierarchical models\n",
        "can extract the global trends of how effective the treatment is across the full population of patients.\n",
        "\n",
        "In healthcare, there may also be many datasets available, with different formats that require slightly different models\n",
        "to fit them. The high levels of customization possible in model composition and defining the analysis class mean\n",
        "that fitting diverse datasets with hierarchical models is feasible. This also means that a common problem in healthcare\n",
        "data, missing data, can be treated in a statistically robust manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import numpy as np\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and \n",
        " `visualize` functions.\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        " - `plot_profile_1d`: a function for plotting 1D profile datasets including their noise.\n",
        "\n",
        "These are functionally identical to the `Analysis`, `Gaussian` and `plot_profile_1d` objects and functions you have \n",
        "seen and used elsewhere throughout the workspace.\n",
        "\n",
        "__Model__\n",
        "\n",
        "Our model is a single `Gaussian`. \n",
        "\n",
        "We put this in a `Collection` so that when we extend the model in later tutorials we use the same API throughout\n",
        "all tutorials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.ex.Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "We quickly set up the name of each dataset, which is used below for loading the datasets.\n",
        "\n",
        "The dataset contains 10 Gaussians, but for speed we'll fit just 5. You can change this to 10 to see how the result\n",
        "changes with more datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_datasets = 5\n",
        "\n",
        "dataset_name_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_name_list.append(f\"dataset_{dataset_index}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each 1D Gaussian dataset we now set up the correct path, load it, and plot it. \n",
        "\n",
        "Notice how much lower the signal-to-noise is than you are used too, you probably find it difficult to estimate \n",
        "the centre of some of the Gaussians by eye!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name in dataset_name_list:\n",
        "    \"\"\"\n",
        "    Load the dataset from the `autofit_workspace/dataset` folder.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", \"gaussian_x1__low_snr\", dataset_name\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    af.ex.plot_profile_1d(\n",
        "        xvalues=np.arange(data.shape[0]),\n",
        "        profile_1d=data,\n",
        "        title=dataset_name,\n",
        "        ylabel=\"Data Values\",\n",
        "        color=\"k\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fits (one-by-one)__\n",
        "\n",
        "For every dataset we now create an `Analysis` and fit it with a `Gaussian`.\n",
        "\n",
        "The `Result` is stored in the list `result_list`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_list = []\n",
        "\n",
        "for dataset_name in dataset_name_list:\n",
        "    \"\"\"\n",
        "    Load the dataset from the `autofit_workspace/dataset` folder.\n",
        "    \"\"\"\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", \"gaussian_x1__low_snr\", dataset_name\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    For each dataset create a corresponding `Analysis` class.\n",
        "    \"\"\"\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    \"\"\"\n",
        "    Create the `DynestyStatic` non-linear search and use it to fit the data.\n",
        "\n",
        "    We use custom dynesty settings which ensure the posterior is explored fully and that our error estimates are robust.\n",
        "    \"\"\"\n",
        "    dynesty = af.DynestyStatic(\n",
        "        name=\"global_model\",\n",
        "        path_prefix=path.join(\n",
        "            \"howtofit\", \"chapter_graphical_models\", \"tutorial_1_individual_models\"\n",
        "        ),\n",
        "        unique_tag=dataset_name,\n",
        "        nlive=200,\n",
        "        dlogz=1e-4,\n",
        "        sample=\"rwalk\",\n",
        "        walks=10,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Dynesty has begun running, checkout \\n\"\n",
        "        f\"autofit_workspace/output/howtofit/chapter_graphica_models/tutorial_1_individual_models/{dataset_name} for live \\n\"\n",
        "        f\"output of the results. This Jupyter notebook cell with progress once Dynesty has completed, this could take a \\n\"\n",
        "        f\"few minutes!\"\n",
        "    )\n",
        "\n",
        "    result_list.append(dynesty.fit(model=model, analysis=analysis))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Results__\n",
        "\n",
        "Checkout the output folder, you should see five new sets of results corresponding to our Gaussian datasets.\n",
        "\n",
        "In the `model.results` file of each fit, it will be clear that the `centre` value of every fit (and the other \n",
        "parameters) have much larger errors than other **PyAutoFit** examples due to the low signal to noise of the data.\n",
        "\n",
        "The `result_list` allows us to plot the median PDF value and 3.0 confidence intervals of the `centre` estimate from\n",
        "the model-fit to each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "samples_list = [result.samples for result in result_list]\n",
        "\n",
        "mp_instances = [samps.median_pdf() for samps in samples_list]\n",
        "ue3_instances = [samp.errors_at_upper_sigma(sigma=3.0) for samp in samples_list]\n",
        "le3_instances = [samp.errors_at_lower_sigma(sigma=3.0) for samp in samples_list]\n",
        "\n",
        "mp_centres = [instance.gaussian.centre for instance in mp_instances]\n",
        "ue3_centres = [instance.gaussian.centre for instance in ue3_instances]\n",
        "le3_centres = [instance.gaussian.centre for instance in le3_instances]\n",
        "\n",
        "plt.errorbar(\n",
        "    x=[f\"Gaussian {index}\" for index in range(total_datasets)],\n",
        "    y=mp_centres,\n",
        "    marker=\".\",\n",
        "    linestyle=\"\",\n",
        "    yerr=[le3_centres, ue3_centres],\n",
        ")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These model-fits are consistent with a range of `centre` values. \n",
        "\n",
        "We can show this by plotting the 1D and 2D PDF's of each model fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "for samples in samples_list:\n",
        "    search_plotter = aplt.DynestyPlotter(samples=samples)\n",
        "    search_plotter.cornerplot()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also print the values of each centre estimate, including their estimates at 3.0 sigma.\n",
        "\n",
        "Note that above we used the samples to estimate the size of the errors on the parameters. Below, we use the samples to \n",
        "get the value of the parameter at these sigma confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "u1_instances = [samp.values_at_upper_sigma(sigma=1.0) for samp in samples_list]\n",
        "l1_instances = [samp.values_at_lower_sigma(sigma=1.0) for samp in samples_list]\n",
        "\n",
        "u1_centres = [instance.gaussian.centre for instance in u1_instances]\n",
        "l1_centres = [instance.gaussian.centre for instance in l1_instances]\n",
        "\n",
        "u3_instances = [samp.values_at_upper_sigma(sigma=3.0) for samp in samples_list]\n",
        "l3_instances = [samp.values_at_lower_sigma(sigma=3.0) for samp in samples_list]\n",
        "\n",
        "u3_centres = [instance.gaussian.centre for instance in u3_instances]\n",
        "l3_centres = [instance.gaussian.centre for instance in l3_instances]\n",
        "\n",
        "for index in range(total_datasets):\n",
        "    print(f\"Centre estimate of Gaussian dataset {index}:\\n\")\n",
        "    print(\n",
        "        f\"{mp_centres[index]} ({l1_centres[index]} {u1_centres[index]}) [1.0 sigma confidence interval]\"\n",
        "    )\n",
        "    print(\n",
        "        f\"{mp_centres[index]} ({l3_centres[index]} {u3_centres[index]}) [3.0 sigma confidence interval] \\n\"\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Estimating the Centre__\n",
        "\n",
        "So how might we estimate our global `centre` value? \n",
        "\n",
        "A simple approach takes the weighted average of the value inferred by all five fits above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ue1_instances = [samp.values_at_upper_sigma(sigma=1.0) for samp in samples_list]\n",
        "le1_instances = [samp.values_at_lower_sigma(sigma=1.0) for samp in samples_list]\n",
        "\n",
        "ue1_centres = [instance.gaussian.centre for instance in ue1_instances]\n",
        "le1_centres = [instance.gaussian.centre for instance in le1_instances]\n",
        "\n",
        "error_list = [ue1 - le1 for ue1, le1 in zip(ue1_centres, le1_centres)]\n",
        "\n",
        "values = np.asarray(mp_centres)\n",
        "sigmas = np.asarray(error_list)\n",
        "\n",
        "weights = 1 / sigmas ** 2.0\n",
        "weight_averaged = np.sum(1.0 / sigmas ** 2)\n",
        "\n",
        "weighted_centre = np.sum(values * weights) / np.sum(weights, axis=0)\n",
        "weighted_error = 1.0 / np.sqrt(weight_averaged)\n",
        "\n",
        "print(\n",
        "    f\"Weighted Average Centre Estimate = {weighted_centre} ({weighted_error}) [1.0 sigma confidence intervals]\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Posterior Multiplication__\n",
        "\n",
        "An alternative and more accurate way to combine each individual inferred centre is multiply their posteriors together.\n",
        "\n",
        "In order to do this, a smooth 1D profile must be fit to the posteriors via a Kernel Density Estimator (KDE).\n",
        "\n",
        "[There is currently no support for posterior multiplication and an example illustrating this is currently missing \n",
        "from this tutorial. However, I will discuss KDE multiplication throughout these tutorials to give the reader context \n",
        "for how this approach to parameter estimation compares to graphical models.]\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "Lets wrap up the tutorial. The methods used above combine the results of different fits and estimate a global \n",
        "value of `centre` alongside estimates of its error. \n",
        "\n",
        "In this tutorial, we fitted just 5 datasets. Of course, we could easily fit more datasets, and we would find that\n",
        "as we added more datasets our estimate of the global centre would become more precise.\n",
        "\n",
        "In the next tutorial, we will compare this result to one inferred via a graphical model. \n",
        "\n",
        "Tutorial 2: Graphical Models\n",
        "============================\n",
        "\n",
        "We have fitted a dataset containing 5 noisy 1D Gaussian which had a shared `centre` value. We estimated\n",
        "the `centre` by fitting each dataset individually and combining the value of the `centre` inferred by each fit into\n",
        "an overall estimate, using a weighted average.\n",
        "\n",
        "Graphical models use a different approach. They are a single model that is fitted to the entire dataset simultaneously. \n",
        "The model includes specific model component for every individual 1D Gaussian in the sample. However, the graphical \n",
        "model also has shared parameters between these individual model components.\n",
        "\n",
        "This example fits a graphical model using the same sample fitted in the previous tutorial, consisting of many 1D \n",
        "Gaussians. However, whereas previously the `centre` of each Gaussian was a free parameter in each fit, in the graphical \n",
        "model there is only a single parameter for the `centre` shared by all 1D Gaussians.\n",
        "\n",
        "This graphical model creates a non-linear parameter space with parameters for every Gaussian in our sample. For 5\n",
        "Gaussians each with their own model parameters but a single shared centre:\n",
        "\n",
        " - Each Gaussian has 2 free parameters from the components that are not shared (`normalization`, `sigma`).\n",
        " - There is one additional free parameter, which is the `centre` shared by all 5 Gaussians."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and \n",
        " `visualize` functions.\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        " - `plot_profile_1d`: a function for plotting 1D profile datasets including their noise.\n",
        "\n",
        "These are functionally identical to the `Analysis`, `Gaussian` and `plot_profile_1d` objects and functions you \n",
        "have seen and used elsewhere throughout the workspace.\n",
        "\n",
        "__Dataset__\n",
        "\n",
        "For each dataset we now set up the correct path and load it. \n",
        "\n",
        "Whereas in the previous tutorial we fitted each dataset one-by-one, in this tutorial we instead store each dataset \n",
        "in a list so that we can set up a single model-fit that fits the 5 datasets simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_datasets = 5\n",
        "\n",
        "dataset_name_list = []\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", \"gaussian_x1__low_snr\", dataset_name\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    dataset_name_list.append(dataset_name)\n",
        "    data_list.append(data)\n",
        "    noise_map_list.append(noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting the Gaussians we can remind ourselves that determining their centres by eye is difficult."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name, data in zip(dataset_name_list, data_list):\n",
        "    af.ex.plot_profile_1d(\n",
        "        xvalues=np.arange(data.shape[0]),\n",
        "        profile_1d=data,\n",
        "        title=dataset_name,\n",
        "        ylabel=\"Data Values\",\n",
        "        color=\"k\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "For each dataset we now create a corresponding `Analysis` class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We now compose the graphical model that we fit, using the `Model` object you are now familiar with.\n",
        "\n",
        "We begin by setting up a shared prior for `centre`. \n",
        "\n",
        "We set up this up as a single `GaussianPrior` which is passed to separate `Model`'s for each `Gaussian` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "centre_shared_prior = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now set up a list of `Model`'s, each of which contain a `Gaussian` that is used to fit each of the datasets \n",
        "loaded above.\n",
        "\n",
        "All of these models use the `centre_shared_prior`, meaning that all model-components use the same value of `centre` \n",
        "for every individual model component. \n",
        "\n",
        "For a fit using five Gaussians, this reduces the dimensionality of parameter space from N=15 (e.g. 3 parameters per \n",
        "Gaussian) to N=11 (e.g. 5 `sigma`'s 5 `normalizations` and 1 `centre`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_list = []\n",
        "\n",
        "for model_index in range(len(data_list)):\n",
        "    gaussian = af.Model(af.ex.Gaussian)\n",
        "\n",
        "    gaussian.centre = centre_shared_prior  # This prior is used by all 3 Gaussians!\n",
        "    gaussian.normalization = af.LogUniformPrior(lower_limit=1e-6, upper_limit=1e6)\n",
        "    gaussian.sigma = af.UniformPrior(lower_limit=0.0, upper_limit=25.0)\n",
        "\n",
        "    model_list.append(gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factors__\n",
        "\n",
        "Above, we composed a model consisting of three `Gaussian`'s with a shared `centre` prior. We also loaded three datasets\n",
        "which we intend to fit with each of these `Gaussians`, setting up each in an `Analysis` class that defines how the \n",
        "model is used to fit the data.\n",
        "\n",
        "We now simply pair each model-component to each `Analysis` class, so that:\n",
        "\n",
        "- `gaussian_0` fits `data_0` via `analysis_0`.\n",
        "- `gaussian_1` fits `data_1` via `analysis_1`.\n",
        "- `gaussian_2` fits `data_2` via `analysis_2`.\n",
        "\n",
        "The point where a `Model` and `Analysis` class meet is called an `AnalysisFactor`. \n",
        "\n",
        "This term denotes that we are composing a graphical model, which is commonly called a 'factor graph'. A  factor \n",
        "defines a node on this graph where we have some data, a model, and we fit the two together. The 'links' between these \n",
        "different nodes then define the global model we are fitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "We now combine our `AnalysisFactor`'s to compose a factor graph.\n",
        "\n",
        "What is a factor graph? A factor graph defines the graphical model's graph. For example, it defines the different \n",
        "model components that make up our model (e.g. the individual `Gaussian` classes) and how their parameters are linked or \n",
        "shared (e.g. that each `Gaussian` has its own unique `normalization` and `sigma`, but a shared `centre` parameter).\n",
        "\n",
        "This is what our factor graph looks like (visualization of graphs not implemented yet): \n",
        "\n",
        "The factor graph above is made up of two components:\n",
        "\n",
        "- Nodes: these are points on the graph where we have a unique set of data and a model that is made up of a subset of \n",
        "our overall graphical model. This is effectively the `AnalysisFactor` objects we created above. \n",
        "\n",
        "- Links: these define the model components and parameters that are shared across different nodes and thus retain the \n",
        "same values when fitting different datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fit will use the factor graph's `global_prior_model`, which uses the models contained in every analysis factor \n",
        "to contrast the overall global model that is fitted.\n",
        "\n",
        "Printing the `info` attribute of this model reveals the overall structure of the model, which is grouped in terms\n",
        "of the analysis factors and therefore datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "We can now create a non-linear search and use it to the fit the factor graph, using its `global_prior_model` property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"howtofit\", \"chapter_graphical_models\"),\n",
        "    name=\"tutorial_2_graphical_model\",\n",
        "    nlive=200,\n",
        "    dlogz=1e-4,\n",
        "    sample=\"rwalk\",\n",
        "    walks=10,\n",
        ")\n",
        "\n",
        "result = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result's `info` attribute shows that the result is expressed following the same struture of analysis factors\n",
        "that the `global_prior_model.info` attribute revealed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now inspect the inferred value of `centre`, and compare this to the value we estimated in the previous tutorial\n",
        "via a weighted average or posterior multiplicaition using KDE.(feature missing currently). \n",
        "\n",
        "(The errors of the weighted average and KDE below is what was estimated for a run on my PC, yours may be slightly \n",
        "different!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    f\"Weighted Average Centre Estimate = 48.535531422571886 (4.139907734505303) [1.0 sigma confidence intervals] \\n\"\n",
        ")\n",
        "\n",
        "centre = result.samples.median_pdf()[0].centre\n",
        "\n",
        "u1_error = result.samples.values_at_upper_sigma(sigma=1.0)[0].centre\n",
        "l1_error = result.samples.values_at_lower_sigma(sigma=1.0)[0].centre\n",
        "\n",
        "u3_error = result.samples.values_at_upper_sigma(sigma=3.0)[0].centre\n",
        "l3_error = result.samples.values_at_lower_sigma(sigma=3.0)[0].centre\n",
        "\n",
        "print(\"Inferred value of the shared centre via a graphical model fit: \\n\")\n",
        "print(f\"{centre} ({l1_error} {u1_error}) [1.0 sigma confidence intervals]\")\n",
        "print(f\"{centre} ({l3_error} {u3_error}) [3.0 sigma confidence intervals]\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graphical model's centre estimate and errors are pretty much exactly the same as the weighted average or KDE!\n",
        "\n",
        "Whats the point of fitting a graphical model if the much simpler approach of the previous tutorial gives the\n",
        "same answer? \n",
        "\n",
        "The answer, is model complexity. Graphical models become more powerful as we make our model more complex,\n",
        "our non-linear parameter space higher dimensionality and the degeneracies between different parameters on the graph\n",
        "more significant. \n",
        "\n",
        "We will demonstrate this in the next tutorial.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "In this tutorial, we showed that for our extremely simple model the graphical model gives pretty much the\n",
        "same estimate of the 1D Gaussian centre's as simpler approaches followed in the previous tutorial. \n",
        "\n",
        "We will next show the strengths of graphical models by fitting more complex models.\n",
        "\n",
        "Tutorial 3: Graphical Benefits\n",
        "==============================\n",
        "\n",
        "In the previous tutorials, we fitted a dataset containing 5 noisy 1D Gaussian which had a shared `centre` value and\n",
        "compared different approaches to estimate the shared `centre`. This included a simple approach fitting each dataset\n",
        "one-by-one and estimating the centre via a weighted average or posterior multiplication and a more complicated\n",
        "approach using a graphical model.\n",
        "\n",
        "The estimates were consistent with one another, making it hard to justify the use of the more complicated graphical\n",
        "model. However, the model fitted in the previous tutorial was extremely simple, and by making it slightly more complex\n",
        "we will show the benefits of the graphical model.\n",
        "\n",
        "__The Model__\n",
        "\n",
        "In this tutorial, each dataset now contains two Gaussians, and they all have the same shared centres, located at\n",
        "pixels 40 and 60."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and \n",
        " `visualize` functions.\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        " - `plot_profile_1d`: a function for plotting 1D profile datasets including their noise.\n",
        "\n",
        "These are functionally identical to the `Analysis`, `Gaussian` and `plot_profile_1d` objects and functions you have seen \n",
        "and used elsewhere throughout the workspace.\n",
        "\n",
        "__Dataset__\n",
        "\n",
        "For each dataset we now set up the correct path and load it. \n",
        "\n",
        "Note that we are loading a new dataset called `gaussian_x2__offset_centres`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_datasets = 5\n",
        "\n",
        "dataset_name_list = []\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", \"gaussian_x2__offset_centres\", dataset_name\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    dataset_name_list.append(dataset_name)\n",
        "    data_list.append(data)\n",
        "    noise_map_list.append(noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting the datasets we see that each dataset contains two Gaussians. \n",
        "\n",
        "Their centres are offset from one another and not located at pixel 50, like in the previous tutorials. \n",
        "\n",
        "As discussed above, the Gaussians in every dataset are in facted centred at pixels 40 and 60."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name, data in zip(dataset_name_list, data_list):\n",
        "    af.ex.plot_profile_1d(\n",
        "        xvalues=np.arange(data.shape[0]),\n",
        "        profile_1d=data,\n",
        "        title=dataset_name,\n",
        "        ylabel=\"Data Values\",\n",
        "        color=\"k\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "For each dataset we now create a corresponding `Analysis` class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model (one-by-one)__\n",
        "\n",
        "We are first going to fit each dataset one by one.\n",
        "\n",
        "Our model therefore now has two 1D `Gaussian`'s.\n",
        "\n",
        "To remove solutions where the Gaussians flip locations and fit the other Gaussian, we set uniform priors on the\n",
        "`centre`'s which ensures one Gaussian stays on the left side of the data (fitting the Gaussian at pixel 40) \n",
        "whilst the other stays on the right (fitting the Gaussian at pixel 60)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0 = af.Model(af.ex.Gaussian)\n",
        "\n",
        "gaussian_0.centre = af.UniformPrior(lower_limit=0.0, upper_limit=50.0)\n",
        "\n",
        "gaussian_1 = af.Model(af.ex.Gaussian)\n",
        "\n",
        "gaussian_1.centre = af.UniformPrior(lower_limit=50.0, upper_limit=100.0)\n",
        "\n",
        "model = af.Collection(gaussian_0=gaussian_0, gaussian_1=gaussian_1)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fits (one-by-one)__\n",
        "\n",
        "For every dataset we now create an `Analysis` and fit it with a `Gaussian`.\n",
        "\n",
        "The `Result` is stored in the list `result_list`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_list = []\n",
        "\n",
        "for i, analysis in enumerate(analysis_list):\n",
        "    \"\"\"\n",
        "    Create the `DynestyStatic` non-linear search and use it to fit the data.\n",
        "\n",
        "    We use custom dynesty settings which ensure the posterior is explored fully and that our error estimates are robust.\n",
        "    \"\"\"\n",
        "    search = af.DynestyStatic(\n",
        "        name=f\"individual_fit_{i}\",\n",
        "        path_prefix=path.join(\n",
        "            \"howtofit\", \"chapter_graphical_models\", \"tutorial_3_graphical_benefits\"\n",
        "        ),\n",
        "        nlive=200,\n",
        "        dlogz=1e-4,\n",
        "        sample=\"rwalk\",\n",
        "        walks=10,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Dynesty has begun running, checkout \\n\"\n",
        "        f\"autofit_workspace/output/howtofit/chapter_graphical_models/tutorial_3_graphical_benefits/{dataset_name} for live \\n\"\n",
        "        f\"output of the results. This Jupyter notebook cell with progress once Dynesty has completed, this could take a \\n\"\n",
        "        f\"few minutes!\"\n",
        "    )\n",
        "\n",
        "    result_list.append(search.fit(model=model, analysis=analysis))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Centre Estimates (Weighted Average)__\n",
        "\n",
        "We can now compute the centre estimate of both Gaussians, including their errors, from the individual model fits\n",
        "performed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_list = [result.samples for result in result_list]\n",
        "\n",
        "mp_instances = [samps.median_pdf() for samps in samples_list]\n",
        "\n",
        "mp_centres_0 = [instance.gaussian_0.centre for instance in mp_instances]\n",
        "mp_centres_1 = [instance.gaussian_1.centre for instance in mp_instances]\n",
        "\n",
        "ue1_instances = [samp.values_at_upper_sigma(sigma=1.0) for samp in samples_list]\n",
        "le1_instances = [samp.values_at_lower_sigma(sigma=1.0) for samp in samples_list]\n",
        "\n",
        "ue1_centres_0 = [instance.gaussian_0.centre for instance in ue1_instances]\n",
        "le1_centres_0 = [instance.gaussian_0.centre for instance in le1_instances]\n",
        "\n",
        "error_0_list = [ue1 - le1 for ue1, le1 in zip(ue1_centres_0, le1_centres_0)]\n",
        "\n",
        "values_0 = np.asarray(mp_centres_0)\n",
        "sigmas_0 = np.asarray(error_0_list)\n",
        "\n",
        "weights_0 = 1 / sigmas_0 ** 2.0\n",
        "weight_averaged_0 = np.sum(1.0 / sigmas_0 ** 2)\n",
        "\n",
        "weighted_centre_0 = np.sum(values_0 * weights_0) / np.sum(weights_0, axis=0)\n",
        "weighted_error_0 = 1.0 / np.sqrt(weight_averaged_0)\n",
        "\n",
        "ue1_centres_1 = [instance.gaussian_1.centre for instance in ue1_instances]\n",
        "le1_centres_1 = [instance.gaussian_1.centre for instance in le1_instances]\n",
        "\n",
        "error_1_list = [ue1 - le1 for ue1, le1 in zip(ue1_centres_1, le1_centres_1)]\n",
        "\n",
        "values_1 = np.asarray(mp_centres_1)\n",
        "sigmas_1 = np.asarray(error_1_list)\n",
        "\n",
        "weights_1 = 1 / sigmas_1 ** 2.0\n",
        "weight_averaged_1 = np.sum(1.0 / sigmas_1 ** 2)\n",
        "\n",
        "weighted_centre_1 = np.sum(values_1 * weights_1) / np.sum(weights_1, axis=0)\n",
        "weighted_error_1 = 1.0 / np.sqrt(weight_averaged_1)\n",
        "\n",
        "print(\n",
        "    f\"Centre 0 via Weighted Average: {weighted_centre_0} ({weighted_error_0}) [1.0 sigma confidence intervals]\"\n",
        ")\n",
        "print(\n",
        "    f\"Centre 1 via Weighted Average: {weighted_centre_1} ({weighted_error_1}) [1.0 sigma confidence intervals]\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The estimate of the centres is not accurate, with both estimates well offset from the input values of 40 and 60.\n",
        "\n",
        "We will next show that the graphical model offers a notable improvement, but first lets consider why this\n",
        "approach is suboptimal.\n",
        "\n",
        "The most important difference between this model and the model fitted in the previous tutorial is that there are now\n",
        "two shared parameters we are trying to estimate, which are degenerate with one another.\n",
        "\n",
        "We can see this by inspecting the probability distribution function (PDF) of the fit, placing particular focus on the \n",
        "2D degeneracy between the Gaussians centres. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_plotter = aplt.DynestyPlotter(samples=result_list[0].samples)\n",
        "search_plotter.cornerplot()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The problem is that the simple approach of taking a weighted average does not capture the curved banana-like shape\n",
        "of the PDF between the two centres. This leads to significant error over estimation and biased inferences on the centre.\n",
        "\n",
        "__Discussion__\n",
        "\n",
        "Let us now consider other downsides of fitting each dataset one-by-one, from a statistical perspective. We \n",
        "will contrast these to the graphical model later in the tutorial:\n",
        "\n",
        "1) By fitting each dataset one-by-one this means that each model-fit fails to fully exploit the information we know \n",
        "about the global model. We know that there are only two single shared values of `centre` across the full dataset \n",
        "that we want to estimate. However, each individual fit has its own `centre` value which is able to assume different \n",
        "values than the `centre` values used to fit the other datasets. This means that large degeneracies between the two \n",
        "centres are present in each model-fit.\n",
        "\n",
        "By not fitting our model as a global model, we do not maximize the amount of information that we can extract from the \n",
        "dataset as a whole. If a model fits dataset 1 poorly, this should be reflected in how we interpret how well the model \n",
        "fits datasets 2 and 3. Our non-linear search should have a global view of how well the model fits the whole dataset. \n",
        "This is the crucial aspect of fitting each dataset individually that we miss, and what a graphical model addresses.\n",
        "\n",
        "2) When we combined the result to estimate the global `centre` value via a weighted average, we marginalized over \n",
        "the samples in 1D. As showed above, when there are strong degeneracies between models parameters the information on \n",
        "the covariance between these parameters is lost when computing the global `centre`. This increases the inferred \n",
        "uncertainties. A graphical model performs no such 1D marginalization and therefore fully samples the\n",
        "parameter covariances.\n",
        "\n",
        "3) In Bayesian inference it is important that we define priors on all of the model parameters. By estimating the \n",
        "global `centre` after the model-fits are completed it is unclear what prior the global `centre` actually has! We\n",
        "actually defined the prior five times -- once for each fit -- which is not a well defined prior. In a graphical model \n",
        "the prior is clearly defined.\n",
        "\n",
        "What would have happened if we had estimate the shared centres via 2D posterior multiplication using a KDE? We\n",
        "will discuss this at the end of the tutorial after fitting a graphical model.\n",
        "\n",
        "__Model (Graphical)__\n",
        "\n",
        "We now compose a graphical model and fit it.\n",
        "\n",
        "Our model now consists of two Gaussians with two `centre_shared_prior` variables, such that the same centres are\n",
        "used for each Gaussians across all datasets. \n",
        "\n",
        "We again restrict one Gaussian's centre between pixels 0 -> 50 and the other 50 -> 100 to remove solutions where\n",
        "the Gaussians flip location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "centre_0_shared_prior = af.UniformPrior(lower_limit=0.0, upper_limit=50.0)\n",
        "centre_1_shared_prior = af.UniformPrior(lower_limit=50.0, upper_limit=100.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now set up a list of `Model`'s, each of which contain two `Gaussian`'s that are used to fit each of the datasets \n",
        "loaded above.\n",
        "\n",
        "All of these `Model`'s use the `centre_shared_prior`'s abpve. This means all model-components use the same value \n",
        "of `centre` for every model composed and fitted. \n",
        "\n",
        "For a fit to five datasets (each using two Gaussians), this reduces the dimensionality of parameter space \n",
        "from N=30 (e.g. 6 parameters per pair of Gaussians) to N=22 (e.g. 10 `sigma`'s 10 `normalizations` and 2 `centre`'s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_list = []\n",
        "\n",
        "for model_index in range(len(data_list)):\n",
        "    gaussian_0 = af.Model(af.ex.Gaussian)\n",
        "    gaussian_1 = af.Model(af.ex.Gaussian)\n",
        "\n",
        "    gaussian_0.centre = centre_0_shared_prior  # This prior is used by all Gaussians!\n",
        "    gaussian_1.centre = centre_1_shared_prior  # This prior is used by all Gaussians!\n",
        "\n",
        "    model = af.Collection(gaussian_0=gaussian_0, gaussian_1=gaussian_1)\n",
        "\n",
        "    model_list.append(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factors__\n",
        "\n",
        "We again create the graphical model using `AnalysisFactor` objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "The analysis factors are then used to create the factor graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The factor graph model can again be printed via the `info` attribute, which shows that there are two shared\n",
        "parameters across the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "We can now create a non-linear search and use it to the fit the factor graph, again using its `global_prior_model` \n",
        "property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"howtofit\", \"chapter_graphical_models\"),\n",
        "    name=\"tutorial_3_graphical_benefits\",\n",
        "    sample=\"rwalk\",\n",
        ")\n",
        "\n",
        "result = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result's `info` attribute shows that the result is expressed following the same structure of analysis factors\n",
        "that the `global_prior_model.info` attribute revealed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now inspect the inferred `centre` values and compare this to the values estimated above via a weighted average.  \n",
        "\n",
        "(The errors of the weighted average is what was estimated for a run on my PC, yours may be slightly different!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "centre_0 = result.samples.median_pdf()[0].gaussian_0.centre\n",
        "\n",
        "u1_error_0 = result.samples.values_at_upper_sigma(sigma=1.0)[0].gaussian_0.centre\n",
        "l1_error_0 = result.samples.values_at_lower_sigma(sigma=1.0)[0].gaussian_0.centre\n",
        "\n",
        "u3_error_0 = result.samples.values_at_upper_sigma(sigma=3.0)[0].gaussian_0.centre\n",
        "l3_error_0 = result.samples.values_at_lower_sigma(sigma=3.0)[0].gaussian_0.centre\n",
        "\n",
        "centre_1 = result.samples.median_pdf()[0].gaussian_1.centre\n",
        "\n",
        "u1_error_1 = result.samples.values_at_upper_sigma(sigma=1.0)[0].gaussian_1.centre\n",
        "l1_error_1 = result.samples.values_at_lower_sigma(sigma=1.0)[0].gaussian_1.centre\n",
        "\n",
        "u3_error_1 = result.samples.values_at_upper_sigma(sigma=3.0)[0].gaussian_1.centre\n",
        "l3_error_1 = result.samples.values_at_lower_sigma(sigma=3.0)[0].gaussian_1.centre\n",
        "\n",
        "print(\n",
        "    f\"Centre 0 via Weighted Average: 29.415828686393333 (15.265325182888517) [1.0 sigma confidence intervals] \\n\"\n",
        ")\n",
        "print(\n",
        "    f\"Centre 1 via Weighted Average: 54.13825075629124 (2.3460686758693234) [1.0 sigma confidence intervals] \\n\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Inferred value of Gaussian 0's shared centre via a graphical fit to {total_datasets} datasets: \\n\"\n",
        ")\n",
        "print(\n",
        "    f\"{centre_0} ({l1_error_0} {u1_error_0}) ({u1_error_0 - l1_error_0}) [1.0 sigma confidence intervals]\"\n",
        ")\n",
        "print(\n",
        "    f\"{centre_0} ({l3_error_0} {u3_error_0}) ({u3_error_0 - l3_error_0}) [3.0 sigma confidence intervals]\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Inferred value of Gaussian 1's shared centre via a graphical fit to {total_datasets} datasets: \\n\"\n",
        ")\n",
        "print(\n",
        "    f\"{centre_1} ({l1_error_1} {u1_error_1}) ({u1_error_1 - l1_error_1}) [1.0 sigma confidence intervals]\"\n",
        ")\n",
        "print(\n",
        "    f\"{centre_1} ({l3_error_1} {u3_error_1}) ({u3_error_1 - l3_error_1}) [3.0 sigma confidence intervals]\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, using a graphical model allows us to infer a more precise and accurate model.\n",
        "\n",
        "__Discussion__\n",
        "\n",
        "Unlike a fit to each dataset one-by-one, the graphical model:\n",
        "\n",
        "1) Infers a PDF on the global centre that fully accounts for the degeneracies between the models fitted to different \n",
        "datasets. This reduces significantly the large 2D degeneracies between the two centres we saw when inspecting the PDFs \n",
        "of each individual fit.\n",
        "\n",
        "2) Fully exploits the information we know about the global model, for example that the centre of every Gaussian in every \n",
        "dataset is aligned. Now, the fit of the Gaussian in dataset 1 informs the fits in datasets 2 and 3, and visa versa.\n",
        "\n",
        "3) Has a well defined prior on the global centre, instead of 5 independent priors on the centre of each dataset.\n",
        "\n",
        "__Posterior Multiplication__\n",
        "\n",
        "What if we had combined the results of the individual model fits using 2D posterior multiplication via a KDE?\n",
        "\n",
        "This would produce an inaccurate estimate of the error, because each posterior contains the prior on the centre five \n",
        "times which given the properties of this model should not be repeated.\n",
        "\n",
        "However, it is possible to convert each posterior to a likelihood (by dividing by its prior), combining these 5\n",
        "likelihoods to form a joint likelihood via 2D KDE multiplication and then insert just one prior back (again using a 2D\n",
        "KDE) at the end to get a posterior which does not have repeated priors. \n",
        "\n",
        "This posterior, in theory, should be equivalent to the graphical model, giving the same accurate estimates of the\n",
        "centres with precise errors. The process extracts the same information, fully accounting for the 2D structure of the\n",
        "PDF between the two centres for each fit.\n",
        "\n",
        "However, in practise, this will likely not work well. Every time we use a KDE to represent and multiply a posterior, we \n",
        "make an approximation which will impact our inferred errors. The removal of the prior before combining the likelihood\n",
        "and reinserting it after also introduces approximations, especially because the fit performed by the non-linear search\n",
        "is informed by the prior. \n",
        "\n",
        "Crucially, whilst posterior multiplication can work in two dimensions, for models with many more dimensions and \n",
        "degeneracies between parameters that are in 3D, 4D of more dimensions it will introduce more and more numerical\n",
        "inaccuracies.\n",
        "\n",
        "A graphical model fully samples all of the information a large dataset contains about the model, without making \n",
        "such large approximation. Therefore, irrespective of how complex the model gets, it extracts significantly more \n",
        "information contained in the dataset.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "In this tutorial, we demonstrated the strengths of a graphical model over fitting each dataset one-by-one. \n",
        "\n",
        "We argued that irrespective of how one may try to combine the results of many individual fits, the approximations that \n",
        "are made will always lead to a suboptimal estimation of the model parameters and fail to fully extract all information\n",
        "from the dataset. \n",
        "\n",
        "We argued that for high dimensional complex models a graphical model is the only way to fully extract all of the \n",
        "information contained in the dataset.\n",
        "\n",
        "In the next tutorial, we will consider a natural extension of a graphical model called a hierarchical model.\n",
        "\n",
        "Tutorial 4: Hierarchical\n",
        "========================\n",
        "\n",
        "In the previous tutorial, we fitted a graphical model with the aim of determining an estimate of shared parameters,\n",
        "the `centre`'s of a dataset of 1D Gaussians. We did this by fitting all datasets simultaneously. When there are shared\n",
        "parameters in a model, this is a powerful and effective tool, but things may not always be so simple.\n",
        "\n",
        "A common extension to the problem is one where we expect that the shared parameter(s) of the model do not have exactly\n",
        "the same value in every dataset. Instead, our expectation is that the parameter(s) are drawn from a common\n",
        "parent distribution (e.g. a Gaussian distribution). It is the parameters of this distribution that we consider shared\n",
        "across the dataset (e.g. the means and scatter of the Gaussian distribution). These are the parameters we ultimately\n",
        "wish to infer to understand the global behaviour of our model.\n",
        "\n",
        "This is called a hierarchical model, whihc we fit in this tutorial. The `centre` of each 1D Gaussian is now no\n",
        "longer the same in each dataset and they are instead drawn from a shared parent Gaussian distribution\n",
        "(with `mean=50.0` and `sigma=10.0`). The hierarchical model will recover the `mean` and `sigma` values of the parent\n",
        "distribution'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and \n",
        " `visualize` functions.\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        " - `plot_profile_1d`: a function for plotting 1D profile datasets including their noise.\n",
        "\n",
        "These are functionally identical to the `Analysis`, `Gaussian` and `plot_profile_1d` objects and functions you \n",
        "have seen and used elsewhere throughout the workspace.\n",
        "\n",
        "__Dataset__\n",
        "\n",
        "For each dataset we now set up the correct path and load it. \n",
        "\n",
        "We are loading a new Gaussian dataset, where the Gaussians have different centres which were drawn from a parent\n",
        "Gaussian distribution with a mean centre value of 50.0 and sigma of 10.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_datasets = 5\n",
        "\n",
        "dataset_name_list = []\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", \"gaussian_x1__hierarchical\", dataset_name\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    dataset_name_list.append(dataset_name)\n",
        "    data_list.append(data)\n",
        "    noise_map_list.append(noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting the Gaussians we can just about make out that their centres are not all at pixel 50, and are spread out\n",
        "around it (albeit its difficult to be sure, due to the low signal-to-noise of the data). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name, data in zip(dataset_name_list, data_list):\n",
        "    af.ex.plot_profile_1d(\n",
        "        xvalues=np.arange(data.shape[0]),\n",
        "        profile_1d=data,\n",
        "        title=dataset_name,\n",
        "        ylabel=\"Data Values\",\n",
        "        color=\"k\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "For each dataset we now create a corresponding `Analysis` class, like in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    analysis_list.append(analysis)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Individual Factors__\n",
        "\n",
        "We first set up a model for each `Gaussian` which is individually fitted to each 1D dataset, which forms the\n",
        "factors on the factor graph we compose. \n",
        "\n",
        "This uses a nearly identical for loop to the previous tutorials, however a shared `centre` is no longer used and each \n",
        "`Gaussian` is given its own prior for the `centre`. \n",
        "\n",
        "We will see next how this `centre` is used to construct the hierarchical model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_list = []\n",
        "\n",
        "for model_index in range(len(data_list)):\n",
        "    gaussian = af.Model(af.ex.Gaussian)\n",
        "\n",
        "    gaussian.centre = af.GaussianPrior(\n",
        "        mean=50.0, sigma=20.0, lower_limit=0.0, upper_limit=100.0\n",
        "    )\n",
        "    gaussian.normalization = af.GaussianPrior(mean=3.0, sigma=5.0, lower_limit=0.0)\n",
        "    gaussian.sigma = af.GaussianPrior(mean=10.0, sigma=10.0, lower_limit=0.0)\n",
        "\n",
        "    model_list.append(gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factors__\n",
        "\n",
        "Now we have our `Analysis` classes and model components, we can compose our `AnalysisFactor`'s.\n",
        "\n",
        "These are composed in the same way as for the graphical model in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We now compose the hierarchical model that we fit, using the individual Gaussian model components created above.\n",
        "\n",
        "We first create a `HierarchicalFactor`, which represents the parent Gaussian distribution from which we will assume \n",
        "that the `centre` of each individual `Gaussian` dataset is drawn. \n",
        "\n",
        "For this parent `Gaussian`, we have to place priors on its `mean` and `sigma`, given that they are parameters in our\n",
        "model we are ultimately fitting for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "hierarchical_factor = af.HierarchicalFactor(\n",
        "    af.GaussianPrior,\n",
        "    mean=af.GaussianPrior(mean=50.0, sigma=10, lower_limit=0.0, upper_limit=100.0),\n",
        "    sigma=af.GaussianPrior(mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=100.0),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now add each of the individual model `Gaussian`'s `centre` parameters to the `hierarchical_factor`.\n",
        "\n",
        "This composes the hierarchical model whereby the individual `centre` of every `Gaussian` in our dataset is now assumed \n",
        "to be drawn from a shared parent distribution. It is the `mean` and `sigma` of this distribution we are hoping to \n",
        "estimate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "for model in model_list:\n",
        "    hierarchical_factor.add_drawn_variable(model.centre)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "We now create the factor graph for this model, using the list of `AnalysisFactor`'s and the hierarchical factor.\n",
        "\n",
        "Note that the `hierarchical_factor` is passed in below, which was not the case in previous tutorials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list, hierarchical_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The factor graph model `info` attribute shows that the hierarchical factor's parameters are included in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "We can now create a non-linear search and used it to the fit the hierarchical model, again using \n",
        "its `global_prior_model` property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"howtofit\", \"chapter_graphical_models\"),\n",
        "    name=\"tutorial_4_hierarchical\",\n",
        "    sample=\"rwalk\",\n",
        ")\n",
        "\n",
        "result = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result's `info` attribute shows the result, including the hierarchical factor's parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now inspect the inferred value of hierarchical factor's mean and sigma.\n",
        "\n",
        "We see that they are consistent with the input values of `mean=50.0` and `sigma=10.0`.\n",
        "\n",
        "The hierarchical factor results are at the end of the samples list, hence why we extract them using `[-1]` and [-2]`\n",
        "below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "mean = samples.median_pdf(as_instance=False)[-2]\n",
        "\n",
        "u1_error = samples.values_at_upper_sigma(sigma=1.0)[-2]\n",
        "l1_error = samples.values_at_lower_sigma(sigma=1.0)[-2]\n",
        "\n",
        "u3_error = samples.values_at_upper_sigma(sigma=3.0)[-2]\n",
        "l3_error = samples.values_at_lower_sigma(sigma=3.0)[-2]\n",
        "\n",
        "print(\n",
        "    \"Inferred value of the mean of the parent hierarchical distribution for the centre: \\n\"\n",
        ")\n",
        "print(f\"{mean} ({l1_error} {u1_error}) [1.0 sigma confidence intervals]\")\n",
        "print(f\"{mean} ({l3_error} {u3_error}) [3.0 sigma confidence intervals]\")\n",
        "\n",
        "scatter = samples.median_pdf(as_instance=False)[-1]\n",
        "\n",
        "u1_error = samples.values_at_upper_sigma(sigma=1.0)[-1]\n",
        "l1_error = samples.values_at_lower_sigma(sigma=1.0)[-1]\n",
        "\n",
        "u3_error = samples.values_at_upper_sigma(sigma=3.0)[-1]\n",
        "l3_error = samples.values_at_lower_sigma(sigma=3.0)[-1]\n",
        "\n",
        "print(\n",
        "    \"Inferred value of the scatter (the sigma value of the Gassuain) of the parent hierarchical distribution for the centre: \\n\"\n",
        ")\n",
        "print(f\"{scatter} ({l1_error} {u1_error}) [1.0 sigma confidence intervals]\")\n",
        "print(f\"{scatter} ({l3_error} {u3_error}) [3.0 sigma confidence intervals]\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Comparison to One-by-One Fits__\n",
        "\n",
        "We can compare the inferred values above to the values inferred for individual fits in the \n",
        "tutorial `tutorial_optional_hierarchical_individual.py`.\n",
        "\n",
        "This fits the hierarchical model is a much simpler way -- fitting each dataset one-by-one and then fitting the \n",
        "parent Gaussian distribution to those results.\n",
        "\n",
        "For the results below, inferred on my laptop, we can see that the correct mean and scatter of the parent Gaussian is \n",
        "inferred but the errors are much larger than the graphical model fit above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"Inferred value of the mean of the parent hierarchical distribution for one-by-one fits: \\n\"\n",
        ")\n",
        "print(\n",
        "    \"50.00519854538594 (35.825675441265815 65.56274024242403) [1.0 sigma confidence intervals]\"\n",
        ")\n",
        "print(\n",
        "    \"50.00519854538594 (1.3226539914914734 96.92151898283811) [3.0 sigma confidence intervals]\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Inferred value of the scatter of the parent hierarchical distribution for one-by-one fits: \\n\"\n",
        ")\n",
        "print(\n",
        "    \"15.094393493747617 (4.608862348173649 31.346751522582483) [1.0 sigma confidence intervals]\"\n",
        ")\n",
        "print(\n",
        "    \"15.094393493747617 (0.060533647989089806 49.05537884440667) [3.0 sigma confidence intervals]\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Benefits of Graphical Model__\n",
        "\n",
        "We compared the results inferred in this script via a graphical model to a simpler approach which fits each dataset \n",
        "one-by-one and infers the hierarchical parent distribution's parameters afterwards.\n",
        "\n",
        "The graphical model provides a more accurate and precise estimate of the parent distribution's parameters. This is \n",
        "because the fit to each dataset informs the hierarchical distribution's parameters, which in turn improves\n",
        "constraints on the other datasets. In a hierarchical fit, we describe this as \"the datasets talking to one another\". \n",
        "\n",
        "For example, by itself, dataset_0 may give weak constraints on the centre spanning the range 20 -> 85 at 1 sigma \n",
        "confidence. Now, consider if simultaneously all of the other datasets provide strong constraints on the \n",
        "hierarchical's distribution's parameters, such that its `mean = 50 +- 5.0` and `sigma = 10.0 +- 2.0` at 1 sigma \n",
        "confidence. \n",
        "\n",
        "This will significantly change our inferred parameters for dataset 0, as the other datasets inform us\n",
        "that solutions where the centre is well below approximately 40 are less likely, because they are inconsistent with\n",
        "the parent hierarchical distribution's parameters!\n",
        "\n",
        "For complex graphical models with many hierarchical factors, this phenomena of the \"datasets talking to one another\" \n",
        "is crucial in breaking degeneracies between parameters and maximally extracting information from extremely large\n",
        "datasets.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "By composing and fitting hierarchical models in the graphical modeling framework we can fit for global trends\n",
        "within large datasets. The tools applied in this tutorial and the previous tutorial can be easily extended to \n",
        "compose complex graphical models, with multiple shared parameters and hierarchical factors.\n",
        "\n",
        "However, there is a clear challenge scaling the graphical modeling framework up in this way: model complexity. As the \n",
        "model becomes more complex, an inadequate sampling of parameter space will lead one to infer local maxima. Furthermore,\n",
        "one will soon hit computational limits on how many datasets can feasibly be fitted simultaneously, both in terms of\n",
        "CPU time and memory limitations. \n",
        "\n",
        "Therefore, the next tutorial introduces expectation propagation, a framework that inspects the factor graph of a \n",
        "graphical model and partitions the model-fit into many separate fits on each graph node. When a fit is complete, \n",
        "it passes the information learned about the model to neighboring nodes. \n",
        "\n",
        "Therefore, graphs comprising hundreds of model components (and tens of thousands of parameters) can be fitted as \n",
        "many bite-sized model fits, where the model fitted at each node consists of just tens of parameters. This makes \n",
        "graphical models scalable to largest datasets and most complex models!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}