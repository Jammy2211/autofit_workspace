{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 1: Global Models\n",
        "=========================\n",
        "\n",
        "In chapter 1, we primarily focused on fitting one model to one dataset. We have inspected the results of those\n",
        "individual model-fits using tools such as the `Aggregator` and demonstrated that when necessary we are able to fit\n",
        "more complex models, for example 1D profiles composed of 2 or 3 Gaussians / Exponentials.\n",
        "\n",
        "However, for many problems we may have a large dataset, and we are not particularly interested in how well the model\n",
        "fits each dataset individually. Instead, we wish to fit this model (or many similar models) to the full\n",
        "dataset and determine the 'global' trends of the model across the dataset.\n",
        "\n",
        "In this chapter we demonstrate how this can be done using graphical models, which not only fit the model to each\n",
        "individual dataset but simultaneously compose and fit higher-level model components that seek to capture global trends\n",
        "of the model across the data.\n",
        "\n",
        "To take a real world example, in a healthcare setting we are not interested in whether a treatment successfully helps\n",
        "an individual patient recover. We can only claim that a treatment is truly effective if it successfully helps a\n",
        "large sample of patients to show a higher chance of recover. Medical datasets may contain data on thousands of patients\n",
        "which each need modeled individually to determine these global trends. Using the methods of chapter 1 this require us\n",
        "to fit models with thousands of parameters, which is computationally unfeasiable. Graphical models provide us with a\n",
        "means to break this fitting procedure down into a computationally tractable problem.\n",
        "\n",
        "In this tutorial, we will demonstrate the problem using our toy model of fitting noisy 1D datasets. We use a dataset\n",
        "containing 3 noisy 1D Gaussians and we will suppose that we *know* that all 3 Gaussians have the same `centre`,\n",
        "which we term the global `centre` value in our full dataset that we seek to estimate.\n",
        "\n",
        "The datasets fitted in this tutorial are much lower signal-to-noise than those fitted in chapter 1 and there will be a\n",
        "large uncertainty on the value of `centre` in an individual model-fit. In this tutorial we will consider attempt to\n",
        "estimate the global `centre` without using a graphical model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "from os import path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the `Analysis` class of chapter 1, which includes `visualize` and `save_attributes_for_aggregator` methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from analysis import Analysis"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll fit a single Gaussian as our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import profiles as p\n",
        "\n",
        "model = af.CollectionPriorModel(gaussian=p.Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform visualization we'll plot the 1D data as a line. \n",
        "\n",
        "To facilitate this we define the function `plot_line` below, which uses Matplotlib to create the 1D plots we've seen \n",
        "in previous tutorials. This function has additional inputs so the plot can be output to a specified output path with a \n",
        "given output file name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def plot_line(xvalues, line, title=None, ylabel=None, errors=None, color=\"k\"):\n",
        "    \"\"\"\n",
        "    Plot a 1D line of data on a plot of x versus y, where the x-axis is the x coordinate of the line and the y-axis\n",
        "    is the intensity of the line at that coordinate.\n",
        "\n",
        "    The function include options to output the image to the hard-disk as a .png.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    xvalues : np.ndarray\n",
        "        The x-coordinates the profile is defined on.\n",
        "    line : np.ndarray\n",
        "        The intensity values of the profile which are plotted.\n",
        "    ylabel : str\n",
        "        The y-label of the plot.\n",
        "    \"\"\"\n",
        "    plt.errorbar(\n",
        "        x=xvalues, y=line, yerr=errors, color=color, ecolor=\"k\", elinewidth=1, capsize=2\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x value of profile\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()\n",
        "    plt.clf()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each dataset we now set up the correct path, load it, and plot it, to show how low signal-to-noise it is!\n",
        "\n",
        "fit it using a `NonLinearSearch`, as we did in \n",
        "tutorial 7 of chapter 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_names = [\n",
        "    \"gaussian_x1_0__low_snr\",\n",
        "    \"gaussian_x1_1__low_snr\",\n",
        "    \"gaussian_x1_2__low_snr\",\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This for loop runs over every dataset, checkout the comments below for how we set up the path structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name in dataset_names:\n",
        "\n",
        "    \"\"\"Load the dataset from the `autofit_workspace/dataset` folder.\"\"\"\n",
        "    dataset_path = path.join(\"dataset\", \"example_1d\", dataset_name)\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    plot_line(\n",
        "        xvalues=np.arange(data.shape[0]),\n",
        "        line=data,\n",
        "        title=dataset_name,\n",
        "        ylabel=\"Data Values\",\n",
        "        color=\"k\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For every dataset we now create an `Analysis` class using it and use `Emcee` to fit it with a `Gaussian`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name in dataset_names:\n",
        "\n",
        "    \"\"\"Load the dataset from the `autofit_workspace/dataset` folder.\"\"\"\n",
        "    dataset_path = path.join(\"dataset\", \"example_1d\", dataset_name)\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    \"\"\"Create the `DynestyStatic` `NonLinearSearch` and use it to fit the data.\"\"\"\n",
        "    analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    emcee = af.DynestyStatic(\n",
        "        name=dataset_name,\n",
        "        path_prefix=path.join(\n",
        "            \"howtofit\", \"tutorial_1_global_model\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Emcee has begun running, checkout \\n\"\n",
        "        f\"autofit_workspace/output/howtofit/chapter_graphica_models/tutorial_1_global_model/{dataset_name} for live \\n\"\n",
        "        f\"output of the results. This Jupyter notebook cell with progress once Emcee has completed, this could take a \\n\"\n",
        "        f\"few minutes!\"\n",
        "    )\n",
        "\n",
        "    emcee.fit(model=model, analysis=analysis)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkout the output folder, you should see three new sets of results corresponding to our 3 `Gaussian` datasets.\n",
        "\n",
        "In the `model.results` file of each fit, it will be clear that the `centre` value of every fit (and the other \n",
        "parameters) have much larger errors than other **PyAutoFit** examples due to the low signal to noise of the data.\n",
        ".\n",
        "We now load the results of all 3 model-fits using the `Aggregator`, so we can try determine the global `centre` value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg = af.Aggregator(\n",
        "    directory=path.join(\n",
        "        \"output\", \"howtofit\", \"chapter_graphical_models\", \"tutorial_1_global_model\"\n",
        "    )\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The aggregator allows us to plot the median PDF value and 3.0 confidence intervals of the `centre` estimate from\n",
        "the model-fit to each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mp_instances = [samps.median_pdf_instance for samps in agg.values(\"samples\")]\n",
        "ue3_instances = [\n",
        "    samp.error_instance_at_upper_sigma(sigma=3.0) for samp in agg.values(\"samples\")\n",
        "]\n",
        "le3_instances = [\n",
        "    samp.error_instance_at_lower_sigma(sigma=3.0) for samp in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "mp_centres = [instance.gaussian.centre for instance in mp_instances]\n",
        "ue3_centres = [instance.gaussian.centre for instance in ue3_instances]\n",
        "le3_centres = [instance.gaussian.centre for instance in le3_instances]\n",
        "\n",
        "print(mp_centres)\n",
        "\n",
        "plt.errorbar(\n",
        "    x=[\"Gaussian 1\", \"Gaussian 2\", \"Gaussian 3\"],\n",
        "    y=mp_centres,\n",
        "    marker=\".\",\n",
        "    linestyle=\"\",\n",
        "    yerr=[le3_centres, ue3_centres],\n",
        ")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# \"\"\"\n",
        "# These model-fits are consistent with a range range of global `centre` values. We could also show this by plotting the\n",
        "# 1D PDF's of each model fit, using the library:\n",
        "#\n",
        "#  corner.py: https://corner.readthedocs.io/en/latest/\n",
        "#\n",
        "# (In built visualization for PDF's and non-linear searches is a future feature of PyAutoFit, but for now you'll have to\n",
        "# use the libraries yourself!).\n",
        "# \"\"\"\n",
        "#\n",
        "# import corner\n",
        "#\n",
        "#\n",
        "# samples = list(agg.values(\"samples\"))\n",
        "# print(samples[0].parameters_extract[2])\n",
        "# pdf = corner.quantile(x=samples[0].parameters_extract[2], q=np.linspace(0.0, 1.0, 50), weights=samples[0].weights)\n",
        "# print(pdf)\n",
        "# # pdfs = [corner.quantile(x=samps.parameters, q=np.linspace(0.0, 1.0, 20), weights=samps.weights) for samps in samples]\n",
        "# #print(pdfs)\n",
        "#\n",
        "# #plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So how might we estimate our global `centre`? We could take the mean of the data point and?\n",
        "Alternatively, we could combine the samples into a joint set of samples and compute their joint PDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO : RICH, can you make this so we can just add Samples objects together? E.g.:\n",
        "# TODO : samples_joint = samples_0 + samples_1 + samples_2 or samples_joint = sum(agg.values(\"samples\"))\n",
        "\n",
        "samples_gen = agg.values(\"samples\")\n",
        "\n",
        "parameters_joint = []\n",
        "log_likelihoods_joint = []\n",
        "log_priors_joint = []\n",
        "weights_joint = []\n",
        "log_evidence = 0.0\n",
        "total_samples = 0\n",
        "\n",
        "for samples in agg.values(\"samples\"):\n",
        "\n",
        "    model = samples.model\n",
        "    parameters_joint += samples.parameters\n",
        "    log_likelihoods_joint += samples.log_likelihoods\n",
        "    log_priors_joint += samples.log_priors\n",
        "    weights_joint += samples.weights\n",
        "    number_live_points = samples.number_live_points\n",
        "    log_evidence += samples.log_evidence\n",
        "    total_samples += samples.total_samples\n",
        "\n",
        "# samples_joint = af.NestSamples(\n",
        "#     model=model,\n",
        "#     parameters=parameters_joint,\n",
        "#     log_likelihoods=log_likelihoods_joint,\n",
        "#     log_priors=log_priors_joint,\n",
        "#     weights=weights_joint,\n",
        "#     number_live_points=number_live_points,\n",
        "#     log_evidence=log_evidence,\n",
        "#     total_samples=total_samples,\n",
        "# )\n",
        "#\n",
        "# print(samples_joint.median_pdf_instance.gaussian.centre)\n",
        "# print(samples_joint.error_instance_at_upper_sigma(sigma=3.0).gaussian.centre)\n",
        "# print(samples_joint.error_instance_at_lower_sigma(sigma=3.0).gaussian.centre)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets wrap up the tutorial. The methods used above to combine the results of different fits and estimate a global \n",
        "value of `centre` alongside estimates of its error. This is great, but it is unfortunately far from optimal for a\n",
        "number of reasons:\n",
        "\n",
        " - When we combined results to estimate the global `centre`, we marginalized over the samples in 1D. This occured when \n",
        " we took the mean of each `centre` estimate and errors and when using a joint PDF. If there are strong degeneracies\n",
        " between models parameters (which there is between `centre` and $I$) information on the covariance between these\n",
        " parameters is lost when computing the global `centre`, increasing the estimate uncertanties and potentially biasing \n",
        " the estimated mean value.\n",
        " \n",
        " - In Bayesian inference it is important we define priors on all model parameters. By estimating the global `centre` \n",
        " value after the model-fits were completed it is unclear what our prior on the global `centre` actually is, as we \n",
        " never defined one! \n",
        "\n",
        "Fitting each dataset one-by-one also means that each model-fit fails to fully exploit all of the information we know \n",
        "about the global model. At the beginning of the tutorial, we stated that there was a single global value of `centre` \n",
        "that is the same across the dataset. However, each individual fit had its own `centre` value which was free to be \n",
        "different to the `centre` values used to fit the other datasets, in contradiction to the global model! \n",
        " \n",
        "The goodness of a model's fit to each dataset *should* inform us on how we the model fits the other datasets. If a \n",
        "model fits dataset 1 really well, but not datasets 2 and 3, the fit to dataset 1 should reflect this *during the \n",
        "non-linear search*! The fact we *know* there is only a single global `centre` value should be reflected in the \n",
        "non-linear search. \n",
        "\n",
        "If we could set up a model-fit that fitted all 3 datasets simultaneously using the same `centre` value for every \n",
        "likelihood evaluation, we would be able to fully exploit our knowledge of the global-model to produce an improved \n",
        "estimate of the global `centre`. This is the topic of the next tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}