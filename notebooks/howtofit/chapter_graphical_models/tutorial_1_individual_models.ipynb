{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 1: Individual Modelss\n",
        "=========================\n",
        "\n",
        "In chapter 1, we focused on fitting one model to one dataset. We inspected the results of those individual model-fits\n",
        "and demonstrated that when necessary we are able to fit more complex models, for example 1D profiles composed of 2 or 3\n",
        "Gaussians / Exponentials.\n",
        "\n",
        "For many problems we may have a large dataset and not be interested in how well the model fits each individual dataset.\n",
        "Instead, our interest is fitting this dataset in its entirety, to determine 'global' trends of how the model fits the\n",
        "data.\n",
        "\n",
        "This chapter of **HowToFit** shows you how to compose and fit graphical models to these large datasets. We'll see that\n",
        "a graphical model links together the many individual models that fit each dataset, but that it also contains shared\n",
        "parameters that describe the global behaviour of how the model fits the full dataset.\n",
        "\n",
        "Lets consider a real world example. In a healthcare setting, we may fit a model to some data and be able to show\n",
        "that a certain treatment helps an individual patient recover from illness. However, for us to claim this treatment is\n",
        "effective more generally, we would have to fit this model to a large patient dataset. One could imagine that we would\n",
        "compose and fit many individual models to the dataset on each patient, but now link them together with shared parameters\n",
        "describing how effective the treatment is across the full global population.\n",
        "\n",
        "Medical datasets may contain data on thousands of patients, and we may therefore need to fit very complex models with\n",
        "many thousands of parameters to determine global trends. We may have different datasets on different patients, meaning\n",
        "that we need high levels of customization in the models we compose and fitting procedures that we apply. The\n",
        "**PyAutoFit** graphical modeling framework provides the tools we need to do these tasks in a computationally tractable\n",
        "manner.\n",
        "\n",
        "This chapter illustrates the problem using the toy model of fitting noisy 1D datasets. The dataset contains up to 10\n",
        "noisy 1D Gaussians, which all have the same value of `centre=50.0`. The centre is therefore the shared parameter we want\n",
        "to infer across the full dataset -- it is the global aspect of our model we are actually interested in!\n",
        "\n",
        "This tutorial does not use graphical models. Instead, it attempts to estimate the `centre` is the simpliest way\n",
        "imaginable, by fitting each dataset one-by-one and combining the results after all model-fitting is complete. In\n",
        "tutorial 2, we will do the fitting 'properly' using graphical models, and compare to our estimate in this tutorial to\n",
        "see how much our estimate of the `centre` improves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import numpy as np\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and \n",
        " `visualize` functions.\n",
        " \n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        " - `plot_profile_1d`: a function for plotting 1D profile datasets including their noise.\n",
        "\n",
        "These are functionally identical to the `Analysis`, `Gaussian` and `plot_profile_1d` objects and functions you have \n",
        "seen and used elsewhere throughout the workspace.\n",
        "\n",
        "__Model__\n",
        "\n",
        "Our model is a single `Gaussian`. \n",
        "\n",
        "We put this in a `Collection` so that when we extend the model in later tutorials we use the same API throughout\n",
        "all tutorials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.ex.Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "We quickly set up the name of each dataset, which is used below for loading the datasets.\n",
        "\n",
        "The dataset contains 10 Gaussians, but for speed we'll fit just 5. You can change this to 10 to see how the result\n",
        "changes with more datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_datasets = 5\n",
        "\n",
        "dataset_name_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_name_list.append(f\"dataset_{dataset_index}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each 1D Gaussian dataset we now set up the correct path, load it, and plot it. \n",
        "\n",
        "Notice how much lower the signal-to-noise is than you are used too, you probably find it difficult to estimate \n",
        "the centre of some of the Gaussians by eye!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name in dataset_name_list:\n",
        "    \"\"\"\n",
        "    Load the dataset from the `autofit_workspace/dataset` folder.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", \"gaussian_x1__low_snr\", dataset_name\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    af.ex.plot_profile_1d(\n",
        "        xvalues=np.arange(data.shape[0]),\n",
        "        profile_1d=data,\n",
        "        title=dataset_name,\n",
        "        ylabel=\"Data Values\",\n",
        "        color=\"k\",\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fits (one-by-one)__\n",
        "\n",
        "For every dataset we now create an `Analysis` and use `Dynesty` to fit it with a `Gaussian`.\n",
        "\n",
        "The `Result` is stored in the list `result_list`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_list = []\n",
        "\n",
        "for dataset_name in dataset_name_list:\n",
        "    \"\"\"\n",
        "    Load the dataset from the `autofit_workspace/dataset` folder.\n",
        "    \"\"\"\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", \"gaussian_x1__low_snr\", dataset_name\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    For each dataset create a corresponding `Analysis` class.\n",
        "    \"\"\"\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    \"\"\"\n",
        "    Create the `DynestyStatic` non-linear search and use it to fit the data.\n",
        "    \n",
        "    We use custom dynesty settings which ensure the posterior is explored fully and that our error estimates are robust.\n",
        "    \"\"\"\n",
        "    dynesty = af.DynestyStatic(\n",
        "        name=\"global_model\",\n",
        "        path_prefix=path.join(\n",
        "            \"howtofit\", \"chapter_graphical_models\", \"tutorial_1_individual_models\"\n",
        "        ),\n",
        "        unique_tag=dataset_name,\n",
        "        nlive=200,\n",
        "        dlogz=1e-4,\n",
        "        sample=\"rwalk\",\n",
        "        walks=10,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Dynesty has begun running, checkout \\n\"\n",
        "        f\"autofit_workspace/output/howtofit/chapter_graphica_models/tutorial_1_individual_models/{dataset_name} for live \\n\"\n",
        "        f\"output of the results. This Jupyter notebook cell with progress once Dynesty has completed, this could take a \\n\"\n",
        "        f\"few minutes!\"\n",
        "    )\n",
        "\n",
        "    result_list.append(dynesty.fit(model=model, analysis=analysis))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Results__\n",
        "\n",
        "Checkout the output folder, you should see five new sets of results corresponding to our Gaussian datasets.\n",
        "\n",
        "In the `model.results` file of each fit, it will be clear that the `centre` value of every fit (and the other \n",
        "parameters) have much larger errors than other **PyAutoFit** examples due to the low signal to noise of the data.\n",
        "\n",
        "The `result_list` allows us to plot the median PDF value and 3.0 confidence intervals of the `centre` estimate from\n",
        "the model-fit to each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "samples_list = [result.samples for result in result_list]\n",
        "\n",
        "mp_instances = [samps.median_pdf() for samps in samples_list]\n",
        "ue3_instances = [samp.errors_at_upper_sigma(sigma=3.0) for samp in samples_list]\n",
        "le3_instances = [samp.errors_at_lower_sigma(sigma=3.0) for samp in samples_list]\n",
        "\n",
        "mp_centres = [instance.gaussian.centre for instance in mp_instances]\n",
        "ue3_centres = [instance.gaussian.centre for instance in ue3_instances]\n",
        "le3_centres = [instance.gaussian.centre for instance in le3_instances]\n",
        "\n",
        "plt.errorbar(\n",
        "    x=[f\"Gaussian {index}\" for index in range(total_datasets)],\n",
        "    y=mp_centres,\n",
        "    marker=\".\",\n",
        "    linestyle=\"\",\n",
        "    yerr=[le3_centres, ue3_centres],\n",
        ")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These model-fits are consistent with a range of `centre` values. \n",
        "\n",
        "We can show this by plotting the 1D and 2D PDF's of each model fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "for samples in samples_list:\n",
        "    search_plotter = aplt.DynestyPlotter(samples=samples)\n",
        "    search_plotter.cornerplot()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also print the values of each centre estimate, including their estimates at 3.0 sigma.\n",
        "\n",
        "Note that above we used the samples to estimate the size of the errors on the parameters. Below, we use the samples to \n",
        "get the value of the parameter at these sigma confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "u1_instances = [samp.values_at_upper_sigma(sigma=1.0) for samp in samples_list]\n",
        "l1_instances = [samp.values_at_lower_sigma(sigma=1.0) for samp in samples_list]\n",
        "\n",
        "u1_centres = [instance.gaussian.centre for instance in u1_instances]\n",
        "l1_centres = [instance.gaussian.centre for instance in l1_instances]\n",
        "\n",
        "u3_instances = [samp.values_at_upper_sigma(sigma=3.0) for samp in samples_list]\n",
        "l3_instances = [samp.values_at_lower_sigma(sigma=3.0) for samp in samples_list]\n",
        "\n",
        "u3_centres = [instance.gaussian.centre for instance in u3_instances]\n",
        "l3_centres = [instance.gaussian.centre for instance in l3_instances]\n",
        "\n",
        "for index in range(total_datasets):\n",
        "    print(f\"Centre estimate of Gaussian dataset {index}:\\n\")\n",
        "    print(\n",
        "        f\"{mp_centres[index]} ({l1_centres[index]} {u1_centres[index]}) [1.0 sigma confidence interval]\"\n",
        "    )\n",
        "    print(\n",
        "        f\"{mp_centres[index]} ({l3_centres[index]} {u3_centres[index]}) [3.0 sigma confidence interval] \\n\"\n",
        "    )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Estimating the Centre__\n",
        "\n",
        "So how might we estimate our global `centre` value? \n",
        "\n",
        "A simple approach takes the weighted average of the value inferred by all five fits above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ue1_instances = [samp.values_at_upper_sigma(sigma=1.0) for samp in samples_list]\n",
        "le1_instances = [samp.values_at_lower_sigma(sigma=1.0) for samp in samples_list]\n",
        "\n",
        "ue1_centres = [instance.gaussian.centre for instance in ue1_instances]\n",
        "le1_centres = [instance.gaussian.centre for instance in le1_instances]\n",
        "\n",
        "error_list = [ue1 - le1 for ue1, le1 in zip(ue1_centres, le1_centres)]\n",
        "\n",
        "values = np.asarray(mp_centres)\n",
        "sigmas = np.asarray(error_list)\n",
        "\n",
        "weights = 1 / sigmas**2.0\n",
        "weight_averaged = np.sum(1.0 / sigmas**2)\n",
        "\n",
        "weighted_centre = np.sum(values * weights) / np.sum(weights, axis=0)\n",
        "weighted_error = 1.0 / np.sqrt(weight_averaged)\n",
        "\n",
        "print(\n",
        "    f\"Weighted Average Centre Estimate = {weighted_centre} ({weighted_error}) [1.0 sigma confidence intervals]\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Posterior Multiplication__\n",
        "\n",
        "An alternative and more accurate way to combine each individual infered centre is multiply their posteriors together.\n",
        "\n",
        "In order to do this, a smooth 1D profile must be fit to the posteriors via a Kernel Density Estimator (KDE).\n",
        "\n",
        "[**PyAutoFit** does not currently support posterior multiplication and an example illustrating this is currently\n",
        "missing from this tutorial. However, I will discuss KDE multiplication throughout these tutorials to give the\n",
        "reader context for how this approach to parameter estimation compares to graphical models.]\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "Lets wrap up the tutorial. The methods used above combine the results of different fits and estimate a global \n",
        "value of `centre` alongside estimates of its error. \n",
        "\n",
        "In this tutorial, we fitted just 5 datasets. Of course, we could easily fit more datasets, and we would find that\n",
        "as we added more datasets our estimate of the global centre would become more precise.\n",
        "\n",
        "In the next tutorial, we will compare this result to one inferred via a graphical model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}