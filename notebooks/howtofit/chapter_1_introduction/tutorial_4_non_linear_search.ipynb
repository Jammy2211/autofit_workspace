{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 4: Non-linear Search\n",
        "=============================\n",
        "\n",
        "Its finally time to take our model and fit it to data.\n",
        "\n",
        "So, how do we infer the parameters for the 1D `Gaussian` that give a good fit to our data?  Previously, we tried a very\n",
        "basic approach, randomly guessing models until we found one that gave a good fit and high log_likelihood. We discussed\n",
        "that this was not a viable strategy for more complex models. Surprisingly, this is the basis of how model fitting\n",
        "actually works!\n",
        "\n",
        "Basically, our model-fitting algorithm guesses lots of models, tracking the log likelihood of these models. As the\n",
        "algorithm progresses, it begins to guess more models using parameter combinations that gave higher log_likelihood\n",
        "solutions previously. If a set of parameters provided a good fit to the data previously, a model with similar values\n",
        "probably will too.\n",
        "\n",
        "This is called a non-linear search, and is where the notion of a \"parameter space\" comes in. We are essentially\n",
        "searching the parameter space defined by the log likelihood function we introduced in the previous tutorial. Why is it\n",
        "called non-linear? Because this function is non-linear.\n",
        "\n",
        "Non linear searches are a common tool used by scientists in a wide range of fields. We will use a non-linear search\n",
        "algorithm called `Emcee`, which for those familiar with statistic inference is a Markov Chain Monte Carlo (MCMC)\n",
        "method. For now, lets not worry about the details of how Emcee actually works. Instead, just picture that a non-linear\n",
        "search in **PyAutoFit** operates as follows:\n",
        "\n",
        " 1) Randomly guess a model and map the parameters via the priors to an instance of the model, in this case\n",
        " our `Gaussian`.\n",
        "\n",
        " 2) Use this model instance to generate model data and compare this model data to the data to compute a log likelihood.\n",
        "\n",
        " 3) Repeat this many times, choosing models whose parameter values are near those of models which have higher log\n",
        " likelihood values. If a new model's log likelihood is higher than previous models, new models will be chosen with\n",
        " parameters nearer this model.\n",
        "\n",
        "The idea is that if we keep guessing models with higher log-likelihood values, we will inevitably `climb` up the\n",
        "gradient of the log likelihood in parameter space until we eventually hit the highest log likelihood models.\n",
        "\n",
        "To be clear, this overly simplified description of an MCMC algorithm is not how `Emcee` actually works in detail. We\n",
        "are omitting crucial details on how our priors impact our inference as well as how the MCMC algorithm provides us with\n",
        "reliable errors on our parameter estimates. The goal of this chapter to teach you how to use **PyAutoFit**, not the\n",
        "actual details of Bayesian inference. If you are interested in the details of how MCMC works, I recommend you checkout\n",
        "the following web links:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\n",
        "\n",
        "https://twiecki.io/blog/2015/11/10/mcmc-sampling/\n",
        "\n",
        "https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import os\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Load the dataset from the `autofit_workspace/dataset` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets remind ourselves what the data looks like, using defining a `plot_profile_1d` method for convenience.\n",
        "\n",
        "Note that this function has tools for outputting the images to hard-disk as `.png` files, which we'll use later\n",
        "in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def plot_profile_1d(\n",
        "    xvalues: np.ndarray,\n",
        "    profile_1d: np.ndarray,\n",
        "    title: Optional[str] = None,\n",
        "    ylabel: Optional[str] = None,\n",
        "    errors: Optional[np.ndarray] = None,\n",
        "    color: Optional[str] = \"k\",\n",
        "    output_path: Optional[str] = None,\n",
        "    output_filename: Optional[str] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot a 1D image of data on a plot of x versus y, where the x-axis is the x coordinate of the 1D profile\n",
        "    and the y-axis is the value of the 1D profile at that coordinate.\n",
        "\n",
        "    The function include options to output the image to the hard-disk as a .png.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    xvalues\n",
        "        The x-coordinates the profile is defined on.\n",
        "    profile_1d\n",
        "        The normalization values of the profile which are plotted.\n",
        "    ylabel\n",
        "        The y-label of the plot.\n",
        "    errors\n",
        "        The errors on each data point, which are related to its noise-map.\n",
        "    output_path\n",
        "        The path the image is to be output to hard-disk as a .png.\n",
        "    output_filename\n",
        "        The filename of the file if it is output as a .png.\n",
        "    output_format\n",
        "        Determines where the plot is displayed on your screen (\"show\") or output to the hard-disk as a png (\"png\").\n",
        "    \"\"\"\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=profile_1d,\n",
        "        yerr=errors,\n",
        "        color=color,\n",
        "        ecolor=\"k\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x value of profile\")\n",
        "    plt.ylabel(ylabel)\n",
        "    if output_filename is None:\n",
        "        plt.show()\n",
        "    else:\n",
        "        if not path.exists(output_path):\n",
        "            os.makedirs(output_path)\n",
        "        plt.savefig(path.join(output_path, f\"{output_filename}.png\"))\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "xvalues = np.arange(data.shape[0])\n",
        "\n",
        "plot_profile_1d(\n",
        "    xvalues=xvalues, profile_1d=data, errors=noise_map, title=\"Data\", ylabel=\"Data\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Lets import the `Gaussian` class for this tutorial, which is the model we will fit using the non-linear search.\n",
        "\n",
        "As discussed in the previous tutorial, by import this from a module rather than defining it in the tutorial script\n",
        "itself its priors are automatically loaded from configuration files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gaussian as g"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "The non-linear search requires an `Analysis` class, which:\n",
        "\n",
        " - Receives the data to be fitted and prepares it so the model can fit it.\n",
        " \n",
        " - Defines the `log_likelihood_function` used to compute the `log_likelihood` from a model instance. \n",
        " \n",
        " - Passes this `log_likelihood` to the non-linear search so that it can determine parameter values for the next \n",
        " model that it samples.\n",
        "\n",
        "For our 1D `Gaussian` model-fitting example, here is our `Analysis` class (read the comment in \n",
        "the `log_likelihood_function` for a description of how model mapping is used to set up the model that each iteration\n",
        "of the non-linear search fits):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        The `instance` that comes into this method is an instance of the `Gaussian` class above, with the parameters\n",
        "        set to values chosen by the non-linear search (These are commented out to prevent excessive print statements\n",
        "        when we run the non-linear search).\n",
        "\n",
        "        This instance`s parameter values are chosen by the non-linear search based on the priors of each parameter and\n",
        "        the previous models with the highest likelihood result. They are set up as physical values, by mapping unit\n",
        "        values chosen by the non-linear search.\n",
        "\n",
        "        print(\"Gaussian Instance:\")\n",
        "        print(\"Centre = \", instance.centre)\n",
        "        print(\"Normalization = \", instance.normalization)\n",
        "        print(\"Sigma = \", instance.sigma)\n",
        "\n",
        "        Below, we fit the data with the `Gaussian` instance, using its \"model_data_1d_via_xvalues_from\" function to create the\n",
        "        model data.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Visualization__\n",
        "\n",
        "The `Analysis` class above is all we need to fit our model to data with a non-linear search. However, it will provide\n",
        "us with limited output to inspect whether the fit was a success or not\n",
        "\n",
        "By extending the `Analysis` class with a `visualize` function, we can perform on-the-fly visualization, which outputs\n",
        "images of the quantities we described in tutorial 2 to hard-disk as `.png` files using the `plot_profile_1d` function above. \n",
        "\n",
        "Visualization of the results of the search, such as the corner plot of what is called the \"Probability Density \n",
        "Function\", are also automatically output during the model-fit on the fly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        The `log_likelihood_function` is identical to the previous tutorial.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def visualize(self, paths, instance, during_analysis):\n",
        "        \"\"\"\n",
        "        During a model-fit, the `visualize` method is called throughout the non-linear search. The `instance` passed\n",
        "        into the visualize method is maximum log likelihood solution obtained by the model-fit so far and it can be\n",
        "        used to provide on-the-fly images showing how the model-fit is going.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "\n",
        "        \"\"\"\n",
        "        The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\n",
        "        \"\"\"\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=self.data,\n",
        "            title=\"Data\",\n",
        "            ylabel=\"Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"data\",\n",
        "        )\n",
        "\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=model_data,\n",
        "            title=\"Model Data\",\n",
        "            ylabel=\"Model Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"model_data\",\n",
        "        )\n",
        "\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=residual_map,\n",
        "            title=\"Residual Map\",\n",
        "            ylabel=\"Residuals\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"residual_map\",\n",
        "        )\n",
        "\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=chi_squared_map,\n",
        "            title=\"Chi-Squared Map\",\n",
        "            ylabel=\"Chi-Squareds\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"chi_squared_map\",\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "To perform the non-linear search using `Emcee`, we simply compose our model using a `Model`, instantiate the \n",
        "`Analysis` class and pass them to an instance of the `Emcee` class. \n",
        "\n",
        "We also pass a `name` and `path_prefrix`, which specifies that when the results are output to the folder \n",
        "`autofit_workspace/output` they'll also be written to the folder `howtofit/chapter_1/tutorial_4_non_linear_search`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(g.Gaussian)\n",
        "\n",
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "We begin the non-linear search by calling its `fit` method. This will take a minute or so to run (which is very fast \n",
        "for a model-fit). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"\"\"\n",
        "    Emcee has begun running.\n",
        "    This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "Once completed, the non-linear search returns a `Result` object, which contains lots of information about the \n",
        "NonLinearSearch.\n",
        " \n",
        "A full description of the `Results` object will be given in tutorial 6 and can also be found at:\n",
        " \n",
        "`autofit_workspace/overview/simple/results`\n",
        "`autofit_workspace/overview/complex/results`.\n",
        "\n",
        "The `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets use the `result` it to inspect the maximum likelihood model instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Likelihood Model:\\n\")\n",
        "max_log_likelihood_instance = result.samples.max_log_likelihood()\n",
        "print(\"Centre = \", max_log_likelihood_instance.centre)\n",
        "print(\"Normalization = \", max_log_likelihood_instance.normalization)\n",
        "print(\"Sigma = \", max_log_likelihood_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this to plot the maximum log likelihood fit over the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = result.max_log_likelihood_instance.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"Emcee model fit to 1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples__\n",
        "\n",
        "Above, we used the `Result`'s `samples` property, which in this case is a `SamplesMCMC` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This object acts as an interface between the `Emcee` output results on your hard-disk and this Python code. For\n",
        "example, we can use it to get the parameters and log likelihood of an accepted emcee sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples.parameter_lists[10][:])\n",
        "print(result.samples.log_likelihood_list[10])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also use it to get a model instance of the `median_pdf` model, which is the model where each parameter is\n",
        "the value estimated from the probability distribution of parameter space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mp_instance = result.samples.median_pdf()\n",
        "print()\n",
        "print(\"Median PDF Model:\\n\")\n",
        "print(\"Centre = \", mp_instance.centre)\n",
        "print(\"Normalization = \", mp_instance.normalization)\n",
        "print(\"Sigma = \", mp_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Probability Density Functions (PDF's) of the results can be plotted using the Emcee's visualization \n",
        "tool `corner.py`, which is wrapped via the `EmceePlotter` object.\n",
        "\n",
        "The PDF shows the 1D and 2D probabilities estimated for every parameter after the model-fit. The two dimensional \n",
        "figures can show the degeneracies between different parameters, for example how increasing $\\sigma$ and decreasing \n",
        "the normalization $I$ can lead to similar likelihoods and probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_plotter = aplt.EmceePlotter(samples=result.samples)\n",
        "search_plotter.corner()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The PDF figure above can be seen to have labels for all parameters, whereby sigma appears as a sigma symbol, the\n",
        "normalization is `N`, and centre is `x`. This is set via the config file `config/notation/label.yaml`. When you write\n",
        "your own model-fitting code with **PyAutoFit**, you can update this config file so your PDF's automatically have the\n",
        "correct labels.\n",
        "\n",
        "we'll come back to the `Samples` objects in tutorial 6!\n",
        "\n",
        "__Output to Hard-Disk__\n",
        "\n",
        "The non-linear search `dynesty` above did not output results to hard-disk, which for quick model-fits and\n",
        "experimenting with different models is desirable.\n",
        "\n",
        "For many problems it is preferable for all results to be written to hard-disk. The benefits of doing this include:\n",
        "\n",
        "- Inspecting results in an ordered directory structure can be more efficient than using a Jupyter Notebook.\n",
        "- Results can be output on-the-fly, to check that a fit is progressing as expected mid way through.\n",
        "- An unfinished run can be resumed where it was terminated.\n",
        "- Additional information about a fit (e.g. visualization) can be output.\n",
        "- On high performance computers which use a batch system, this is the only way to transfer results.\n",
        "\n",
        "Any model-fit performed by **PyAutoFit** can be saved to hard-disk, by simply giving the non-linear search a \n",
        "`name`. A `path_prefix` can optionally be input to customize the output directory.\n",
        "\n",
        "__Unique Identifier__\n",
        "\n",
        "In the output folder, you will note that results are in a folder which is a collection of random characters. This acts \n",
        "as a `unique_identifier` of the model-fit, where this identifier is generated based on the model, priors and search that \n",
        "are used in the fit.\n",
        " \n",
        "An identical combination of model and search generates the same identifier, meaning that rerunning the\n",
        "script will use the existing results to resume the model-fit. In contrast, if you change the model, priors or search,\n",
        "a new unique identifier will be generated, ensuring that the model-fit results are output into a separate folder. \n",
        "\n",
        "__Contents__\n",
        "\n",
        "In particular, you'll find (in a folder that is a random string of characters):\n",
        "\n",
        " - `model.info`: A file listing every model component, parameter and prior in your model-fit.\n",
        "\n",
        " - `model.results`: A file giving the latest best-fit model, parameter estimates and errors of the fit.\n",
        " \n",
        " - `output.log`: A file containing the text output of the non-linear search.\n",
        " \n",
        " - `samples`: A folder containing the `Emcee` output in hdf5 format.txt (you'll probably never need to look at these, \n",
        "   but its good to know what they are).\n",
        " \n",
        " - `search.summary` A file containing information on the search, including the total number of samples,\n",
        " overall run-time and time it takes to evaluate the log likelihood function.\n",
        " \n",
        " - `image`: A folder containing `.png` files of the fits defined in the `visualize` method.\n",
        " \n",
        " - Other metadata which you can ignore for now (e.g. the pickles folder)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Emcee(\n",
        "    path_prefix=path.join(\"howtofit\", \"chapter_1\"), name=\"tutorial_4_non_linear_search\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"\"\"\n",
        "    Emcee has begun running - checkout the autofit_workspace/output/howtofit/tutorial_4_non_linear_search\n",
        "    folder for live output of the results.\n",
        "    This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}