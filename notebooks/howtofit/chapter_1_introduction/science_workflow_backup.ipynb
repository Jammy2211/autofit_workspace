{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Mapping via Priors__\n",
        "\n",
        "We can again use **PyAutoFit** to set the `Gaussian` as a model and map it to instances of the `Gaussian`, however\n",
        "we can now do this via priors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)\n",
        "print(\"Model `Gaussian` object: \\n\")\n",
        "print(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now set the prior for each parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.normalization = af.LogUniformPrior(lower_limit=0.1, upper_limit=10.0)\n",
        "model.sigma = af.GaussianPrior(mean=10.0, sigma=5.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The updated priors are reflected in the model's `info` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 4: Non-linear Search\n",
        "============================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import os\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Load the dataset from the `autofit_workspace/dataset` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets remind ourselves what the data looks like, using defining a `plot_profile_1d` method for convenience.\n",
        "\n",
        "Note that this function has tools for outputting the images to hard-disk as `.png` files, which we'll use later\n",
        "in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def plot_profile_1d(\n",
        "    xvalues: np.ndarray,\n",
        "    profile_1d: np.ndarray,\n",
        "    title: Optional[str] = None,\n",
        "    ylabel: Optional[str] = None,\n",
        "    errors: Optional[np.ndarray] = None,\n",
        "    color: Optional[str] = \"k\",\n",
        "    output_path: Optional[str] = None,\n",
        "    output_filename: Optional[str] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot a 1D image of data on a plot of x versus y, where the x-axis is the x coordinate of the 1D profile\n",
        "    and the y-axis is the value of the 1D profile at that coordinate.\n",
        "\n",
        "    The function include options to output the image to the hard-disk as a .png.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    xvalues\n",
        "        The x-coordinates the profile is defined on.\n",
        "    profile_1d\n",
        "        The normalization values of the profile which are plotted.\n",
        "    ylabel\n",
        "        The y-label of the plot.\n",
        "    errors\n",
        "        The errors on each data point, which are related to its noise-map.\n",
        "    output_path\n",
        "        The path the image is to be output to hard-disk as a .png.\n",
        "    output_filename\n",
        "        The filename of the file if it is output as a .png.\n",
        "    output_format\n",
        "        Determines where the plot is displayed on your screen (\"show\") or output to the hard-disk as a png (\"png\").\n",
        "    \"\"\"\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=profile_1d,\n",
        "        yerr=errors,\n",
        "        color=color,\n",
        "        ecolor=\"k\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x value of profile\")\n",
        "    plt.ylabel(ylabel)\n",
        "    if output_filename is None:\n",
        "        plt.show()\n",
        "    else:\n",
        "        if not path.exists(output_path):\n",
        "            os.makedirs(output_path)\n",
        "        plt.savefig(path.join(output_path, f\"{output_filename}.png\"))\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "xvalues = np.arange(data.shape[0])\n",
        "\n",
        "plot_profile_1d(\n",
        "    xvalues=xvalues, profile_1d=data, errors=noise_map, title=\"Data\", ylabel=\"Data\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Lets import the `Gaussian` class for this tutorial, which is the model we will fit using the non-linear search.\n",
        "\n",
        "As discussed in the previous tutorial, by import this from a module rather than defining it in the tutorial script\n",
        "itself its priors are automatically loaded from configuration files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gaussian as g"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "The non-linear search requires an `Analysis` class, which:\n",
        "\n",
        " - Receives the data to be fitted and prepares it so the model can fit it.\n",
        "\n",
        " - Defines the `log_likelihood_function` used to compute the `log_likelihood` from a model instance. \n",
        "\n",
        " - Passes this `log_likelihood` to the non-linear search so that it can determine parameter values for the next \n",
        " model that it samples.\n",
        "\n",
        "For our 1D `Gaussian` model-fitting example, here is our `Analysis` class (read the comment in \n",
        "the `log_likelihood_function` for a description of how model mapping is used to set up the model that each iteration\n",
        "of the non-linear search fits):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        Returns the log likelihood of the fit of an `instance` containing one 1D Profile (e.g. a Gaussian) to the\n",
        "        dataset, using a model instance.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance\n",
        "            A list of 1D profiles with parameters set via the non-linear search.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The log likelihood value indicating how well this model fit the `MaskedDataset`.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"        \n",
        "        The `instance` that comes into this method is an instance of the `Gaussian` class above, with the parameters\n",
        "        set to values chosen by the non-linear search (These are commented out to prevent excessive print statements\n",
        "        when we run the non-linear search).\n",
        "\n",
        "        This instance`s parameter values are chosen by the non-linear search based on the priors of each parameter and\n",
        "        the previous models with the highest likelihood result. They are set up as physical values, by mapping unit\n",
        "        values chosen by the non-linear search.\n",
        "\n",
        "        print(\"Gaussian Instance:\")\n",
        "        print(\"Centre = \", instance.centre)\n",
        "        print(\"Normalization = \", instance.normalization)\n",
        "        print(\"Sigma = \", instance.sigma)\n",
        "\n",
        "        Below, we fit the data with the `Gaussian` instance, using its \"model_data_1d_via_xvalues_from\" function to create the\n",
        "        model data.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Visualization__\n",
        "\n",
        "The `Analysis` class above is all we need to fit our model to data with a non-linear search. However, it will provide\n",
        "us with limited output to inspect whether the fit was a success or not\n",
        "\n",
        "By extending the `Analysis` class with a `visualize` function, we can perform on-the-fly visualization, which outputs\n",
        "images of the quantities we described in tutorial 2 to hard-disk as `.png` files using the `plot_profile_1d` function above. \n",
        "\n",
        "Visualization of the results of the search, such as the corner plot of what is called the \"Probability Density \n",
        "Function\", are also automatically output during the model-fit on the fly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        The `log_likelihood_function` is identical to the previous tutorial.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def visualize(self, paths, instance, during_analysis):\n",
        "        \"\"\"\n",
        "        During a model-fit, the `visualize` method is called throughout the non-linear search. The `instance` passed\n",
        "        into the visualize method is maximum log likelihood solution obtained by the model-fit so far and it can be\n",
        "        used to provide on-the-fly images showing how the model-fit is going.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "\n",
        "        \"\"\"\n",
        "        The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\n",
        "        \"\"\"\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=self.data,\n",
        "            title=\"Data\",\n",
        "            ylabel=\"Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"data\",\n",
        "        )\n",
        "\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=model_data,\n",
        "            title=\"Model Data\",\n",
        "            ylabel=\"Model Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"model_data\",\n",
        "        )\n",
        "\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=residual_map,\n",
        "            title=\"Residual Map\",\n",
        "            ylabel=\"Residuals\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"residual_map\",\n",
        "        )\n",
        "\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=chi_squared_map,\n",
        "            title=\"Chi-Squared Map\",\n",
        "            ylabel=\"Chi-Squareds\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"chi_squared_map\",\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "To perform the non-linear search using `Emcee`, we simply compose our model using a `Model`, instantiate the \n",
        "`Analysis` class and pass them to an instance of the `Emcee` class. \n",
        "\n",
        "We also pass a `name` and `path_prefrix`, which specifies that when the results are output to the folder \n",
        "`autofit_workspace/output` they'll also be written to the folder `howtofit/chapter_1/tutorial_4_non_linear_search`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(g.Gaussian)\n",
        "\n",
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "We begin the non-linear search by calling its `fit` method. This will take a minute or so to run (which is very fast \n",
        "for a model-fit). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"\"\"\n",
        "    Emcee has begun running.\n",
        "    This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "Once completed, the non-linear search returns a `Result` object, which contains lots of information about the \n",
        "NonLinearSearch.\n",
        "\n",
        "A full description of the `Results` object will be given in tutorial 6 and can also be found at:\n",
        "\n",
        "`autofit_workspace/overview/simple/results`\n",
        "`autofit_workspace/overview/complex/results`.\n",
        "\n",
        "The `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets use the `result` it to inspect the maximum likelihood model instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Likelihood Model:\\n\")\n",
        "max_log_likelihood_instance = result.samples.max_log_likelihood()\n",
        "print(\"Centre = \", max_log_likelihood_instance.centre)\n",
        "print(\"Normalization = \", max_log_likelihood_instance.normalization)\n",
        "print(\"Sigma = \", max_log_likelihood_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this to plot the maximum log likelihood fit over the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = result.max_log_likelihood_instance.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"Emcee model fit to 1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also use it to get a model instance of the `median_pdf` model, which is the model where each parameter is\n",
        "the value estimated from the probability distribution of parameter space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mp_instance = result.samples.median_pdf()\n",
        "print()\n",
        "print(\"Median PDF Model:\\n\")\n",
        "print(\"Centre = \", mp_instance.centre)\n",
        "print(\"Normalization = \", mp_instance.normalization)\n",
        "print(\"Sigma = \", mp_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The PDF figure above can be seen to have labels for all parameters, whereby sigma appears as a sigma symbol, the\n",
        "normalization is `N`, and centre is `x`. This is set via the config file `config/notation/label.yaml`. When you write\n",
        "your own model-fitting code with **PyAutoFit**, you can update this config file so your PDF's automatically have the\n",
        "correct labels.\n",
        "\n",
        "we'll come back to the `Samples` objects in tutorial 6!\n",
        "\n",
        "__Output to Hard-Disk__\n",
        "\n",
        "The non-linear search `dynesty` above did not output results to hard-disk, which for quick model-fits and\n",
        "experimenting with different models is desirable.\n",
        "\n",
        "For many problems it is preferable for all results to be written to hard-disk. The benefits of doing this include:\n",
        "\n",
        "- Inspecting results in an ordered directory structure can be more efficient than using a Jupyter Notebook.\n",
        "- Results can be output on-the-fly, to check that a fit is progressing as expected mid way through.\n",
        "- An unfinished run can be resumed where it was terminated.\n",
        "- Additional information about a fit (e.g. visualization) can be output.\n",
        "- On high performance computers which use a batch system, this is the only way to transfer results.\n",
        "\n",
        "Any model-fit performed by **PyAutoFit** can be saved to hard-disk, by simply giving the non-linear search a \n",
        "`name`. A `path_prefix` can optionally be input to customize the output directory.\n",
        "\n",
        "__Unique Identifier__\n",
        "\n",
        "In the output folder, you will note that results are in a folder which is a collection of random characters. This acts \n",
        "as a `unique_identifier` of the model-fit, where this identifier is generated based on the model, priors and search that \n",
        "are used in the fit.\n",
        "\n",
        "An identical combination of model and search generates the same identifier, meaning that rerunning the\n",
        "script will use the existing results to resume the model-fit. In contrast, if you change the model, priors or search,\n",
        "a new unique identifier will be generated, ensuring that the model-fit results are output into a separate folder. \n",
        "\n",
        "__Contents__\n",
        "\n",
        "In particular, you'll find (in a folder that is a random string of characters):\n",
        "\n",
        " - `model.info`: A file listing every model component, parameter and prior in your model-fit.\n",
        "\n",
        " - `model.results`: A file giving the latest best-fit model, parameter estimates and errors of the fit.\n",
        "\n",
        " - `output.log`: A file containing the text output of the non-linear search.\n",
        "\n",
        " - `samples`: A folder containing the `Emcee` output in hdf5 format.txt (you'll probably never need to look at these, \n",
        "   but its good to know what they are).\n",
        "\n",
        " - `search.summary` A file containing information on the search, including the total number of samples,\n",
        " overall run-time and time it takes to evaluate the log likelihood function.\n",
        "\n",
        " - `image`: A folder containing `.png` files of the fits defined in the `visualize` method.\n",
        "\n",
        " - Other metadata which you can ignore for now (e.g. the pickles folder)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Emcee(\n",
        "    path_prefix=path.join(\"howtofit\", \"chapter_1\"), name=\"tutorial_4_non_linear_search\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"\"\"\n",
        "    Emcee has begun running - checkout the autofit_workspace/output/howtofit/tutorial_4_non_linear_search\n",
        "    folder for live output of the results.\n",
        "    This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Limits__\n",
        "\n",
        "We can also set physical limits on parameters, such that a model instance cannot generate parameters outside of a\n",
        "specified range.\n",
        "\n",
        "For example, a `Gaussian` cannot have a negative normalization, so we can set its lower limit to a value of 0.0.\n",
        "\n",
        "This is what the `gaussian_limits` section in the priors config files sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.normalization = af.GaussianPrior(\n",
        "    mean=0.0, sigma=1.0, lower_limit=0.0, upper_limit=1000.0\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Prior Configs__\n",
        "\n",
        "For highly complex models with many parameters, it is cumbersome to have to manually define the prior on every \n",
        "parameter and would likely lead to input mistakes.\n",
        "\n",
        "**PyAutoFit** allows one to define all of the default prior values in a configuration file, such that they are loaded\n",
        "automatically. This means we do not need manually define the priors ourselves.\n",
        "\n",
        "To do this, we define the `Gaussian` class in a standalone Python \n",
        "module `autofit_workspace/*/howtofit/chapter_1_introduction/gaussian.py` (as opposed to this Python script). \n",
        "The name of this module is used to look for a file `gaussian.json` in the directory `autofit_workspace/config/priors` \n",
        "such that the default priors of the model are loaded from the file `gaussian.json`. \n",
        "\n",
        "For example, because our `Gaussian` is in the module `gaussian.py`, its priors are loaded from the priors config\n",
        "file `gaussian.json`. Check this file out now to see the default priors; we'll discuss what the different inputs\n",
        "mean later on.\n",
        "\n",
        "This is illustrated below, where we are using the `Gaussian` defined in `gaussian.py` and inspect its prior to see\n",
        "they have been automatically set up via the config file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gaussian as g\n",
        "\n",
        "model = af.Model(g.Gaussian)\n",
        "print(\"Model `Gaussian` object via priors configs: \\n\")\n",
        "print(model)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}