{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 1: Models\n",
        "==============\n",
        "\n",
        "At the heart of model-fitting is the model: a set of equations, numerical processes and assumptions describing a\n",
        "physical system of interest. The goal of model-fitting is to understand this physical system more, ultimately\n",
        "develop more complex models which describe more aspects of the system more accurately.\n",
        "\n",
        "In Astronomy, a model may describe the distribution of stars within a galaxy. In biology, is may describe the\n",
        "interaction of proteins within a cell. In finance, it may describe the evolution of stock prices in a market.\n",
        "Your model depends on your topic of study, but in all cases the model acts as the mathematical description of\n",
        "some physical system you seek understand better, and hope ultimately to make new predictions of.\n",
        "\n",
        "Whatever your model, the equations that underpin will be defined by \"free parameters\". Changing these parameters\n",
        "changes the prediction of the model.\n",
        "\n",
        "For example, an Astronomy model of the distribution of stars may contain a\n",
        "parameter describing the brightness of the stars, a second parameter defining their number density and a third\n",
        "parameter describing their colors. If we multiplied the parameter describribing the brightness of the stars by 2,\n",
        "the stars would therefore appear twice as bright.\n",
        "\n",
        "Once the model (e.g. the undrlying equations) is defined and a values for the free parameters have been chosen, the\n",
        "model can create \"model data\". This data is a realization of how the physical system appears for that model with\n",
        "those parameters.\n",
        "\n",
        "For example, a model of the distribution of stars within a galaxy can be used to create a model image of that galaxy.\n",
        "By changing the parameters governing the distribution of stars, it can produce many different model images, with\n",
        "different brightness, colors, sizes, etc.\n",
        "\n",
        "In this tutorial, we will learn the basics of defining a model, and we will in particular:\n",
        "\n",
        " - Define a simple model, described by few single equations.\n",
        "\n",
        " - Show that this model is described by 3 or more free parameters.\n",
        "\n",
        " - Use the model, with different sets of parameters, to generate model data.\n",
        "\n",
        "This will all be performed using the **PyAutoFit** API for model composition, which forms the basis of all model\n",
        "fitting performed by **PyAutoFit**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import autofit as af\n",
        "import autofit.plot as aplt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Paths__\n",
        "\n",
        "**PyAutoFit** assumes the current working directory is `/path/to/autofit_workspace/` on your hard-disk (or in Binder). \n",
        "\n",
        "This is so that it can:\n",
        "\n",
        " - Load configuration settings from config files in the `autofit_workspace/config` folder.\n",
        "\n",
        " - Load example data from the `autofit_workspace/dataset` folder.\n",
        "\n",
        " - Output the results of models fits to your hard-disk to the `autofit/output` folder. \n",
        "\n",
        "If you don't have an `autofit_workspace` (perhaps you cloned / forked the **PyAutoFit** GitHub repository?) you can\n",
        "download it here:\n",
        "\n",
        " https://github.com/Jammy2211/autofit_workspace\n",
        "\n",
        "At the top of every tutorial notebook, you will see the following cell. \n",
        "\n",
        "This cell uses the project `pyprojroot` to locate the path to the workspace on your computer and use it to set the \n",
        "working directory of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Parameterization__\n",
        "\n",
        "A model is a set of equations, numerical processes and assumptions that describe a physical system and dataset.\n",
        "\n",
        "We can pretty much consider anything is a model. In this example, our model will simply be one or more 1 dimensional\n",
        "Gaussian, defined by the following equaiton:\n",
        "\n",
        "\\begin{equation*}\n",
        "g(x, I, \\sigma) = \\frac{N}{\\sigma\\sqrt{2\\pi}} \\exp{(-0.5 (x / \\sigma)^2)}\n",
        "\\end{equation*}\n",
        "\n",
        "Where:\n",
        "\n",
        "`x`: Is the x-axis coordinate where the `Gaussian` is evaluated.\n",
        "\n",
        "`N`: Describes the overall normalization of the Gaussian.\n",
        "\n",
        "$\\sigma$: Describes the size of the Gaussian (Full Width Half Maximum = $\\mathrm {FWHM}$ = $2{\\sqrt {2\\ln 2}}\\;\\sigma$)\n",
        "\n",
        "Whilst a 1D Gaussian may seem like a somewhat rudimentary model, it actually has a lot of real-world applicaiton\n",
        "in signal process, where 1D Gausians are fitted to 1D datasets in order to quantify the size of a signal. Our\n",
        "model is therefore a realstic representation of a real world modeling problrem!\n",
        "\n",
        "We therefore now have a model, which as expected is a set of equations (just one in this case) that describes a \n",
        "dataset.\n",
        "\n",
        "The model has 3 parameters, $(x, N, \\sigma)$, where using different combinations of these parameters creates different \n",
        "realizations of the model.\n",
        "\n",
        "So, how do we compose this model is **PyAutoFit**?\n",
        "\n",
        "__Model Composition__\n",
        "\n",
        "To define a \"model component\" in **PyAutoFit** we simply write it as a Python class using the format shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "            self,\n",
        "            centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "            normalization=1.0,  # <- are the Gaussian`s model parameters.\n",
        "            sigma=5.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Gaussian profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to fit example datasets\n",
        "        via a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues: np.ndarray):\n",
        "        \"\"\"\n",
        "\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The format of this Python class defines how **PyAutoFit** will compose the `Gaussian` as a model, where:\n",
        "\n",
        "- The name of the class is the name of the model component, in this case, \"Gaussian\".\n",
        "\n",
        "- The input arguments of the constructor (the `__init__` method) are the parameters of the model, in the\n",
        "  example above `centre`, `normalization` and `sigma`.\n",
        "\n",
        "- The default values of the input arguments define whether a parameter is a single-valued `float` or a \n",
        "  multi-valued `tuple`. For the `Gaussian` class above, no input parameters are a tuple, but later examples use tuples. \n",
        "\n",
        "- It includes functions associated with that model component, specifically the `model_data_1d_via_xvalues_from` \n",
        "  function. When we create instances of a `Gaussian` below, this is used to generate 1D representation of it as a \n",
        "  NumPy array.\n",
        "\n",
        "To compose a model using the `Gaussian` class above we use the `af.Model` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)\n",
        "print(\"Model `Gaussian` object: \\n\")\n",
        "print(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model has a total of 3 parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.total_free_parameters)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All model information is given by printing its `info` attribute.\n",
        "\n",
        "This includes model parameters have priors, which are described fully in tutorial 3 of this chapter.\n",
        "\n",
        "[The `info` below may not display optimally on your computer screen, for example the whitespace between parameter\n",
        "names on the left and parameter priors on the right may lead them to appear across multiple lines. This is a\n",
        "common issue in Jupyter notebooks.\n",
        "\n",
        "The`info_whitespace_length` parameter in the file `config/general.yaml` in the [output] section can be changed to \n",
        "increase or decrease the amount of whitespace (The Jupyter notebook kernel will need to be reset for this change to \n",
        "appear in a notebook).]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Mapping__\n",
        "\n",
        "Instances of model components created via the `af.Model` object can be created, where an input `vector` of\n",
        "parameters is mapped to the Python class the model object was created using.\n",
        "\n",
        "We first need to know the order of parameters in the model, so we know how to define the input `vector`. This\n",
        "information is contained in the models `paths` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.paths)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We input values for the 3 free parameters of our model following the order of paths \n",
        "above (`centre=30.0`, `normalization=2.0` and `sigma=3.0`), creating an `instance` of the `Gaussian` via the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = model.instance_from_vector(vector=[30.0, 2.0, 3.0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is an instance of the `Gaussian` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Model Instance: \\n\")\n",
        "print(instance)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It has the parameters of the `Gaussian` with the values input above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Instance Parameters \\n\")\n",
        "print(\"x = \", instance.centre)\n",
        "print(\"normalization = \", instance.normalization)\n",
        "print(\"sigma = \", instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use functions associated with the class, specifically the `model_data_1d_via_xvalues_from` function, to \n",
        "create a realization of the `Gaussian` and plot it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = np.arange(0.0, 100.0, 1.0)\n",
        "\n",
        "model_data = instance.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"1D Gaussian Model Data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Gaussian Value\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__More Complex Models__\n",
        "\n",
        "The code above seemed like a lot of work just to create an instance of the `Guassian` class. Couldn't we have\n",
        "just done the following instead?\n",
        "\n",
        " `instance = Gaussian(centre=30.0, normalization=2.0, sigma=3.0)`.\n",
        "\n",
        "Yes, we could have. \n",
        "\n",
        "However, the model composition API used above is designed to make composing complex models, made of multiple \n",
        "components with many free parameters, straightforward and scalable.\n",
        "\n",
        "To illustrate this, lets end the tutorial by composing a model made of multiple Gaussians and also another 1D\n",
        "profile, an Exponential, which is defined following the equation:\n",
        "\n",
        "\\begin{equation*}\n",
        "g(x, I, \\lambda) = N \\lambda \\exp{- \\lambda x }\n",
        "\\end{equation*}\n",
        "\n",
        "Where:\n",
        "\n",
        "`x`: Is the x-axis coordinate where the `Exponential` is evaluated.\n",
        "\n",
        "`N`: Describes the overall normalization of the `Exponential`\n",
        "\n",
        "$\\lambda$: Describes the rate of decay of the exponential.\n",
        "\n",
        "We first define the `Exponential` using the same format as above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Exponential:\n",
        "    def __init__(\n",
        "            self,\n",
        "            centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "            normalization=1.0,  # <- are the Gaussian`s model parameters.\n",
        "            rate=0.01,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Exponential profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to fit example datasets\n",
        "        via a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        ratw\n",
        "            The decay rate controlling has fast the Exponential declines.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.rate = rate\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues: np.ndarray):\n",
        "        \"\"\"\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return self.normalization * np.multiply(\n",
        "            self.rate, np.exp(-1.0 * self.rate * abs(transformed_xvalues))\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can easily compose a model consisting of 1 `Gaussian` object and 1 `Exponential` object using the `af.Collection`\n",
        "object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.Model(Gaussian), exponential=af.Model(Exponential))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All of the information about the model created via the collection can be printed at once using its `info` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the `Gaussian` and `Exponential` are being passed to a `Collection` they are automatically \n",
        "assigned as `Model` objects.\n",
        "\n",
        "We can therefore omit the `af.Model` method when passing classes to a `Collection`, making the Python code more\n",
        "concise and readable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=Gaussian, exponential=Exponential)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `model.info` appears identical to the previous example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A `Collection` behaves analogous to a `Model`, but it contains a multiple model components.\n",
        "\n",
        "We can see this by printing its `paths` attribute, where paths to all 6 free parameters via both model components\n",
        "are shown.\n",
        "\n",
        "The reason the paths have the entries `.gaussian.` and `.expoential.` is becuase these are the names we input into \n",
        "the `af.Collection` object above. If you change the input from `gaussian=` to `gaussian_edited=` this will be reflected \n",
        "in the `paths` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.paths)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A model instance can again be created by mapping an input `vector`, which now has 6 entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = model.instance_from_vector(vector=[0.1, 0.2, 0.3, 0.4, 0.5, 0.01])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This `instance` contains each of the model components we defined above, using the input argument name of the\n",
        "`Collection` to define the attributes in the `instance`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Instance Parameters \\n\")\n",
        "print(\"x (Gaussian) = \", instance.gaussian.centre)\n",
        "print(\"normalization (Gaussian) = \", instance.gaussian.normalization)\n",
        "print(\"sigma (Gaussian) = \", instance.gaussian.sigma)\n",
        "print(\"x (Exponential) = \", instance.exponential.centre)\n",
        "print(\"normalization (Exponential) = \", instance.exponential.normalization)\n",
        "print(\"sigma (Exponential) = \", instance.exponential.rate)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the context of the equations that define the model, the model is simply the sum of the two equations that define\n",
        "the `Gaussian` and `Exponential`.\n",
        "\n",
        "Generating the `model_data` therefore requires us to simply sum each individual model component`s `model_data`, which\n",
        "we do and visualize below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = np.arange(0.0, 100.0, 1.0)\n",
        "\n",
        "model_data_0 = instance.gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "model_data_1 = instance.exponential.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "\n",
        "model_data = model_data_0 + model_data_1\n",
        "\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.plot(xvalues, model_data_0, \"b\", \"--\")\n",
        "plt.plot(xvalues, model_data_1, \"k\", \"--\")\n",
        "plt.title(\"1D Gaussian + Exponential Model Data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Extensibility__\n",
        "\n",
        "It is hopefully now clear why we use `Model` and `Collection` objects to compose our model.\n",
        "\n",
        "They can easily be extended to compose complex models with many components and parameters. For example, we could\n",
        "input more `Gaussian` and `Exponential` components into the `Collection`, or we could write new Python classes\n",
        "that represent new model components with more parameters.\n",
        "\n",
        "These objects serve many other key purposes that we will cover in later tutorials, \n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "In this tutorial, we introduced how to define and compose a model, which we can generate model data from. \n",
        "\n",
        "To end, have a think about your particular field of study and the problem you are hoping to solve through \n",
        "model-fitting., What is the model you might want to fit? What Python class using the format above are requird to\n",
        "compose the right model? What are the free parameters of you model?\n",
        "\n",
        "If you decide to add a new model-component to the `autofit_workspace` specific to your model-fitting task, first\n",
        "checkout the following script, which explains how to set up the **PyAutoFit** configuration files associated with \n",
        "your model.\n",
        "\n",
        "`autofit_workspace/*/overview/new_model_component/new_model_component.ipynb`\n",
        "\n",
        "Below are two more example Python classes one might define to perform model fitting, the first is the model of a \n",
        "linear-regression line of the form $y = mx + c$ that you might fit to a 1D data-set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class LinearFit:\n",
        "    def __init__(self, gradient=1.0, intercept=0.0):\n",
        "        self.gradient = gradient\n",
        "        self.intercept = intercept\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The second example is a two-dimensional Gaussian. Here, the centre now has two coordinates (y,x), which in \n",
        "**PyAutoFit** is more suitably defined using a tuple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian2D:\n",
        "    def __init__(self, centre=(0.0, 0.0), normalization=0.1, sigma=1.0):\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 2: Fitting Data\n",
        "====================\n",
        "\n",
        "We have now learnt that a model is a set of equations, numerical processes and assumptions describing\n",
        "a physical system. We defined a couple of simple models made of 1D equations like a Gaussian, composed them as models\n",
        "in **PyAutoFit** using the `Model` and `Collection` objects, and used these models to create model data for different\n",
        "values of their parameters.\n",
        "\n",
        "For our model to inform us about a real physical system, we need to fit it to data. By fitting it to the data,\n",
        "we can determine whether the model provides a good or bad fit to the data. If it is a good fit, we will learn which\n",
        "parameter values best describe the data and therefore the physical system as a whole. If it is a bad fit, we will\n",
        "learn that our model is not representative of the physical system and therefore that we need to change it.\n",
        "\n",
        "The process of defining a model, fitting it to data and using it to learn about the system we are modeling is at the\n",
        "heart of model-fitting. One would typically repeat this process many times, making the model more complex to better\n",
        "fit more data, better describing the physical system we are interested in.\n",
        "\n",
        "In Astronomy, this is the process that was followed to learn about the distributions of stars in galaxies. Fitting\n",
        "high quality images of galaxies with ever more complex models, allowed astronomers to determine that the stars in\n",
        "galaxies are distributed in structures like disks, bars and bulges, and it taught them that stars appear differently\n",
        "in red and blue images due to their age.\n",
        "\n",
        "In this tutorial, we will learn how to fit the `model_data` created by a model to data, and we will in particular:\n",
        "\n",
        " - Load data of a 1D Gaussian signal which is the data we will fit.\n",
        "\n",
        " - Subtract the model data from the data to compute quantities like the residuals of the fit.\n",
        "\n",
        " - Quantify the goodness-of-fit of a model to the data quantitatively using a key quantity in model-fitting called the\n",
        "   `log_likelihood`.\n",
        "\n",
        "This will all be performed using the **PyAutoFit** API for model composition, which forms the basis of all model\n",
        "fitting performed by **PyAutoFit**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Our data is noisy 1D data containing a signal, where the underlying signal is generated using the equation of \n",
        "a 1D Gaussian, a 1D Exponential or a sum of multiple 1D profiles.\n",
        " \n",
        "We now load this data from .json files, where:\n",
        "\n",
        " - The `data` is a 1D numpy array of values corresponding to the observed signal.\n",
        " - The `noise_map` is a 1D numpy array of values corresponding to the estimate noise value in every data point.\n",
        " \n",
        "These datasets are created via the scripts `autofit_workspace/howtofit/simulators`, feel free to check them out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now plot the 1D signal via `matplotlib`.\n",
        "\n",
        "The 1D signal is observed on uniformly spaced `xvalues`, which are computed using the `arange` function \n",
        "and `data.shape[0]` method.\n",
        "\n",
        "These x values will be used again below, when we create model data from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = np.arange(data.shape[0])\n",
        "plt.plot(xvalues, data, color=\"k\")\n",
        "plt.title(\"1D Dataset Containing a Gaussian.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Signal Value\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot above only showed the signal, and did not show the noise estimated in every data point. \n",
        "\n",
        "We can plot the signal, including its `noise_map`, using the `matplotlib` `errorbar` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.errorbar(\n",
        "    xvalues, data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.title(\"1D Gaussian dataset with errors from the noise-map.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Signal Value\")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Data__\n",
        "\n",
        "How do we actually fit our `Gaussian` model to this data? First, we generate `model_data` of the 1D `Gaussian` model,\n",
        "following the same steps as the previous tutorial. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "        normalization=1.0,  # <- are the Gaussian`s model parameters.\n",
        "        sigma=5.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Gaussian profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to fit example datasets\n",
        "        via a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues: np.ndarray):\n",
        "        \"\"\"\n",
        "\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this model to create `model_data` of the `Gaussian` by passing it an input `xvalues` of the observed\n",
        "data.\n",
        "\n",
        "We do this below, and plot the resulting model-data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)\n",
        "\n",
        "gaussian = model.instance_from_vector(vector=[60.0, 20.0, 15.0])\n",
        "\n",
        "model_data = gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"1D Gaussian model.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile Normalization\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is often more informative to plot the `data` and `model_data` on the same plot for comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"Model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Different values of `centre`, `normalization` and `sigma` change the `Gaussian``s appearance. \n",
        "\n",
        "Have a go at editing some of the values input into `instance_from_vector()`, recomputing the `model_data` and\n",
        "plotting it above to see this behaviour.\n",
        "\n",
        "__Residuals__\n",
        "\n",
        "The comparison of the `data` and `model_data` above is informative, but it can be more useful to show the\n",
        "residuals, which are calculated as `data - model_data` in 1D:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "residual_map = data - model_data\n",
        "plt.plot(xvalues, residual_map, color=\"r\")\n",
        "plt.title(\"Residuals of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Are these residuals a good fit to the data? Without considering the noise in the data, we can't be sure.\n",
        "\n",
        "We can plot the residual-map with error-bars for the noise-map, which below shows that the model is a pretty bad fit,\n",
        "because many of the residuals are far away from 0 even after accounting for the noise in every data point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "residual_map = data - model_data\n",
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=residual_map,\n",
        "    yerr=noise_map,\n",
        "    color=\"r\",\n",
        "    ecolor=\"r\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        ")\n",
        "plt.title(\"Residuals of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Normalized Residuals__\n",
        "\n",
        "A different way to quantify and visualize how good (or bad) the fit is, is using the normalized residual-map (sometimes\n",
        "called the standardized residuals).\n",
        "\n",
        "This is defined as the residual-map divided by the noise-map. \n",
        "\n",
        "If you are familiar with the concept of `sigma` variancdes in statistics, the normalized residual-map is equivalent\n",
        "to the number of `sigma` the residual is from zero. For example, a normalized residual of 2.0 (which has confidence\n",
        "internals for 95%) means that the probability that the model under-estimates the data by that value is just 5.0%.\n",
        "\n",
        "The residual map with error bars and normalized residual map portray the same information, but the normalized\n",
        "residual map is better for visualization for problems with more than 1 dimension, as plotting the error bars in\n",
        "2D or more dimensions is not straight forward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "normalized_residual_map = residual_map / noise_map\n",
        "plt.plot(xvalues, normalized_residual_map, color=\"r\")\n",
        "plt.title(\"Normalized residuals of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Normalized Residuals\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Chi Squared__\n",
        "\n",
        "We now define the `chi_squared_map`, which is the `normalized_residual_map` squared, and will be used to compute the\n",
        "the final goodness of fit measure.\n",
        "\n",
        "The normalized residual map has both positive and negative values. When we square it, we therefore get only positive\n",
        "values. This means that a normalized residual of -0.2 and 0.2 both become 0.04, and therefore in the context of a\n",
        "`chi_squared` signify the same goodness-of-fit.\n",
        "\n",
        "Again, it is clear that the model gives a poor fit to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared_map = (normalized_residual_map) ** 2\n",
        "plt.plot(xvalues, chi_squared_map, color=\"r\")\n",
        "plt.title(\"Chi-Squared Map of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Chi-Squareds\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we now reduce all the information in our `chi_squared_map` into a single goodness-of-fit measure by defining the \n",
        "`chi_squared`: the sum of all values in the `chi_squared_map`.\n",
        "\n",
        "This is why having all positive and negative values in the normalized residual map become positive is important,\n",
        "as this summed measure would otherwise cancel out the positive and negative values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared = np.sum(chi_squared_map)\n",
        "print(\"Chi-squared = \", chi_squared)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The lower the chi-squared, the fewer residuals in the model's fit to the data and therefore the better our fit as\n",
        "a whole!\n",
        "\n",
        "__Noise Normalization__\n",
        "\n",
        "We now define a second quantity that will enter our final quantification of the goodness-of-fit, called the\n",
        "`noise_normalization`.\n",
        "\n",
        "This is the log sum of all noise values squared in our data. Given the noise-map is fixed, the `noise_normalization`\n",
        "retains the same value for all models that we fit, and therefore could be omitted. Nevertheless, its good practise\n",
        "to include it as it has an important meaning statistically.\n",
        "\n",
        "Lets not worry about what a `noise_normalization` actually means, because its not important for us to successfully\n",
        "get a model to fit a dataset. In a nutshell, it relates the noise in the dataset being drawn from a Gaussian\n",
        "distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Likelihood__\n",
        "\n",
        "From the `chi_squared` and `noise_normalization` we can define a final goodness-of-fit measure, the `log_likelihood`. \n",
        "\n",
        "This is the sum of the `chi_squared` and `noise_normalization` multiplied by -0.5. Why -0.5? Again, lets not worry\n",
        "about this for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "print(\"Log Likelihood = \", log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we stated that a lower `chi_squared` corresponds to a better model-data fit. \n",
        "\n",
        "When computing the `log_likelihood` we multiplied the `chi_squared` by -0.5. Therefore, a higher log likelihood\n",
        "corresponds to a better model fit, as one would hope!\n",
        "\n",
        "__Fitting Functions__\n",
        "\n",
        "If you are familiar with model-fitting, you'll have probably heard of terms like 'residuals', 'chi-squared' and\n",
        "'log_likelihood' before. These are the standard metrics by which a model-fit`s quality is quantified. They are used for\n",
        "model fitting in general, so not just when your data is 1D but when its a 2D image, 3D datacube or something else\n",
        "entirely!\n",
        "\n",
        "If you have not performed model fitting before and these terms are new to you, make sure you are clear on exactly what\n",
        "they all mean as they are at the core of all model-fitting performed in **PyAutoFit** (and statistical inference in\n",
        "general)!\n",
        "\n",
        "Lets recap everything we've learnt so far:\n",
        "    \n",
        " - We can define a model, like a 1D `Gaussian`, using Python classes that follow a certain format.\n",
        " \n",
        " - The model can be set up as a `Collection` and `Model`, having its parameters mapped to an instance of the\n",
        "   model class (e.g the `Gaussian`).  \n",
        "\n",
        " - Using this model instance, we can create model-data and compare it to data and quantify the goodness-of-fit via a \n",
        "   log likelihood.\n",
        "\n",
        "We now have everything we need to fit our model to our data! \n",
        "\n",
        "So, how do we go about finding the best-fit model? That is, what model which maximizes the log likelihood?\n",
        "\n",
        "The most simple thing we can do is guess parameters. When we guess parameters that give a good fit (e.g. a higher \n",
        "log likelihood), we then guess new parameters with values near those previous vlaues. We can repeat this process, \n",
        "over and over, until we find a really good model!\n",
        "\n",
        "For a 1D  `Gaussian` this works pretty well. Below, we fit 3 different `Gaussian` models and end up landing on\n",
        "the best-fit model (the model I used to create the dataset in the first place!).\n",
        "\n",
        "For convenience, I've create functions which compute the `log_likelihood` of a model-fit and plot the data and model\n",
        "data with errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def log_likelihood_from(data, noise_map, model_data):\n",
        "    residual_map = data - model_data\n",
        "    normalized_residual_map = residual_map / noise_map\n",
        "    chi_squared_map = (normalized_residual_map) ** 2\n",
        "    chi_squared = sum(chi_squared_map)\n",
        "    noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "    log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "    return log_likelihood\n",
        "\n",
        "\n",
        "def plot_model_fit(xvalues, data, noise_map, model_data, color=\"k\"):\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=data,\n",
        "        yerr=noise_map,\n",
        "        color=color,\n",
        "        ecolor=\"k\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.plot(xvalues, model_data, color=\"r\")\n",
        "    plt.title(\"Fit of model-data to data.\")\n",
        "    plt.xlabel(\"x values of profile\")\n",
        "    plt.ylabel(\"Profile Value\")\n",
        "    plt.show()\n",
        "    plt.clf()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Guess 1__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gaussian = model.instance_from_vector(vector=[50.0, 10.0, 5.0])\n",
        "model_data = gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "plot_model_fit(\n",
        "    xvalues=xvalues,\n",
        "    data=data,\n",
        "    noise_map=noise_map,\n",
        "    model_data=model_data,\n",
        "    color=\"r\",\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(f\"Log Likelihood: {log_likelihood}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Guess 2__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gaussian = model.instance_from_vector(vector=[50.0, 25.0, 5.0])\n",
        "model_data = gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "plot_model_fit(\n",
        "    xvalues=xvalues,\n",
        "    data=data,\n",
        "    noise_map=noise_map,\n",
        "    model_data=model_data,\n",
        "    color=\"r\",\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(f\"Log Likelihood: {log_likelihood}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Guess 3__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gaussian = model.instance_from_vector(vector=[50.0, 25.0, 10.0])\n",
        "model_data = gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "plot_model_fit(\n",
        "    xvalues=xvalues,\n",
        "    data=data,\n",
        "    noise_map=noise_map,\n",
        "    model_data=model_data,\n",
        "    color=\"r\",\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(f\"Log Likelihood: {log_likelihood}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Extensibility__\n",
        "\n",
        "Fitting models made of multiple components is straight forward. \n",
        "\n",
        "We again simply create the model via  the `Collection` object, use it to generate `model_data` and fit it to the \n",
        "data in order to compute the log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian_0=Gaussian, gaussian_1=Gaussian)\n",
        "\n",
        "instance = model.instance_from_vector(vector=[40.0, 0.2, 0.3, 60.0, 0.5, 1.0])\n",
        "\n",
        "model_data_0 = instance.gaussian_0.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "model_data_1 = instance.gaussian_1.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "\n",
        "model_data = model_data_0 + model_data_1"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot the data and model data below, showing that we get a bad fit (a low log likelihood) for this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_model_fit(\n",
        "    xvalues=xvalues,\n",
        "    data=data,\n",
        "    noise_map=noise_map,\n",
        "    model_data=model_data,\n",
        "    color=\"r\",\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(f\"Log Likelihood: {log_likelihood}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the model had just 3 parameters, it was feasible to guess values by eye and find a good fit. \n",
        "\n",
        "With six parameters, this approach becomes inefficient, and doing it with even more parameters would be impossible!\n",
        "\n",
        "In the next turorial, we will learn a more efficient and automated approach for fitting models to data.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "To end, have another quick think about the model you ultimately want to fit with **PyAutoFit**. What does the\n",
        "data look like? Is it one dimension? two dimensions? Can you easily define a model which generates realizations of\n",
        "this data? Can you picture what a residual map would look like and how you would infer a log likelihood from it?\n",
        "\n",
        "If not, don't worry about it for now, because you first need to learn how to fit a model to data using **PyAutoFit**.\n",
        "\n",
        "Part 3: Non Linear Search\n",
        "=========================\n",
        "\n",
        "In the previous tutorials, we defined a model and fitted it to data via fitting functions. We quantified the goodness\n",
        "of fit via the log likliehood and showed that for models with only a few free parameters, we could find good fits to\n",
        "the data by manually guessing parameter values. However, for more complex models, this approach was infeasible.\n",
        "\n",
        "In this tutorial, we will learn how to fit the model to data properly, using a technique that can scale up to\n",
        "models with 10s or 100s of parameters.\n",
        "\n",
        "__Parameter Space__\n",
        "\n",
        "In mathematics, we can write a function as follows:\n",
        "\n",
        "$f(x) = x^2$\n",
        "\n",
        "In this function, when we input the parameter $x$ in to the function $f$, it returns a value $f(x)$.\n",
        "\n",
        "This mapping between values of $x$ and $f(x)$ define the \"parameter space\" of this function (which fot\n",
        "the function $f(x) = x^2$ is a parabola).\n",
        "\n",
        "A function can have multiple parameters, for 3 parameters, $x$, $y$ and $z$:\n",
        "\n",
        "$f(x, y, z) = x + y^2 - z^3$\n",
        "\n",
        "The mapping between values of $x$, $y$ and $z$ and $f(x, y, z)$ define another parameter space, albeit it now\n",
        "has 3 dimensions.\n",
        "\n",
        "The concept of a parameter space relates closely to how in the previous tutorial we use instances of a 1D Gaussian\n",
        "profile, with parameters $(x, I, \\sigma)$ to fit data with a model and compute a log likelihood.\n",
        "\n",
        "This process can be thought of as a function $f (x, I, \\sigma)$, where the value returned by this function is the\n",
        "log likelihood.\n",
        "\n",
        "With that, we have introduced one of the most important concepts in model-fitting,\n",
        "the \"log likelihood function\". This function describes how we use an instance of the model (e.g. where the\n",
        "parameters have values) to compute a log likelihood describing good of a fit to the data it is.\n",
        "\n",
        "We can write this log likelihood function as follows:\n",
        "\n",
        "$f(x, N, \\sigma) = log_likelihood$\n",
        "\n",
        "By expressing the likelihood in this way, we can therefore now think of our model as having a parameter space. This\n",
        "parameter space consists of an N dimensional surface (where N is the number of free parameters) spanning all possible\n",
        "values of model parameters. This surface itself can be considered the \"likelihood surface\", and finding the peak of\n",
        "this surface is our goal when we perform model-fitting.\n",
        "\n",
        "This parameter space is \"non-linear\", meaning that the relationship between input parameters and log likelihood does\n",
        "not behave linearly. This simply means that it is not possible to predict what a log likelihood will be from a set of\n",
        "model parameters, unless a whole fit to the data is performed in order to compute the value.\n",
        "\n",
        "__Non Linear Search__\n",
        "\n",
        "Now that we are thinking about the problem in terms of a non-linear parameter space with a likelihood surface, we can\n",
        "now introduce the method used to fit the model to the data, the \"non-linear search\".\n",
        "\n",
        "Previously, we tried a basic approach, randomly guessing models until we found one that gave a good fit and\n",
        "high `log_likelihood`. Surprisingly, this is the basis of how model fitting using a non-linear search actually works!\n",
        "\n",
        "The non-linear search guesses lots of models, tracking the log likelihood of these models. As the algorithm\n",
        "progresses, it preferentially tries more models using parameter combinations that gave higher log likelihood solutions\n",
        "previously. The rationale is that if a parameters set provided a good fit to the data, models with similar values will\n",
        "too.\n",
        "\n",
        "There are two key differences between guessing random models to find a good fit and a non-linear search:\n",
        "\n",
        " - The non-linear search fits the model to the data in mere miliseconds. It therefore can compute the log likelihood\n",
        "   of tens of thousands of different model parameter combinations in order to find the highest likelihood solutions.\n",
        "   This would have been impractical for a human.\n",
        "\n",
        " - The non-linear search has a much better tracking system to remember which models it guess previously and what\n",
        "   their log likelihoods were. This means it can sample all possible solutions more thoroughly, whilst honing in on\n",
        "   those which give the highest likelihood more quickly.\n",
        "\n",
        "We can think of our non-linear search as \"searching\" parameter space, trying to find the regions of parameter space\n",
        "with the highest log likelihood values. Its goal is to find them, and then converge on the highest log likelihood\n",
        "solutions possible. In doing so, it can tell us what model parameters best-fit the data.\n",
        "\n",
        "This picture of how a non-linear search is massively simplified, and omits key details on how statistical principles\n",
        "are upheld to ensure that results are statistically robust. The goal of this chapter is to teach you how to fit a\n",
        "model to data, not the underlying principles of Bayesian inference on which model-fitting is based.\n",
        "\n",
        "If you are interested, more infrmation can be found at the following web links:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\n",
        "\n",
        "https://twiecki.io/blog/2015/11/10/mcmc-sampling/\n",
        "\n",
        "https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50\n",
        "\n",
        "__MCMC__\n",
        "\n",
        "There are many different non-linear search algorithms, which search parameter space in different ways. This tutorial\n",
        "uses a a Markov Chain Monte Carlo (MCMC) method alled `Emcee`. For now, lets not worry about the details of how\n",
        "an MCMC method actually works, and just use the simplified picture we painted above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import autofit as af\n",
        "import autofit.plot as aplt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Load and plot the dataset from the `autofit_workspace/dataset` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "xvalues = np.arange(data.shape[0])\n",
        "print(xvalues)\n",
        "\n",
        "plt.errorbar(\n",
        "    xvalues, data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.title(\"1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile Normalization\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Lets again define our 1D `Gaussian` model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "            self,\n",
        "            centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "            normalization=1.0,  # <- are the Gaussian`s model parameters.\n",
        "            sigma=5.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Gaussian profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to fit example datasets\n",
        "        via a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues: np.ndarray):\n",
        "        \"\"\"\n",
        "\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now compose our model, a single 1D Gaussian, which we will fit to the data via the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)\n",
        "\n",
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Priors__\n",
        "\n",
        "When we print its `.info`, we see that the parameters have priors (e.g. `UniformPrior`). We have so far not worried \n",
        "about what these meant, but now we understand how a non-linear search works we can now discuss what priors are.\n",
        "\n",
        "A parameter, for example the `centre` of the `Gaussian`, could take any value between negative and positive infinity. \n",
        "However, when we inspect the data it is clearly confined to values between 0.0 and 100.0. Our model parameter space \n",
        "should reflect this, and only contain solutions with these physically plausible values between 0.0 --> 100.0.\n",
        "\n",
        "One role of priors is to define where parameter space has valid solutions. The `centre` parameter has \n",
        "a `UniformPrior` with a  `lower_limit=0.0` and `upper_limit=100.0`. It therefore is already confined to the values \n",
        "discussed above.\n",
        "\n",
        "Priors have a second role: they encode our previous beliefs about a model and what values we expect the parameters \n",
        "to have. \n",
        "\n",
        "For example, imagine we had multiple datasets observing the same signal and we had already fitted the model to the \n",
        "first signal already. We may set priors that reflect this result, as we have prior knowledge of what the parameters\n",
        "will likely be. \n",
        "\n",
        "Setting priros in this way actually changes the result inferred when fitting the second dataset, because the priors \n",
        "partly constrain the result based on the information learned in the first fit. Other types of priors you will \n",
        "see throughout the autofit workspace (e.g `GaussianPrior`, `LogUniformPrior`) allow one to encode this type of \n",
        "information in a fit..\n",
        "\n",
        "In this tutorial, we will stick to uniform priors, as they are conceptually the most simple.\n",
        "\n",
        "Lets manually set the priors of the model we fit in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.normalization = af.UniformPrior(lower_limit=0.0, upper_limit=10.0)\n",
        "model.sigma = af.UniformPrior(lower_limit=0.0, upper_limit=10.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "The non-linear search requires an `Analysis` class, which:\n",
        "\n",
        " 1) Receives the data that the model fits.\n",
        "\n",
        " 2) Defines the `log_likelihood_function`, which computes a `log_likelihood` from a model instance. \n",
        "\n",
        " 3) Provides an interface between the non-linear search and the `log_likelihood_function`, so the search can determine\n",
        "    the goodness of fit of any set of model parameters.\n",
        "\n",
        "The non-linear search calls the `log_likelihood_function` many times, enabling it map out the high likelihood regions \n",
        "of parameter space and converges on the highest log likelihood solutions.\n",
        "\n",
        "Below is a suitable `Analysis` class for fitting a 1D gaussian to the data loaded above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "        \"\"\"\n",
        "        In this example the `Analysis` object only contains the `data` and `noise-map`.\n",
        "\n",
        "        It can be easily extended, for more complex data-sets and model fitting problems.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data\n",
        "            A 1D numpy array containing the data (e.g. a noisy 1D Gaussian) fitted in the workspace examples.\n",
        "        noise_map\n",
        "            A 1D numpy array containing the noise values of the data, used for computing the goodness of fit\n",
        "            metric.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        Returns the log likelihood of a fit of multiple 1D Gaussians to the dataset.\n",
        "\n",
        "        The `instance` that comes into this method is an instance of the `Gaussian` model above. The parameter values\n",
        "        are chosen by the non-linear search, based on where it thinks the high likelihood regions of parameter\n",
        "        space are.\n",
        "\n",
        "        The lines of Python code are commented out below to prevent excessive print statements when we run the\n",
        "        non-linear search, but feel free to uncomment them and run the search so you can see all the models it tries.\n",
        "\n",
        "        print(\"Gaussian Instance:\")\n",
        "        print(\"Centre = \", instance.centre)\n",
        "        print(\"Normalization = \", instance.normalization)\n",
        "        print(\"Sigma = \", instance.sigma)\n",
        "\n",
        "        We fit the data with the `Gaussian` instance, using its \"model_data_1d_via_xvalues_from\" function to create the\n",
        "        model data.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create an instance of the `Analysis` class by simply passing it the `data` and `noise_map`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "To use the non-linear search `Emcee` we simply create an instance of the `af.Emcee` object and pass the analysis\n",
        "and model to its `fit` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)\n",
        "\n",
        "search = af.Emcee()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "We begin the non-linear search by calling its `fit` method. This will take a minute or so to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"\"\"\n",
        "    Emcee has begun running.\n",
        "    This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "Upon completion the non-linear search returns a `Result` object, which contains information about the model-fit.\n",
        "\n",
        "The `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result has a \"maximum log likelihood instance\", which is the instance of the model (e.g. the `Gaussian`) with\n",
        "the model parameters that gave the highest overall log likelihood out of any model trialed by the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Likelihood Model:\\n\")\n",
        "max_log_likelihood_instance = result.samples.max_log_likelihood()\n",
        "print(\"Centre = \", max_log_likelihood_instance.centre)\n",
        "print(\"Normalization = \", max_log_likelihood_instance.normalization)\n",
        "print(\"Sigma = \", max_log_likelihood_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this to plot the maximum log likelihood fit over the data and confirm that a good fit was inferred:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = result.max_log_likelihood_instance.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"Emcee model fit to 1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples__\n",
        "\n",
        "Above, we used the `Result`'s `samples` property, which in this case is a `SamplesMCMC` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This object acts as an interface between the `Emcee` output results on your hard-disk and this Python code. For\n",
        "example, we can use it to get the parameters and log likelihood of an accepted emcee sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples.parameter_lists[10][:])\n",
        "print(result.samples.log_likelihood_list[10])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Probability Density Functions (PDF's) of the results can be plotted using the Emcee's visualization \n",
        "tool `corner.py`, which is wrapped via the `EmceePlotter` object.\n",
        "\n",
        "The PDF shows the 1D and 2D probabilities estimated for every parameter after the model-fit. The two dimensional \n",
        "figures can show the degeneracies between different parameters, for example how increasing $\\sigma$ and decreasing \n",
        "the normalization $I$ can lead to similar likelihoods and probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_plotter = aplt.EmceePlotter(samples=result.samples)\n",
        "search_plotter.corner()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A more detailed description of the `Result` object will be given in tutorial 5.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "This tutorial introduced a lot of concepts: the parameter space, likelihood surface, non-linear search, priors, \n",
        "and much more. \n",
        "\n",
        "Make sure you are confident in your understanding of them, however the next tutorial will expand on them all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# %%\n",
        "'''\n",
        "Tutorial 4: Complex Models\n",
        "==========================\n",
        "\n",
        "In this tutorial, we will fix more complex models with N=10, N=20 and more parameters. We will consider the following:\n",
        "\n",
        " - Why more complex model are more difficult to fit, and may lead the non-linear search to incorrectly infer\n",
        "   models with significantly lower likelihoods than the true maximum likelihood model.\n",
        "\n",
        " - Strategies for ensuring the non-linear search correctly estimates the maximum likelihood model.\n",
        "\n",
        " - What drives the run-times of a model-fit, and how one must carefully balance run-times with model complexity.\n",
        "for mitigating this:\n",
        "\n",
        "WHAT I NEED TO WRITE:\n",
        "\n",
        "- Example which fits an N=15 model and gets an incorrect result, concepts like \"local maxima\", model complexity,\n",
        "using composition API to simplify model, etc, using priors to do this.\n",
        "\n",
        "- Sections on run times.\n",
        "\n",
        "- Sections on non-linear search settings.\n",
        "\n",
        "Can rewrite and borrow from HowToLens.\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt\n",
        "from os import path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "We first load the dataset we will fit, which is a new `dataset` where the underlying signal is a sum of two  `Gaussian` \n",
        "profiles which share the same centre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x2\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the data shows the noisy signal is more complicated than just a 1D Gaussian.\n",
        "\n",
        "Note that both Gaussians are centred at the same point (x = 50). We will compose a model that reflects this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = np.arange(data.shape[0])\n",
        "plt.errorbar(\n",
        "    xvalues, data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.title(\"1D Gaussian dataset with errors from the noise-map.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Signal Value\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Models__\n",
        "\n",
        "We create the `Gaussian` class which will form our model components using the standard **PyAutoFit** format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "            self,\n",
        "            centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "            normalization=1.0,  # <- are the Gaussian`s model parameters.\n",
        "            sigma=5.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Gaussian profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to fit example datasets\n",
        "        via a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues: np.ndarray):\n",
        "        \"\"\"\n",
        "\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We now define the  `Analysis` class for this model-fit. \n",
        "\n",
        "The `log_likelihood_function` of this analysis now assumes that the `instance` that is input into it will contain\n",
        "multiple 1D profiles.\n",
        "\n",
        " The way the `model_data` is computed is updating accordingly (the sum of each individual Gaussian's `model_data`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        Returns the log likelihood of the fit of an `instance` containing many 1D\n",
        "        Profiles (e.g. Gaussians) to the dataset, using a model instance.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance\n",
        "            A list of 1D profiles with parameters set via the non-linear search.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The log likelihood value indicating how well this model fit the `MaskedDataset`.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        In the previous tutorial the instance was a single `Gaussian` profile, meaning we could create the model data \n",
        "        using the line:\n",
        "\n",
        "            model_data = instance.gaussian.model_data_1d_via_xvalues_from(xvalues=self.data.xvalues)\n",
        "\n",
        "        In this tutorial our instance is comprised of multiple 1D Gaussians, because we will use a `Collection` to\n",
        "        compose the model:\n",
        "\n",
        "            model = Collection(gaussian_0=Gaussian, gaussian_1=Gaussian).\n",
        "\n",
        "        By using a Collection, this means the instance parameter input into the fit function is a\n",
        "        dictionary where individual profiles (and their parameters) can be accessed as followed:\n",
        "\n",
        "            print(instance.gaussian_0)\n",
        "            print(instance.gaussian_1)\n",
        "            print(instance.gaussian_0.centre)\n",
        "\n",
        "        In this tutorial, the `model_data` is therefore the summed `model_data` of all individual Gaussians in the \n",
        "        model. The function `model_data_from_instance` performs this summation. \n",
        "        \"\"\"\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def model_data_from_instance(self, instance):\n",
        "        \"\"\"\n",
        "        To create the summed profile of all individual profiles, we use a list comprehension to iterate over\n",
        "        all profiles in the instance.\n",
        "\n",
        "        The key point to understand is that the `instance` has the properties of a Python `iterator` and therefore\n",
        "        can be looped over using the standard Python for syntax (e.g. `for profile in instance`).\n",
        "\n",
        "        __Alternative Syntax__\n",
        "\n",
        "        For those not familiar with list comprehensions, the code below shows how to use the instance to create the\n",
        "        summed profile using a more simple for loop.\n",
        "\n",
        "        model_data = np.zeros(shape=self.data.xvalues.shape[0])\n",
        "\n",
        "        for profile in instance:\n",
        "            model_data += profile.model_data_1d_via_xvalues_from(xvalues=self.data.xvalues)\n",
        "\n",
        "        return model_data\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        return sum(\n",
        "            [\n",
        "                profile.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "                for profile in instance\n",
        "            ]\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Collection__\n",
        "\n",
        "Use a `Collection` to compose the model we fit, consisting of two `Gaussian`'s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian_0=Gaussian, gaussian_1=Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Customization__\n",
        "\n",
        "We can fully customize the model that we fit. \n",
        "\n",
        "First, lets align the centres of the two `Gaussian`'s (given we know they are aligned in the data). Note that\n",
        "doing so reduces the number of free parameters in the model by 1, from N=6 to N=5.\n",
        "\n",
        "Lets suppose we have a `dataset` that consists of three `Gaussian` \n",
        "profiles, but we also know the following information about the dataset:\n",
        "\n",
        "- The 2 `Gaussian`'s are centrally aligned.\n",
        "- The `sigma` of one `Gaussian` is equal to 1.0.\n",
        "- The sigma of another `Gaussian` is above 3.0.\n",
        "\n",
        "We can edit the `Model` components we pass into the `Collection` to meet these constraints accordingly.\n",
        "\n",
        "Lets first create the model `Gaussian`'s as we did in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0 = af.Model(Gaussian)\n",
        "gaussian_1 = af.Model(Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can centrally align the two `Gaussian`'s by setting the `centre` of the first `Gaussian` to the `centre` of the\n",
        "second `Gaussian`.\n",
        "\n",
        "This removes a free parameter from the model reducing the dimensionality by 1 (from N=6 to N=5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0.centre = gaussian_1.centre"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can follow the same API to set the `sigma` of the first `Gaussian` to 1.0.\n",
        "\n",
        "This again removes another free parameter from the model (from N=5 to N=4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0.sigma = 1.0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can add assertions, for example requiring that  the `sigma` value of the second `Gaussian` is above 2.0.\n",
        "\n",
        "Assertions do not change the dimensionality of the model, because we are not fixing or removing any free parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_1.add_assertion(gaussian_1.sigma > 3.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We again input these newly customized model components into the `Collection`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(\n",
        "    gaussian_0=gaussian_0,\n",
        "    gaussian_1=gaussian_1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The customized model can be printed via the `info` attribute, where the customizes discussed above can be seen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "Lets now perform the fit using our model which is composed of two profile's in a non-linear parameter space of\n",
        "dimensionality N=4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee()\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running. \\n\"\n",
        "    \"Checkout the autofit_workspace/output/howtofit/tutorial_5__gaussian_x1__exponential_x1 \\n\"\n",
        "    \"folder for live output of the results.\\n\"\n",
        "    \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The `info` attribute shows the result in a readable format, which contains informaiton on the full collection\n",
        "of model components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Cookbooks__\n",
        "\n",
        "This tutorial illustrates how to compose model out of multiple components, using a `Collection`.\n",
        "\n",
        "**PyAutoFit** has many advanced model composition tools, which offer more customization of `Collection` objects,\n",
        "allow models to be composed and fitted to multiple datasets and for multi-level models to be created out of\n",
        "hierarchies of Python classes.\n",
        "\n",
        "Checkout the `autofit_workspace/*/model` package for these cookbooks with give a full run through of all of\n",
        "**PyAutoFit**'s model composition tools, or read them on the readthedocs:\n",
        "\n",
        " - `cookbook 1: Basics  <https://pyautofit.readthedocs.io/en/latest/cookbooks/cookbook_1_basics.html>`_\n",
        "\n",
        " - `cookbook 2: Collections  <https://pyautofit.readthedocs.io/en/latest/cookbooks/cookbook_2_collections.html>`_\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "And with that, we are complete. In this tutorial, we learned how to compose and fit complex models in **PyAutoFit**.\n",
        "\n",
        "To end, you should think again in more detail about your model fitting problem:\n",
        "\n",
        " Are there many different model components you may wish to define and fit?\n",
        "\n",
        " Is your data the super position of many different model components, like the profiles in this tutorial?\n",
        "\n",
        " In this tutorial, all components of our model did the same thing, represent a 1D profile. In your model, you may\n",
        "have model components that represent different parts of your model, which need to be combined in more complicated ways\n",
        "in order to create your model-fit. You now have all the tools you need to define, compose and fit very complex models!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}