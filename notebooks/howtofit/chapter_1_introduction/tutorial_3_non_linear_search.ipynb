{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 3: Non-linear Search\n",
        "=============================\n",
        "\n",
        "Its finally time to take our model and fit it to data, hurrah!\n",
        "\n",
        "So, how do we infer the parameters for the 1D `Gaussian` that give a good fit to our data?  In the last tutorial, we\n",
        "tried a very basic approach, randomly guessing models until we found one that gave a good fit and high log_likelihood.\n",
        "\n",
        "We discussed that this wasn`t a viable strategy for more complex models. Surprisingly, this is the basis of how model\n",
        "fitting actually works! Basically, our model-fitting algorithm guesses lots of models, tracking the log likelihood of\n",
        "these models. As the algorithm progresses, it begins to guess more models using parameter combinations that gave higher\n",
        "log_likelihood solutions previously. If a set of parameters provided a good fit to the data previously, a model with\n",
        "similar values probably will too.\n",
        "\n",
        "This is called a `NonLinearSearch` and its a fairly common tool used by scientists in a wide range of fields. We're\n",
        "going to use a NonLinearSearch algorithm called `Emcee`, which for those familiar with statistic inference is a Markov\n",
        "Chain Monte Carlo (MCMC) method. For now, lets not worry about the details of how Emcee actually works. Instead, just\n",
        "picture that a `NonLinearSearch` in **PyAutoFit** operates as follows:\n",
        "\n",
        " 1) Randomly guess a model and map the parameters via the priors to an instance of the model, in this case\n",
        " our `Gaussian`.\n",
        "\n",
        " 2) Use this model instance to generate model data and compare this model data to the data to compute a log likelihood.\n",
        "\n",
        " 3) Repeat this many times, choosing models whose parameter values are near those of models which have higher log\n",
        " likelihood values. If a new model's log likelihood is higher than previous models, new models will be chosen with\n",
        " parameters nearer this model.\n",
        "\n",
        "The idea is that if we keep guessing models with higher log-likelihood values, we'll inevitably `climb` up the gradient\n",
        "of the log likelihood in parameter space until we eventually hit the highest log likelihood models.\n",
        "\n",
        "To be clear, this overly simplified description of an MCMC algorithm is not how the *NonLinearSearch* really works. We\n",
        "are omitting crucial details on how our priors impact our inference as well as how the MCMC algorithm provides us with\n",
        "reliable errors on our parameter estimates. The goal of this chapter to teach you how to use **PyAutoFit**, not how to\n",
        "perform Bayesian inference. If you are interested in the details of how MCMC works, I recommend you checkout the\n",
        "following web links:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\n",
        "\n",
        "https://twiecki.io/blog/2015/11/10/mcmc-sampling/\n",
        "\n",
        "https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataset from the `autofit_workspace/dataset` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets remind ourselves what the data looks like, using the `plot_line` convenience method fom the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def plot_line(xvalues, line, title=None, errors=None, ylabel=None):\n",
        "\n",
        "    plt.errorbar(\n",
        "        x=xvalues, y=line, yerr=errors, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        "    ),\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x value of profile\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "xvalues = np.arange(data.shape[0])\n",
        "\n",
        "plot_line(xvalues=xvalues, line=data, errors=noise_map, title=\"Data\", ylabel=\"Data\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets import the `Gaussian` class for this tutorial, which is the model we will fit using the `NonLinearSearch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gaussian as g"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `NonLinearSearch` requires an `Analysis` class, which:\n",
        "\n",
        " - Receives the data to be fitted and prepares it so the model can fit it.\n",
        " \n",
        " - Defines the `log_likelihood_function` used to compute the `log_likelihood` from a model instance. \n",
        " \n",
        " - Passes this `log_likelihood` to the `NonLinearSearch` so that it can determine parameter values for the the next \n",
        " model that it samples.\n",
        "\n",
        "For our 1D `Gaussian` model-fitting example, here is our `Analysis` class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "\n",
        "        \"\"\"\n",
        "        The `instance` that comes into this method is an instance of the `Gaussian` class above, with the parameters\n",
        "        set to values chosen by the `NonLinearSearch`. (These are commented out to prevent excessive print statements\n",
        "        when we run the `NonLinearSearch`.\n",
        "\n",
        "        This instance`s parameter values are chosen by the `NonLinearSearch` based on the previous model with the\n",
        "        highest likelihood result.\n",
        "\n",
        "            print(\"Gaussian Instance:\")\n",
        "            print(\"Centre = \", instance.centre)\n",
        "            print(\"Intensity = \", instance.intensity)\n",
        "            print(\"Sigma = \", instance.sigma)\n",
        "\n",
        "        Below, we fit the data with the `Gaussian` instance, using its \"profile_from_xvalues\" function to create the\n",
        "        model data.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.profile_from_xvalues(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform the `NonLinearSearch` using `Emcee`, we simply compose our model using a `PriorModel`, instantiate the \n",
        "`Analysis` class and pass them to an instance of the `Emcee` class. \n",
        "\n",
        "We also pass a `name` and `path_prefrix`, which specifies that when the results are output to the folder \n",
        "`autofit_workspace/output` they'll also be written to the folder `howtofit/chapter_1/tutorial_3`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.PriorModel(g.Gaussian)\n",
        "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.intensity = af.UniformPrior(lower_limit=0.0, upper_limit=1e2)\n",
        "model.sigma = af.UniformPrior(lower_limit=0.0, upper_limit=50.0)\n",
        "\n",
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "emcee = af.Emcee(name=\"tutorial_3_non_linear_search\", path_prefix=\"howtofit\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We begin the `NonLinearSearch` by calling its `fit` method. This will take a minute or so to run (which is very fast \n",
        "for a model-fit). Whilst you're waiting, checkout the folder:\n",
        "\n",
        "`autofit_workspace/output/howtofit`\n",
        "\n",
        "Here, the results of the model-fit are output to your hard-disk (on-the-fly) and you can inspect them as the non-linear\n",
        "search runs. In particular, you'll find:\n",
        "\n",
        " - `model.info`: A file listing every model component, parameter and prior in your model-fit.\n",
        "\n",
        " - `model.results`: A file giving the latest best-fit model, parameter estimates and errors of the fit.\n",
        " \n",
        " - `search`: A folder containing the `Emcee` output in hdf5 format.txt (you'll probably never need to look at these, \n",
        "   but its good to know what they are).\n",
        " \n",
        " - Other `metadata` which you can ignore for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = emcee.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running - checkout the autofit_workspace/output/howtofit/tutorial_3_non_linear_search\"\n",
        "    \" folder for live output of the results.\"\n",
        "    \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
        ")\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once completed, the `NonLinearSearch` returns a `Result` object, which contains lots of information about the \n",
        "NonLinearSearch.\n",
        " \n",
        "A full description of the `Results` object will be given in tutorial 6 and can also be found at:\n",
        " \n",
        "`autofit_workspace/overview/simple/results`\n",
        "`autofit_workspace/overview/complex/results`.\n",
        "\n",
        "In this tutorial, lets use the `result` it to inspect the maximum likelihood model instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Likelihood Model:\\n\")\n",
        "max_log_likelihood_instance = result.samples.max_log_likelihood_instance\n",
        "print(\"Centre = \", max_log_likelihood_instance.centre)\n",
        "print(\"Intensity = \", max_log_likelihood_instance.intensity)\n",
        "print(\"Sigma = \", max_log_likelihood_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this to plot the maximum log likelihood fit over the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = result.max_log_likelihood_instance.profile_from_xvalues(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"Emcee model fit to 1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile intensity\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we used the `Result`'s `samples` property, which in this case is a `MCMCSamples` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This object acts as an interface between the `Emcee` output results on your hard-disk and this Python code. For\n",
        "example, we can use it to get the parameters and log likelihood of an accepted emcee sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples.parameters[10][:])\n",
        "print(result.samples.log_likelihoods[10])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also use it to get a model instance of the `median_pdf` model, which is the model where each parameter is\n",
        "the value estimated from the probability distribution of parameter space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mp_instance = result.samples.median_pdf_instance\n",
        "print()\n",
        "print(\"Median PDF Model:\\n\")\n",
        "print(\"Centre = \", mp_instance.centre)\n",
        "print(\"Intensity = \", mp_instance.intensity)\n",
        "print(\"Sigma = \", mp_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we'll come back to the `Samples` objects in tutorial 6!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}