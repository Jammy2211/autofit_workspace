{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 3: Non Linear Search\n",
        "=============================\n",
        "\n",
        "In the previous tutorials, we defined a model and fitted it to data via fitting functions. We quantified the goodness\n",
        "of fit via the log likliehood and showed that for models with only a few free parameters, we could find good fits to\n",
        "the data by manually guessing parameter values. However, for more complex models, this approach was infeasible.\n",
        "\n",
        "In this tutorial, we will learn how to fit the model to data properly, using a technique that can scale up to\n",
        "models with 10s or 100s of parameters.\n",
        "\n",
        "__Parameter Space__\n",
        "\n",
        "In mathematics, we can write a function as follows:\n",
        "\n",
        "$f(x) = x^2$\n",
        "\n",
        "In this function, when we input the parameter $x$ in to the function $f$, it returns a value $f(x)$.\n",
        "\n",
        "This mapping between values of $x$ and $f(x)$ define the \"parameter space\" of this function (which fot\n",
        "the function $f(x) = x^2$ is a parabola).\n",
        "\n",
        "A function can have multiple parameters, for 3 parameters, $x$, $y$ and $z$:\n",
        "\n",
        "$f(x, y, z) = x + y^2 - z^3$\n",
        "\n",
        "The mapping between values of $x$, $y$ and $z$ and $f(x, y, z)$ define another parameter space, albeit it now\n",
        "has 3 dimensions.\n",
        "\n",
        "The concept of a parameter space relates closely to how in the previous tutorial we use instances of a 1D Gaussian\n",
        "profile, with parameters $(x, I, \\sigma)$ to fit data with a model and compute a log likelihood.\n",
        "\n",
        "This process can be thought of as a function $f (x, I, \\sigma)$, where the value returned by this function is the\n",
        "log likelihood.\n",
        "\n",
        "With that, we have introduced one of the most important concepts in model-fitting,\n",
        "the \"log likelihood function\". This function describes how we use an instance of the model (e.g. where the\n",
        "parameters have values) to compute a log likelihood describing good of a fit to the data it is.\n",
        "\n",
        "We can write this log likelihood function as follows:\n",
        "\n",
        "$f(x, N, \\sigma) = log_likelihood$\n",
        "\n",
        "By expressing the likelihood in this way, we can therefore now think of our model as having a parameter space. This\n",
        "parameter space consists of an N dimensional surface (where N is the number of free parameters) spanning all possible\n",
        "values of model parameters. This surface itself can be considered the \"likelihood surface\", and finding the peak of\n",
        "this surface is our goal when we perform model-fitting.\n",
        "\n",
        "This parameter space is \"non-linear\", meaning that the relationship between input parameters and log likelihood does\n",
        "not behave linearly. This simply means that it is not possible to predict what a log likelihood will be from a set of\n",
        "model parameters, unless a whole fit to the data is performed in order to compute the value.\n",
        "\n",
        "__Non Linear Search__\n",
        "\n",
        "Now that we are thinking about the problem in terms of a non-linear parameter space with a likelihood surface, we can\n",
        "now introduce the method used to fit the model to the data, the \"non-linear search\".\n",
        "\n",
        "Previously, we tried a basic approach, randomly guessing models until we found one that gave a good fit and\n",
        "high `log_likelihood`. Surprisingly, this is the basis of how model fitting using a non-linear search actually works!\n",
        "\n",
        "The non-linear search guesses lots of models, tracking the log likelihood of these models. As the algorithm\n",
        "progresses, it preferentially tries more models using parameter combinations that gave higher log likelihood solutions\n",
        "previously. The rationale is that if a parameters set provided a good fit to the data, models with similar values will\n",
        "too.\n",
        "\n",
        "There are two key differences between guessing random models to find a good fit and a non-linear search:\n",
        "\n",
        " - The non-linear search fits the model to the data in mere miliseconds. It therefore can compute the log likelihood\n",
        "   of tens of thousands of different model parameter combinations in order to find the highest likelihood solutions.\n",
        "   This would have been impractical for a human.\n",
        "\n",
        " - The non-linear search has a much better tracking system to remember which models it guess previously and what\n",
        "   their log likelihoods were. This means it can sample all possible solutions more thoroughly, whilst honing in on\n",
        "   those which give the highest likelihood more quickly.\n",
        "\n",
        "We can think of our non-linear search as \"searching\" parameter space, trying to find the regions of parameter space\n",
        "with the highest log likelihood values. Its goal is to find them, and then converge on the highest log likelihood\n",
        "solutions possible. In doing so, it can tell us what model parameters best-fit the data.\n",
        "\n",
        "This picture of how a non-linear search is massively simplified, and omits key details on how statistical principles\n",
        "are upheld to ensure that results are statistically robust. The goal of this chapter is to teach you how to fit a\n",
        "model to data, not the underlying principles of Bayesian inference on which model-fitting is based.\n",
        "\n",
        "If you are interested, more infrmation can be found at the following web links:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\n",
        "\n",
        "https://twiecki.io/blog/2015/11/10/mcmc-sampling/\n",
        "\n",
        "https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50\n",
        "\n",
        "__MCMC__\n",
        "\n",
        "There are many different non-linear search algorithms, which search parameter space in different ways. This tutorial\n",
        "uses a a Markov Chain Monte Carlo (MCMC) method alled `Emcee`. For now, lets not worry about the details of how\n",
        "an MCMC method actually works, and just use the simplified picture we painted above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import autofit as af\n",
        "import autofit.plot as aplt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Load and plot the dataset from the `autofit_workspace/dataset` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "xvalues = np.arange(data.shape[0])\n",
        "print(xvalues)\n",
        "\n",
        "plt.errorbar(\n",
        "    xvalues, data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.title(\"1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile Normalization\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Lets again define our 1D `Gaussian` model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "        normalization=1.0,  # <- are the Gaussian`s model parameters.\n",
        "        sigma=5.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Gaussian profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to fit example datasets\n",
        "        via a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues: np.ndarray):\n",
        "        \"\"\"\n",
        "\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now compose our model, a single 1D Gaussian, which we will fit to the data via the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)\n",
        "\n",
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Priors__\n",
        "\n",
        "When we print its `.info`, we see that the parameters have priors (e.g. `UniformPrior`). We have so far not worried \n",
        "about what these meant, but now we understand how a non-linear search works we can now discuss what priors are.\n",
        "\n",
        "A parameter, for example the `centre` of the `Gaussian`, could take any value between negative and positive infinity. \n",
        "However, when we inspect the data it is clearly confined to values between 0.0 and 100.0. Our model parameter space \n",
        "should reflect this, and only contain solutions with these physically plausible values between 0.0 --> 100.0.\n",
        "\n",
        "One role of priors is to define where parameter space has valid solutions. The `centre` parameter has \n",
        "a `UniformPrior` with a  `lower_limit=0.0` and `upper_limit=100.0`. It therefore is already confined to the values \n",
        "discussed above.\n",
        "\n",
        "Priors have a second role: they encode our previous beliefs about a model and what values we expect the parameters \n",
        "to have. \n",
        "\n",
        "For example, imagine we had multiple datasets observing the same signal and we had already fitted the model to the \n",
        "first signal already. We may set priors that reflect this result, as we have prior knowledge of what the parameters\n",
        "will likely be. \n",
        "\n",
        "Setting priros in this way actually changes the result inferred when fitting the second dataset, because the priors \n",
        "partly constrain the result based on the information learned in the first fit. Other types of priors you will \n",
        "see throughout the autofit workspace (e.g `GaussianPrior`, `LogUniformPrior`) allow one to encode this type of \n",
        "information in a fit..\n",
        "\n",
        "In this tutorial, we will stick to uniform priors, as they are conceptually the most simple.\n",
        "\n",
        "Lets manually set the priors of the model we fit in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.normalization = af.UniformPrior(lower_limit=0.0, upper_limit=10.0)\n",
        "model.sigma = af.UniformPrior(lower_limit=0.0, upper_limit=10.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "The non-linear search requires an `Analysis` class, which:\n",
        "\n",
        " 1) Receives the data that the model fits.\n",
        "\n",
        " 2) Defines the `log_likelihood_function`, which computes a `log_likelihood` from a model instance. \n",
        "\n",
        " 3) Provides an interface between the non-linear search and the `log_likelihood_function`, so the search can determine\n",
        "    the goodness of fit of any set of model parameters.\n",
        "    \n",
        "The non-linear search calls the `log_likelihood_function` many times, enabling it map out the high likelihood regions \n",
        "of parameter space and converges on the highest log likelihood solutions.\n",
        "\n",
        "Below is a suitable `Analysis` class for fitting a 1D gaussian to the data loaded above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data: np.ndarray, noise_map: np.ndarray):\n",
        "        \"\"\"\n",
        "        The `Analysis` class acts as an interface between the data and model in **PyAutoFit**.\n",
        "\n",
        "        Its `log_likelihood_function` defines how the model is fitted to the data and it is called many times by\n",
        "        the non-linear search fitting algorithm.\n",
        "\n",
        "        In this example the `Analysis` `__init__` constructor only contains the `data` and `noise-map`, but it can be\n",
        "        easily extended to include other quantities.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data\n",
        "            A 1D numpy array containing the data (e.g. a noisy 1D signal) fitted in the workspace examples.\n",
        "        noise_map\n",
        "            A 1D numpy array containing the noise values of the data, used for computing the goodness of fit\n",
        "            metric, the log likelihood.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance) -> float:\n",
        "        \"\"\"\n",
        "        Returns the log likelihood of a fit of a 1D Gaussian to the dataset.\n",
        "\n",
        "        The `instance` that comes into this method is an instance of the `Gaussian` model above. The parameter values\n",
        "        are chosen by the non-linear search, based on where it thinks the high likelihood regions of parameter\n",
        "        space are.\n",
        "\n",
        "        The lines of Python code are commented out below to prevent excessive print statements when we run the\n",
        "        non-linear search, but feel free to uncomment them and run the search to see the parameters of every instance\n",
        "        that it fits.\n",
        "\n",
        "        print(\"Gaussian Instance:\")\n",
        "        print(\"Centre = \", instance.centre)\n",
        "        print(\"Normalization = \", instance.normalization)\n",
        "        print(\"Sigma = \", instance.sigma)\n",
        "\n",
        "        The data is fitted using an `instance` of the `Gaussian` class where its `model_data_1d_via_xvalues_from`\n",
        "        is called in order to create a model data representation of the Gaussian that is fitted to the data.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create an instance of the `Analysis` class by simply passing it the `data` and `noise_map`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "To use the non-linear search `Emcee` we simply create an instance of the `af.Emcee` object and pass the analysis\n",
        "and model to its `fit` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)\n",
        "\n",
        "search = af.Emcee()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "We begin the non-linear search by calling its `fit` method. This will take a minute or so to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"\"\"\n",
        "    The non-linear search has begun running.\n",
        "    This Jupyter notebook cell with progress once the search has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"The search has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "Upon completion the non-linear search returns a `Result` object, which contains information about the model-fit.\n",
        "\n",
        "The `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result has a \"maximum log likelihood instance\", which is the instance of the model (e.g. the `Gaussian`) with\n",
        "the model parameters that gave the highest overall log likelihood out of any model trialed by the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Likelihood Model:\\n\")\n",
        "max_log_likelihood_instance = result.samples.max_log_likelihood()\n",
        "print(\"Centre = \", max_log_likelihood_instance.centre)\n",
        "print(\"Normalization = \", max_log_likelihood_instance.normalization)\n",
        "print(\"Sigma = \", max_log_likelihood_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this to plot the maximum log likelihood fit over the data and confirm that a good fit was inferred:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = result.max_log_likelihood_instance.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"Emcee model fit to 1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples__\n",
        "\n",
        "Above, we used the `Result`'s `samples` property, which in this case is a `SamplesMCMC` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This object acts as an interface between the `Emcee` output results on your hard-disk and this Python code. For\n",
        "example, we can use it to get the parameters and log likelihood of an accepted emcee sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples.parameter_lists[10][:])\n",
        "print(result.samples.log_likelihood_list[10])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Probability Density Functions (PDF's) of the results can be plotted using the Emcee's visualization \n",
        "tool `corner.py`, which is wrapped via the `MCMCPlotter` object.\n",
        "\n",
        "The PDF shows the 1D and 2D probabilities estimated for every parameter after the model-fit. The two dimensional \n",
        "figures can show the degeneracies between different parameters, for example how increasing $\\sigma$ and decreasing \n",
        "the normalization $I$ can lead to similar likelihoods and probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.MCMCPlotter(samples=result.samples)\n",
        "plotter.corner_cornerpy()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A more detailed description of the `Result` object will be given in tutorial 5.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "This tutorial introduced a lot of concepts: the parameter space, likelihood surface, non-linear search, priors, \n",
        "and much more. \n",
        "\n",
        "Make sure you are confident in your understanding of them, however the next tutorial will expand on them all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}