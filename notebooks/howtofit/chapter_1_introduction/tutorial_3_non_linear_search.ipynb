{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 3: Non Linear Search\n",
        "=============================\n",
        "\n",
        "In the previous tutorials, we laid the groundwork by defining a model and manually fitting it to data using fitting\n",
        "functions. We quantified the goodness of fit using the log likelihood and demonstrated that for models with only a few\n",
        "free parameters, we could achieve satisfactory fits by manually guessing parameter values. However, as the complexity\n",
        "of our models increased, this approach quickly became impractical.\n",
        "\n",
        "In this tutorial, we will delve into a more systematic approach for fitting models to data. This technique is designed\n",
        "to handle models with a larger number of parameters\u2014ranging from tens to hundreds. By adopting this approach, we aim\n",
        "to achieve more efficient and reliable model fits, ensuring that our models accurately capture the underlying\n",
        "structure of the data.\n",
        "\n",
        "This approach not only improves the accuracy of our fits but also allows us to explore more complex models that better\n",
        "represent the systems we are studying.\n",
        "\n",
        "__Overview__\n",
        "\n",
        "In this tutorial, we will use a non-linear search to fit a 1D Gaussian profile to noisy data. Specifically, we will:\n",
        "\n",
        "- Introduce concept like a \"parameter space\", \"likelihood surface\" and \"priors\", and relate them to how a non-linear\n",
        "  search works.\n",
        "\n",
        "- Introduce the `Analysis` class, which defines the `log_likelihood_function` that quantifies the goodness of fit of a\n",
        "  model instance to the data.\n",
        "\n",
        "- Fit a 1D Gaussian model to 1D data with a non-linear search, specifically with the nested sampling algorithm\n",
        "  `Dynesty`.\n",
        "\n",
        "All these steps utilize **PyAutoFit**'s API for model-fitting.\n",
        "\n",
        "__Contents__\n",
        "\n",
        "This tutorial is split into the following sections:\n",
        "\n",
        "- **Parameter Space**: Introduce the concept of a \"parameter space\" and how it relates to model-fitting.\n",
        "- **Non-Linear Search**: Introduce the concept of a \"non-linear search\" and how it fits models to data.\n",
        "- **Nested Sampling**: Introduce the nested sampling method used in this tutorial.\n",
        "- **Deeper Background**: Provide links to resources that more thoroughly describe the statistical principles that underpin non-linear searches.\n",
        "- **Data**: Load and plot the 1D Gaussian dataset we'll fit.\n",
        "- **Model**: Introduce the 1D `Gaussian` model we'll fit to the data.\n",
        "- **Priors**: Introduce priors and how they are used to define the parameter space and guide the non-linear search.\n",
        "- **Analysis**: Introduce the `Analysis` class, which contains the `log_likelihood_function` used to fit the model to the data.\n",
        "- **Search**: Perform the model-fit using a non-linear search.\n",
        "- **Result**: The result of the model-fit, including the maximum likelihood model.\n",
        "- **Samples**: The samples of the non-linear search, used to compute parameter estimates and uncertainties.\n",
        "- **Customizing Searches**: How to customize the settings of the non-linear search.\n",
        "- **Wrap Up**: A summary of the concepts introduced in this tutorial.\n",
        "\n",
        "__Parameter Space__\n",
        "\n",
        "In mathematics, a function is defined by its parameters, which relate inputs to outputs.\n",
        "\n",
        "For example, consider a simple function:\n",
        "\n",
        "\\[ f(x) = x^2 \\]\n",
        "\n",
        "Here, \\( x \\) is the parameter input into the function \\( f \\), and \\( f(x) \\) returns \\( x^2 \\). This\n",
        "mapping between \\( x \\) and \\( f(x) \\) defines the \"parameter space\" of the function, which in this case is a parabola.\n",
        "\n",
        "Functions can have multiple parameters, such as \\( x \\), \\( y \\), and \\( z \\):\n",
        "\n",
        "\\[ f(x, y, z) = x + y^2 - z^3 \\]\n",
        "\n",
        "Here, the mapping between \\( x \\), \\( y \\), \\( z \\), and \\( f(x, y, z) \\) defines a parameter space with three\n",
        "dimensions.\n",
        "\n",
        "This concept of a parameter space relates closely to how we define and use instances of models in model-fitting.\n",
        "For instance, in our previous tutorial, we used instances of a 1D Gaussian profile with\n",
        "parameters \\( (x, I, \\sigma) \\) to fit data and compute a log likelihood. This process can be thought of as a\n",
        "function \\( f(x, I, \\sigma) \\), where the output value is the log likelihood.\n",
        "\n",
        "By expressing the likelihood in this manner, we can consider our model as having a parameter space -\u2014 a\n",
        "multidimensional surface that spans all possible values of the model parameters \\( x, I, \\sigma \\).\n",
        "\n",
        "This surface is often referred to as the \"likelihood surface\", and our objective during model-fitting is to find\n",
        "its peak.\n",
        "\n",
        "This parameter space is \"non-linear\", meaning the relationship between the input parameters and the log likelihood\n",
        "does not behave linearly. This non-linearity implies that we cannot predict the log likelihood from a set of model\n",
        "parameters without actually performing a fit to the data by performing the forward model calculation.\n",
        "\n",
        "__Non-Linear Search__\n",
        "\n",
        "Now that we understand our problem in terms of a non-linear parameter space with a likelihood surface, we can\n",
        "introduce the method used to fit the model to the data\u2014the \"non-linear search\".\n",
        "\n",
        "Previously, our approach involved manually guessing models until finding one with a good fit and high log likelihood.\n",
        "Surprisingly, this random guessing forms the basis of how model-fitting using a non-linear search actually works!\n",
        "\n",
        "A non-linear search involves systematically guessing many models while tracking their log likelihoods. As the\n",
        "algorithm progresses, it tends to favor models with parameter combinations that have previously yielded higher\n",
        "log likelihoods. This iterative refinement helps to efficiently explore the vast parameter space.\n",
        "\n",
        "There are two key differences between guessing random models and using a non-linear search:\n",
        "\n",
        "- **Computational Efficiency**: The non-linear search can evaluate the log likelihood of a model parameter\n",
        "  combinations in milliseconds and therefore many thousands of models in minutes. This computational speed enables\n",
        "  it to thoroughly sample potential solutions, which would be impractical for a human.\n",
        "\n",
        "- **Effective Sampling**: The search algorithm maintains a robust memory of previously guessed models and their log\n",
        "  likelihoods. This memory allows it to sample potential solutions more thoroughly and converge on the highest\n",
        "  likelihood solutions more efficiently, which is again impractical for a human.\n",
        "\n",
        "Think of the non-linear search as systematically exploring parameter space to pinpoint regions with the highest log\n",
        "likelihood values. Its primary goal is to identify and converge on the parameter values that best describe the data.\n",
        "\n",
        "__Nested Sampling__\n",
        "\n",
        "There are various non-linear search algorithms, each exploring parameter space differently. This tutorial utilizes a\n",
        "Nested Sampling method called `Dynesty` (https://www.imperial.ac.uk/media/imperial-college/research-centres-and-groups/astrophysics/public/icic/data-analysis-workshop/2016/NestedSampling_JRP.pdf\n",
        "\n",
        "For now, we won't delve into the specifics of how nested samlping methods work; instead, we'll stick with the simplified\n",
        "overview provided above.\n",
        "\n",
        "__Deeper Background__\n",
        "\n",
        "This very simplified explanation of how a non-linear search works overlooks many of the underlying statistical\n",
        "principles that ensure robust results and enable it to sample parameter space effectively.\n",
        "\n",
        "The focus of the HowToFit lectures are to give you a phenomenological understanding how to fit a model to data,\n",
        "rather than delving into the intricacies of Bayesian inference and statistical sampling.\n",
        "\n",
        "If you're interested in learning more about these principles, you can explore resources such as:\n",
        "\n",
        "- [Nested Sampling](https://www.imperial.ac.uk/media/imperial-college/research-centres-and-groups/astrophysics/public/icic/data-analysis-workshop/2016/NestedSampling_JRP.pdf)\n",
        "- [Markov Chain Monte Carlo (MCMC)](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)\n",
        "- [Introduction to MCMC Sampling](https://twiecki.io/blog/2015/11/10/mcmc-sampling/)\n",
        "- [A Zero-Math Introduction to MCMC Methods](https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import autofit.plot as aplt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from os import path\n",
        "\n",
        "import autofit as af\n",
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Load and plot the dataset from the `autofit_workspace/dataset` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "xvalues = np.arange(data.shape[0])\n",
        "\n",
        "plt.errorbar(\n",
        "    xvalues,\n",
        "    data,\n",
        "    yerr=noise_map,\n",
        "    linestyle=\"\",\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        ")\n",
        "plt.title(\"1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile Normalization\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Create the `Gaussian` class from which we will compose model components using the standard format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre: float = 30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "        normalization: float = 1.0,  # <- are the Gaussian`s model parameters.\n",
        "        sigma: float = 5.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Gaussian profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to perform model-fitting\n",
        "        of example datasets.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_from(self, xvalues: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.array\n",
        "            The Gaussian values at the input x coordinates.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now compose our model, a single 1D Gaussian, which we will fit to the data via the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)\n",
        "\n",
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Priors__\n",
        "\n",
        "When we examine the `.info` of our model, we notice that each parameter (like `centre`, `normalization`, \n",
        "and `sigma` in our Gaussian model) is associated with priors, such as `UniformPrior`. These priors define the \n",
        "range of permissible values that each parameter can assume during the model fitting process.\n",
        "\n",
        "For instance, consider the `centre` parameter of our Gaussian. In theory, it could take on any value from \n",
        "negative to positive infinity. However, upon inspecting our dataset, we observe that valid values for `centre` \n",
        "fall strictly between 0.0 and 100.0. By using a `UniformPrior` with `lower_limit=0.0` and `upper_limit=100.0`, \n",
        "we restrict our parameter space to include only physically plausible values.\n",
        "\n",
        "Priors serve two primary purposes:\n",
        "\n",
        "**Defining Valid Parameter Space:** Priors specify the range of parameter values that constitute valid solutions. \n",
        "This ensures that our model explores only those solutions that are consistent with our observed data and physical \n",
        "constraints.\n",
        "\n",
        "**Incorporating Prior Knowledge:** Priors also encapsulate our prior beliefs or expectations about the model \n",
        "parameters. For instance, if we have previously fitted a similar model to another dataset and obtained certain \n",
        "parameter values, we can incorporate this knowledge into our priors for a new dataset. This approach guides the \n",
        "model fitting process towards parameter values that are more probable based on our prior understanding.\n",
        "\n",
        "While we are using `UniformPriors` in this tutorial due to their simplicity, **PyAutoFit** offers various other \n",
        "priors like `GaussianPrior` and `LogUniformPrior`. These priors are useful for encoding different forms of prior \n",
        "information, such as normally distributed values around a mean (`GaussianPrior`) or parameters spanning multiple \n",
        "orders of magnitude (`LogUniformPrior`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.normalization = af.UniformPrior(lower_limit=0.0, upper_limit=10.0)\n",
        "model.sigma = af.UniformPrior(lower_limit=0.0, upper_limit=10.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "In **PyAutoFit**, the `Analysis` class plays a crucial role in interfacing between the data being fitted and the \n",
        "model under consideration. Its primary responsibilities include:\n",
        "\n",
        "**Receiving Data:** The `Analysis` class is initialized with the data (`data`) and noise map (`noise_map`) that \n",
        " the model aims to fit. \n",
        "\n",
        "**Defining the Log Likelihood Function:** The `Analysis` class defines the `log_likelihood_function`, which \n",
        " computes the log likelihood of a model instance given the data. It evaluates how well the model, for a given set of \n",
        " parameters, fits the observed data. \n",
        "\n",
        "**Interface with Non-linear Search:** The `log_likelihood_function` is repeatedly called by the non-linear search \n",
        " algorithm to assess the goodness of fit of different parameter combinations. The search algorithm call this function\n",
        " many times and maps out regions of parameter space that yield high likelihood solutions.\n",
        "    \n",
        "Below is a suitable `Analysis` class for fitting a 1D gaussian to the data loaded above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data: np.ndarray, noise_map: np.ndarray):\n",
        "        \"\"\"\n",
        "        The `Analysis` class acts as an interface between the data and model in **PyAutoFit**.\n",
        "\n",
        "        Its `log_likelihood_function` defines how the model is fitted to the data and it is called many times by\n",
        "        the non-linear search fitting algorithm.\n",
        "\n",
        "        In this example the `Analysis` `__init__` constructor only contains the `data` and `noise-map`, but it can be\n",
        "        easily extended to include other quantities.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data\n",
        "            A 1D numpy array containing the data (e.g. a noisy 1D signal) fitted in the workspace examples.\n",
        "        noise_map\n",
        "            A 1D numpy array containing the noise values of the data, used for computing the goodness of fit\n",
        "            metric, the log likelihood.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance) -> float:\n",
        "        \"\"\"\n",
        "        Returns the log likelihood of a fit of a 1D Gaussian to the dataset.\n",
        "\n",
        "        The `instance` that comes into this method is an instance of the `Gaussian` model above. The parameter values\n",
        "        are chosen by the non-linear search, based on where it thinks the high likelihood regions of parameter\n",
        "        space are.\n",
        "\n",
        "        The lines of Python code are commented out below to prevent excessive print statements when we run the\n",
        "        non-linear search, but feel free to uncomment them and run the search to see the parameters of every instance\n",
        "        that it fits.\n",
        "\n",
        "        print(\"Gaussian Instance:\")\n",
        "        print(\"Centre = \", instance.centre)\n",
        "        print(\"Normalization = \", instance.normalization)\n",
        "        print(\"Sigma = \", instance.sigma)\n",
        "\n",
        "        The data is fitted using an `instance` of the `Gaussian` class where its `model_data_from`\n",
        "        is called in order to create a model data representation of the Gaussian that is fitted to the data.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_from(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create an instance of the `Analysis` class by simply passing it the `data` and `noise_map`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "To use the non-linear search `Dynesty` we simply create an instance of the `af.DynestyStatic` object. The word\n",
        "static refers to us using the static variant of the Dynesty algorithm, rather than the dynamic variant. You don't\n",
        "need to worry about this for now.\n",
        "\n",
        "No inputs are specified for the `Dynesty` object, even though it (and all non-linear searches) have many tunable\n",
        "parameters that control the behaviour of the non-linear search. The next tutorials describes how a search's settings\n",
        "can change the behaviour of the non-linear search and your results.\n",
        "\n",
        "The default settings of the non-linear search are specified in **PyAutoFit** configuration files found in the\n",
        "`autofit_workspace/config/non_linear` folder. \n",
        "\n",
        "For now, we use the default configuration settings, which are sufficient for simple model fitting problems. In \n",
        "chapter 2, we will consider how and when these settings should be manually specified for your model fitting problem.\n",
        "\n",
        "In this example, non-linear search results are stored in memory rather and not written to hard disk because the fits \n",
        "are fast and can therefore be easily regenerated. The next tutorial will perform fits which write results to the\n",
        "hard-disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    sample=\"rwalk\",  # This makes dynesty run faster, dont worry about what it means for now!\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin the model-fit via the non-linear search, we pass it our model and analysis and begin the fit.\n",
        "\n",
        "The fit will take a minute or so to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"\"\"\n",
        "    The non-linear search has begun running.\n",
        "    This Jupyter notebook cell with progress once the search has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "model = af.Model(Gaussian)\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"The search has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "Upon completion the non-linear search returns a `Result` object, which contains information about the model-fit.\n",
        "\n",
        "The `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result has a \"maximum log likelihood instance\", which refers to the specific set of model parameters (e.g., \n",
        "for a `Gaussian`) that yielded the highest log likelihood among all models tested by the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Likelihood Model:\\n\")\n",
        "max_log_likelihood_instance = result.samples.max_log_likelihood()\n",
        "print(\"Centre = \", max_log_likelihood_instance.centre)\n",
        "print(\"Normalization = \", max_log_likelihood_instance.normalization)\n",
        "print(\"Sigma = \", max_log_likelihood_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this to plot the maximum log likelihood fit over the data and confirm that a good fit was inferred:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = result.max_log_likelihood_instance.model_data_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=data,\n",
        "    yerr=noise_map,\n",
        "    linestyle=\"\",\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"Dynesty model fit to 1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples__\n",
        "\n",
        "Above, we used the `Result`'s `samples` property, which in this case is a `SamplesNest` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This object acts as an interface between the `Dynesty` output results and your Jupyter Notebook or Python code. \n",
        "\n",
        "For example, we can use it to get the parameters and log likelihood of an accepted Dynesty sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples.parameter_lists[10][:])\n",
        "print(result.samples.log_likelihood_list[10])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can visualize the \"Probability Density Functions (PDFs)\" of the results using a software package called \n",
        "`anesthetic` through the `NestPlotter` object in Dynesty. \n",
        "\n",
        "These plots display the 1D and 2D probabilities estimated for each parameter after fitting the model. The 2D \n",
        "figures reveal parameter degeneracies, such as how changes in one parameter (like increasing \\(\\sigma\\) while \n",
        "decreasing normalization \\(I\\)) can result in similar likelihoods and probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.NestPlotter(samples=result.samples)\n",
        "plotter.corner_anesthetic()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A more detailed description of the `Result` object is given in tutorial 5 of this chapter.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "This tutorial has laid the foundation with several fundamental concepts in model fitting and statistical inference:\n",
        "\n",
        "1. **Parameter Space**: This refers to the range of possible values that each parameter in a model can take. It \n",
        "defines the dimensions over which the likelihood of different parameter values is evaluated.\n",
        "\n",
        "2. **Likelihood Surface**: This surface represents how the likelihood of the model varies across the parameter space. \n",
        "It helps in identifying the best-fit parameters that maximize the likelihood of the model given the data.\n",
        "\n",
        "3. **Non-linear Search**: This is an optimization technique used to explore the parameter space and find the \n",
        "combination of parameter values that best describe the data. It iteratively adjusts the parameters to maximize the likelihood.\n",
        "\n",
        "4. **Priors**: Priors are probabilities assigned to different values of parameters before considering the data. \n",
        "They encapsulate our prior knowledge or assumptions about the parameter values. Priors can constrain the parameter \n",
        "space, making the search more efficient and realistic.\n",
        "\n",
        "5. **Model Fitting**: The process of adjusting model parameters to minimize the difference between model predictions \n",
        "and observed data, quantified by the likelihood function.\n",
        "\n",
        "Understanding these concepts is crucial as they form the backbone of model fitting and parameter estimation in \n",
        "scientific research and data analysis. In the next tutorials, these concepts will be further expanded upon to \n",
        "deepen your understanding and provide more advanced techniques for model fitting and analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}