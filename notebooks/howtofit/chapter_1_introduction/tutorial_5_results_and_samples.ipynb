{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 5: Results And Samples\n",
        "===============================\n",
        "\n",
        "In this tutorial, we'll cover all of the output that comes from a non-linear search's `Result`  object.\n",
        "\n",
        "We used this object at various points in the chapter. The bulk of material covered here is described in the example\n",
        "script `autofit_workspace/overview/simple/result.py`. Nevertheless, it is a good idea to refresh ourselves about how\n",
        "results in **PyAutoFit** work before covering more advanced material."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt\n",
        "import os\n",
        "from os import path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Load the dataset from the `autofit_workspace/dataset` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1__exponential_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Reused Functions__\n",
        "\n",
        "We'll reuse the `plot_profile_1d` and `Analysis` classes of the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def plot_profile_1d(\n",
        "    xvalues,\n",
        "    profile_1d,\n",
        "    title=None,\n",
        "    ylabel=None,\n",
        "    errors=None,\n",
        "    color=\"k\",\n",
        "    output_path=None,\n",
        "    output_filename=None,\n",
        "):\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=profile_1d,\n",
        "        yerr=errors,\n",
        "        linestyle=\"\",\n",
        "        color=color,\n",
        "        ecolor=\"k\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x value of profile\")\n",
        "    plt.ylabel(ylabel)\n",
        "    if not path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    plt.savefig(path.join(output_path, f\"{output_filename}.png\"))\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def model_data_from_instance(self, instance):\n",
        "        \"\"\"\n",
        "        To create the summed profile of all individual profiles in an instance, we can use a dictionary comprehension\n",
        "        to iterate over all profiles in the instance.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        return sum([profile.model_data_from(xvalues=xvalues) for profile in instance])\n",
        "\n",
        "    def visualize(self, paths, instance, during_analysis):\n",
        "        \"\"\"\n",
        "        This method is identical to the previous tutorial, except it now uses the `model_data_from_instance` method\n",
        "        to create the profile.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "\n",
        "        \"\"\"The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\"\"\"\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=self.data,\n",
        "            title=\"Data\",\n",
        "            ylabel=\"Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"data\",\n",
        "        )\n",
        "\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=model_data,\n",
        "            title=\"Model Data\",\n",
        "            ylabel=\"Model Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"model_data\",\n",
        "        )\n",
        "\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=residual_map,\n",
        "            title=\"Residual Map\",\n",
        "            ylabel=\"Residuals\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"residual_map\",\n",
        "        )\n",
        "\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=chi_squared_map,\n",
        "            title=\"Chi-Squared Map\",\n",
        "            ylabel=\"Chi-Squareds\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"chi_squared_map\",\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "Now lets run the non-linear search to get ourselves a `Result`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "        normalization=1.0,  # <- are the Gaussian`s model parameters.\n",
        "        sigma=5.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Gaussian profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to fit example datasets\n",
        "        via a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_from(self, xvalues: np.ndarray):\n",
        "        \"\"\"\n",
        "\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n",
        "\n",
        "\n",
        "class Exponential:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "        normalization=1.0,  # <- are the Exponential`s model parameters.\n",
        "        rate=0.01,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Exponential profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to fit example datasets\n",
        "        via a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        ratw\n",
        "            The decay rate controlling has fast the Exponential declines.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.rate = rate\n",
        "\n",
        "    def model_data_from(self, xvalues: np.ndarray):\n",
        "        \"\"\"\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return self.normalization * np.multiply(\n",
        "            self.rate, np.exp(-1.0 * self.rate * abs(transformed_xvalues))\n",
        "        )\n",
        "\n",
        "\n",
        "model = af.Collection(gaussian=af.Model(Gaussian), exponential=af.Model(Exponential))\n",
        "\n",
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee(\n",
        "    name=\"tutorial_5_results_and_samples\",\n",
        "    path_prefix=path.join(\"howtofit\", \"chapter_1\"),\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"\"\"\n",
        "    The non-linear search has begun running.\n",
        "    Checkout the autofit_workspace/output/howtofit/tutorial_6__results_and_samples\n",
        "    folder for live output of the results.\n",
        "    This Jupyter notebook cell with progress once the search has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"The search has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "Here, we'll look in detail at what information is contained in the `Result`.\n",
        "\n",
        "It contains an `info` attribute which prints the result in readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples__\n",
        "\n",
        "The result contains a `Samples` object, which contains all of the non-linear search samples. \n",
        "\n",
        "Each sample corresponds to a set of model parameters that were evaluated and accepted by our non linear search, \n",
        "in this example emcee. \n",
        "\n",
        "This also includes their log likelihoods, which are used for computing additional information about the model-fit,\n",
        "for example the error on every parameter. \n",
        "\n",
        "Our model-fit used the MCMC algorithm Emcee, so the `Samples` object returned is a `SamplesMCMC` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"MCMC Samples: \\n\")\n",
        "print(samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Parameters__\n",
        "\n",
        "The parameters are stored as a list of lists, where:\n",
        "\n",
        " - The outer list is the size of the total number of samples.\n",
        " - The inner list is the size of the number of free parameters in the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "print(\"Sample 5's second parameter value (Gaussian -> normalization):\")\n",
        "print(samples.parameter_lists[4][1])\n",
        "print(\"Sample 10`s third parameter value (Gaussian -> sigma)\")\n",
        "print(samples.parameter_lists[9][2], \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Figures of Merit__\n",
        "\n",
        "The Samples class also contains the log likelihood, log prior, log posterior and weight_list of every accepted sample, \n",
        "where:\n",
        "\n",
        "- The log likelihood is the value evaluated from the likelihood function (e.g. -0.5 * chi_squared + the noise \n",
        "normalized).\n",
        "\n",
        "- The log prior encodes information on how the priors on the parameters maps the log likelihood value to the log \n",
        "posterior value.\n",
        "\n",
        "- The log posterior is log_likelihood + log_prior.\n",
        "\n",
        "- The weight gives information on how samples should be combined to estimate the posterior. The weight values depend on \n",
        "the sampler used, for MCMC samples they are all 1 (e.g. all weighted equally).\n",
        "     \n",
        "Lets inspect the last 10 values of each for the analysis.     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"log(likelihood), log(prior), log(posterior) and weight of the tenth sample.\")\n",
        "print(samples.log_likelihood_list[9])\n",
        "print(samples.log_prior_list[9])\n",
        "print(samples.log_posterior_list[9])\n",
        "print(samples.weight_list[9])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Instances__\n",
        "\n",
        "The `Samples` contains many results which are returned as an instance of the model, using the Python class structure\n",
        "of the model composition.\n",
        "\n",
        "For example, we can return the model parameters corresponding to the maximum log likelihood sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_lh_instance = samples.max_log_likelihood()\n",
        "\n",
        "print(\"Max Log Likelihood `Gaussian` Instance:\")\n",
        "print(\"Centre = \", max_lh_instance.gaussian.centre)\n",
        "print(\"Normalization = \", max_lh_instance.gaussian.normalization)\n",
        "print(\"Sigma = \", max_lh_instance.gaussian.sigma, \"\\n\")\n",
        "\n",
        "print(\"Max Log Likelihood Exponential Instance:\")\n",
        "print(\"Centre = \", max_lh_instance.exponential.centre)\n",
        "print(\"Normalization = \", max_lh_instance.exponential.normalization)\n",
        "print(\"Sigma = \", max_lh_instance.exponential.rate, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Vectors__\n",
        "\n",
        "All results can alternatively be returned as a 1D vector of values, by passing `as_instance=False`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_lh_vector = samples.max_log_likelihood(as_instance=False)\n",
        "print(\"Max Log Likelihood Model Parameters: \\n\")\n",
        "print(max_lh_vector, \"\\n\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Labels__\n",
        "\n",
        "Vectors return a lists of all model parameters, but do not tell us which values correspond to which parameters.\n",
        "\n",
        "The following quantities are available in the `Model`, where the order of their entries correspond to the parameters \n",
        "in the `ml_vector` above:\n",
        " \n",
        " - `paths`: a list of tuples which give the path of every parameter in the `Model`.\n",
        " - `parameter_names`: a list of shorthand parameter names derived from the `paths`.\n",
        " - `parameter_labels`: a list of parameter labels used when visualizing non-linear search results (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = samples.model\n",
        "\n",
        "print(model.paths)\n",
        "print(model.parameter_names)\n",
        "print(model.parameter_labels)\n",
        "print(model.model_component_and_parameter_names)\n",
        "print(\"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From here on, we will returned all results information as instances, but every method below can be returned as a\n",
        "vector via the `as_instance=False` input.\n",
        "\n",
        "__Posterior / PDF__\n",
        "\n",
        "The ``Result`` object contains the full posterior information of our non-linear search, which can be used for\n",
        "parameter estimation. \n",
        "\n",
        "The median pdf vector is available from the `Samples` object, which estimates the every parameter via 1D \n",
        "marginalization of their PDFs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "median_pdf_instance = samples.median_pdf()\n",
        "\n",
        "print(\"Max Log Likelihood `Gaussian` Instance:\")\n",
        "print(\"Centre = \", median_pdf_instance.gaussian.centre)\n",
        "print(\"Normalization = \", median_pdf_instance.gaussian.normalization)\n",
        "print(\"Sigma = \", median_pdf_instance.gaussian.sigma, \"\\n\")\n",
        "\n",
        "print(\"Max Log Likelihood Exponential Instance:\")\n",
        "print(\"Centre = \", median_pdf_instance.exponential.centre)\n",
        "print(\"Normalization = \", median_pdf_instance.exponential.normalization)\n",
        "print(\"Sigma = \", median_pdf_instance.exponential.rate, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Plot__\n",
        "\n",
        "Because results are returned as instances, it is straight forward to use them and their associated functionality\n",
        "to make plots of the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_gaussian = max_lh_instance.gaussian.model_data_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "model_exponential = max_lh_instance.exponential.model_data_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "model_data = model_gaussian + model_exponential\n",
        "\n",
        "plt.plot(range(data.shape[0]), data)\n",
        "plt.plot(range(data.shape[0]), model_data)\n",
        "plt.plot(range(data.shape[0]), model_gaussian, \"--\")\n",
        "plt.plot(range(data.shape[0]), model_exponential, \"--\")\n",
        "plt.title(\"Illustrative model fit to 1D `Gaussian` + Exponential profile data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Errors__\n",
        "\n",
        "The samples include methods for computing the error estimates of all parameters, via 1D marginalization at an \n",
        "input sigma confidence limit. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "errors_at_upper_sigma_instance = samples.errors_at_upper_sigma(sigma=3.0)\n",
        "errors_at_lower_sigma_instance = samples.errors_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Upper Error values of Gaussian (at 3.0 sigma confidence):\")\n",
        "print(\"Centre = \", errors_at_upper_sigma_instance.gaussian.centre)\n",
        "print(\"Normalization = \", errors_at_upper_sigma_instance.gaussian.normalization)\n",
        "print(\"Sigma = \", errors_at_upper_sigma_instance.gaussian.sigma, \"\\n\")\n",
        "\n",
        "print(\"lower Error values of Gaussian (at 3.0 sigma confidence):\")\n",
        "print(\"Centre = \", errors_at_lower_sigma_instance.gaussian.centre)\n",
        "print(\"Normalization = \", errors_at_lower_sigma_instance.gaussian.normalization)\n",
        "print(\"Sigma = \", errors_at_lower_sigma_instance.gaussian.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They can also be returned at the values of the parameters at their error values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "values_at_upper_sigma_instance = samples.values_at_upper_sigma(sigma=3.0)\n",
        "values_at_lower_sigma_instance = samples.values_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Upper Parameter values w/ error of Gaussian (at 3.0 sigma confidence):\")\n",
        "print(\"Centre = \", values_at_upper_sigma_instance.gaussian.centre)\n",
        "print(\"Normalization = \", values_at_upper_sigma_instance.gaussian.normalization)\n",
        "print(\"Sigma = \", values_at_upper_sigma_instance.gaussian.sigma, \"\\n\")\n",
        "\n",
        "print(\"lower Parameter values w/ errors of Gaussian (at 3.0 sigma confidence):\")\n",
        "print(\"Centre = \", values_at_lower_sigma_instance.gaussian.centre)\n",
        "print(\"Normalization = \", values_at_lower_sigma_instance.gaussian.normalization)\n",
        "print(\"Sigma = \", values_at_lower_sigma_instance.gaussian.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__PDF__\n",
        "\n",
        "The Probability Density Functions (PDF's) of the results can be plotted using the Emcee's visualization \n",
        "tool `corner.py`, which is wrapped via the `MCMCPlotter` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.MCMCPlotter(samples=result.samples)\n",
        "plotter.corner_cornerpy()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Other Results__\n",
        "\n",
        "The samples contain many useful vectors, including the samples with the highest posterior values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_log_posterior_instance = samples.max_log_posterior()\n",
        "\n",
        "print(\"Maximum Log Posterior Vector:\")\n",
        "print(\"Centre = \", max_log_posterior_instance.gaussian.centre)\n",
        "print(\"Normalization = \", max_log_posterior_instance.gaussian.normalization)\n",
        "print(\"Sigma = \", max_log_posterior_instance.gaussian.sigma, \"\\n\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All methods above are available as a vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "median_pdf_instance = samples.median_pdf(as_instance=False)\n",
        "values_at_upper_sigma = samples.values_at_upper_sigma(sigma=3.0, as_instance=False)\n",
        "values_at_lower_sigma = samples.values_at_lower_sigma(sigma=3.0, as_instance=False)\n",
        "errors_at_upper_sigma = samples.errors_at_upper_sigma(sigma=3.0, as_instance=False)\n",
        "errors_at_lower_sigma = samples.errors_at_lower_sigma(sigma=3.0, as_instance=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Sample Instance__\n",
        "\n",
        "A non-linear search retains every model that is accepted during the model-fit.\n",
        "\n",
        "We can create an instance of any lens model -- below we create an instance of the last accepted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.from_sample_index(sample_index=-1)\n",
        "\n",
        "print(\"Gaussian Instance of last sample\")\n",
        "print(\"Centre = \", instance.gaussian.centre)\n",
        "print(\"Normalization = \", instance.gaussian.normalization)\n",
        "print(\"Sigma = \", instance.gaussian.sigma, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Bayesian Evidence__\n",
        "\n",
        "If a nested sampling `NonLinearSearch` is used, the evidence of the model is also available which enables Bayesian\n",
        "model comparison to be performed (given we are using Emcee, which is not a nested sampling algorithm, the log evidence \n",
        "is None).:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_evidence = samples.log_evidence"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Derived Errors (PDF from samples)__\n",
        "\n",
        "Computing the errors of a quantity like the `sigma` of the Gaussian is simple, because it is sampled by the non-linear \n",
        "search. Thus, to get their errors above we used the `Samples` object to simply marginalize over all over parameters \n",
        "via the 1D Probability Density Function (PDF).\n",
        "\n",
        "Computing errors on derived quantities is more tricky, because they are not sampled directly by the non-linear search. \n",
        "For example, what if we want the error on the full width half maximum (FWHM) of the Gaussian? In order to do this\n",
        "we need to create the PDF of that derived quantity, which we can then marginalize over using the same function we\n",
        "use to marginalize model parameters.\n",
        "\n",
        "Below, we compute the FWHM of every accepted model sampled by the non-linear search and use this determine the PDF \n",
        "of the FWHM. When combining the FWHM's we weight each value by its `weight`. For Emcee, an MCMC algorithm, the\n",
        "weight of every sample is 1, but weights may take different values for other non-linear searches.\n",
        "\n",
        "In order to pass these samples to the function `marginalize`, which marginalizes over the PDF of the FWHM to compute \n",
        "its error, we also pass the weight list of the samples.\n",
        "\n",
        "(Computing the error on the FWHM could be done in much simpler ways than creating its PDF from the list of every\n",
        "sample. We chose this example for simplicity, in order to show this functionality, which can easily be extended to more\n",
        "complicated derived quantities.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fwhm_list = []\n",
        "\n",
        "for sample in samples.sample_list:\n",
        "    instance = sample.instance_for_model(model=samples.model)\n",
        "\n",
        "    sigma = instance.gaussian.sigma\n",
        "\n",
        "    fwhm = 2 * np.sqrt(2 * np.log(2)) * sigma\n",
        "\n",
        "    fwhm_list.append(fwhm)\n",
        "\n",
        "median_fwhm, lower_fwhm, upper_fwhm = af.marginalize(\n",
        "    parameter_list=fwhm_list, sigma=3.0, weight_list=samples.weight_list\n",
        ")\n",
        "\n",
        "print(f\"FWHM = {median_fwhm} ({upper_fwhm} {lower_fwhm}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples Filtering__\n",
        "\n",
        "Our samples object has the results for all three parameters in our model. However, we might only be interested in the\n",
        "results of a specific parameter.\n",
        "\n",
        "The basic form of filtering specifies parameters via their path, which was printed above via the model and is printed \n",
        "again below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Parameter paths in the model which are used for filtering:\")\n",
        "print(samples.model.paths)\n",
        "\n",
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "samples = samples.with_paths([(\"gaussian\", \"centre\")])\n",
        "\n",
        "print(\"All parameters of the very first sample (containing only the Gaussian centre.\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "print(\"Maximum Log Likelihood Model Instances (containing only the Gaussian centre):\\n\")\n",
        "print(samples.max_log_likelihood(as_instance=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we specified each path as a list of tuples of strings. \n",
        "\n",
        "This is how the source code internally stores the path to different components of the model, but it is not \n",
        "in-profile_1d with the PyAutoFIT API used to compose a model.\n",
        "\n",
        "We can alternatively use the following API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "samples = samples.with_paths([\"gaussian.centre\"])\n",
        "\n",
        "print(\"All parameters of the very first sample (containing only the Gaussian centre).\")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we filtered the `Samples` but asking for all parameters which included the path (\"gaussian\", \"centre\").\n",
        "\n",
        "We can alternatively filter the `Samples` object by removing all parameters with a certain path. Below, we remove\n",
        "the Gaussian's `centre` to be left with 2 parameters; the `normalization` and `sigma`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Parameter paths in the model which are used for filtering:\")\n",
        "print(samples.model.paths)\n",
        "\n",
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "samples = samples.without_paths([\"gaussian.centre\"])\n",
        "\n",
        "print(\n",
        "    \"All parameters of the very first sample (containing only the Gaussian normalization and sigma).\"\n",
        ")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Latex__\n",
        "\n",
        "If you are writing modeling results up in a paper, you can use inbuilt latex tools to create latex table \n",
        "code which you can copy to your .tex document.\n",
        "\n",
        "By combining this with the filtering tools below, specific parameters can be included or removed from the latex.\n",
        "\n",
        "Remember that the superscripts of a parameter are loaded from the config file `notation/label.yaml`, providing high\n",
        "levels of customization for how the parameter names appear in the latex table. This is especially useful if your model\n",
        "uses the same model components with the same parameter, which therefore need to be distinguished via superscripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "latex = af.text.Samples.latex(\n",
        "    samples=result.samples,\n",
        "    median_pdf_model=True,\n",
        "    sigma=3.0,\n",
        "    name_to_label=True,\n",
        "    include_name=True,\n",
        "    include_quickmath=True,\n",
        "    prefix=\"Example Prefix \",\n",
        "    suffix=\" \\\\[-2pt]\",\n",
        ")\n",
        "\n",
        "print(latex)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}