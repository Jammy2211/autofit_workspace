{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 4: Why Modeling Is Hard\n",
        "================================\n",
        "\n",
        "We have successfully fitted a simple 1D Gaussian profile to a dataset using a non-linear search. While achieving an\n",
        "accurate model fit has been straightforward, the reality is that model fitting is a challenging problem where many things can go wrong.\n",
        "\n",
        "This tutorial will illustrate why modeling is challenging, highlight common problems that occur when fitting complex\n",
        "models, and show how a good scientific approach can help us overcome these challenges.\n",
        "\n",
        "We will build on concepts introduced in previous tutorials, such as the non-linear parameter space, likelihood surface,\n",
        "and the role of priors.\n",
        "\n",
        "__Overview__\n",
        "\n",
        "In this tutorial, we will fit complex models with up to 15 free parameters and consider the following:\n",
        "\n",
        "- Why more complex models are more difficult to fit and may lead the non-linear search to infer an incorrect solution.\n",
        "\n",
        "- Strategies for ensuring the non-linear search estimates the correct solution.\n",
        "\n",
        "- What drives the run-times of a model fit and how to carefully balance run-times with model complexity.\n",
        "\n",
        "__Contents__\n",
        "\n",
        "- **Data**: Load and plot the 1D Gaussian dataset we'll fit, which is more complex than the previous tutorial.\n",
        "- **Model**: The `Gaussian` model component that we will fit to the data.\n",
        "- **Analysis**: The log likelihood function used to fit the model to the data.\n",
        "- **Collection**: The `Collection` model used to compose the model-fit.\n",
        "- **Model Fit**: Perform the model-fit and examine the results.\n",
        "- **Result**: Determine if the model-fit was successful and what can be done to ensure a good model-fit.\n",
        "- **Why Modeling is Hard**: Introduce the concept of randomness and local maxima and why they make model-fitting challenging.\n",
        "- **Prior Tuning**: Adjust the priors of the model to help the non-linear search find the global maxima solution.\n",
        "- **Reducing Complexity**: Simplify the model to reduce the dimensionality of the parameter space.\n",
        "- **Search More Thoroughly**: Adjust the non-linear search settings to search parameter space more thoroughly.\n",
        "- **Run Times**: Discuss how the likelihood function and complexity of a model impacts the run-time of a model-fit.\n",
        "- **Model Mismatches**: Introduce the concept of model mismatches and how it makes inferring the correct model challenging.\n",
        "- **Astronomy Example**: How the concepts of this tutorial are applied to real astronomical problems.\n",
        "- **Wrap Up**: A summary of the key takeaways of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Load the dataset we fit. \n",
        "\n",
        "This is a new `dataset` where the underlying signal is a sum of five `Gaussian` profiles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x5\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the data reveals that the signal is more complex than a simple 1D Gaussian, as the wings to the left and \n",
        "right are more extended than what a single Gaussian profile can account for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = np.arange(data.shape[0])\n",
        "plt.errorbar(\n",
        "    xvalues,\n",
        "    data,\n",
        "    yerr=noise_map,\n",
        "    linestyle=\"\",\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        ")\n",
        "plt.title(\"1D Gaussian dataset with errors from the noise-map.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Signal Value\")\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Create the `Gaussian` class from which we will compose model components using the standard format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre: float = 30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "        normalization: float = 1.0,  # <- are the Gaussian`s model parameters.\n",
        "        sigma: float = 5.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Gaussian profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to perform model-fitting\n",
        "        of example datasets.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_from(self, xvalues: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.array\n",
        "            The Gaussian values at the input x coordinates.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "To define the Analysis class for this model-fit, we need to ensure that the `log_likelihood_function` can handle an \n",
        "instance containing multiple 1D profiles. Below is an expanded explanation and the corresponding class definition:\n",
        "\n",
        "The log_likelihood_function will now assume that the instance it receives consists of multiple Gaussian profiles. \n",
        "For each Gaussian in the instance, it will compute the model_data and then sum these to create the overall `model_data` \n",
        "that is compared to the observed data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data: np.ndarray, noise_map: np.ndarray):\n",
        "        \"\"\"\n",
        "        The `Analysis` class acts as an interface between the data and model in **PyAutoFit**.\n",
        "\n",
        "        Its `log_likelihood_function` defines how the model is fitted to the data and it is called many times by\n",
        "        the non-linear search fitting algorithm.\n",
        "\n",
        "        In this example, the `log_likelihood_function` receives an instance containing multiple instances of\n",
        "        the `Gaussian` class and sums the `model_data` of each to create the overall model fit to the data.\n",
        "\n",
        "        In this example the `Analysis` `__init__` constructor only contains the `data` and `noise-map`, but it can be\n",
        "        easily extended to include other quantities.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data\n",
        "            A 1D numpy array containing the data (e.g. a noisy 1D signal) fitted in the workspace examples.\n",
        "        noise_map\n",
        "            A 1D numpy array containing the noise values of the data, used for computing the goodness of fit\n",
        "            metric, the log likelihood.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance) -> float:\n",
        "        \"\"\"\n",
        "        Returns the log likelihood of a fit of a 1D Gaussian to the dataset.\n",
        "\n",
        "        In the previous tutorial, the instance was a single `Gaussian` profile, however this function now assumes\n",
        "        the instance contains multiple `Gaussian` profiles.\n",
        "\n",
        "        The `model_data` is therefore the summed `model_data` of all individual Gaussians in the model.\n",
        "\n",
        "        The docstring below describes this in more detail.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance\n",
        "            A list of 1D profiles with parameters set via the non-linear search.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The log likelihood value indicating how well this model fit the `MaskedDataset`.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        In the previous tutorial the instance was a single `Gaussian` profile, meaning we could create the model data \n",
        "        using the line:\n",
        "\n",
        "            model_data = instance.gaussian.model_data_from(xvalues=self.data.xvalues)\n",
        "\n",
        "        In this tutorial our instance is comprised of three 1D Gaussians, because we will use a `Collection` to\n",
        "        compose the model:\n",
        "\n",
        "            model = Collection(gaussian_0=Gaussian, gaussian_1=Gaussian, gaussian_2=Gaussian).\n",
        "\n",
        "        By using a Collection, this means the instance parameter input into the fit function is a\n",
        "        dictionary where individual profiles (and their parameters) can be accessed as followed:\n",
        "\n",
        "            print(instance.gaussian_0)\n",
        "            print(instance.gaussian_1)\n",
        "            print(instance.gaussian_2)\n",
        "            \n",
        "            print(instance.gaussian_0.centre)\n",
        "            print(instance.gaussian_1.centre)\n",
        "            print(instance.gaussian_2.centre)\n",
        "\n",
        "        The `model_data` is therefore the summed `model_data` of all individual Gaussians in the model. \n",
        "        \n",
        "        The function `model_data_from_instance` performs this summation. \n",
        "        \"\"\"\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def model_data_from_instance(self, instance):\n",
        "        \"\"\"\n",
        "        To create the summed profile of all individual profiles, we use a list comprehension to iterate over\n",
        "        all profiles in the instance.\n",
        "\n",
        "        The `instance` has the properties of a Python `iterator` and therefore can be looped over using the standard\n",
        "        Python for syntax (e.g. `for profile in instance`).\n",
        "\n",
        "        __Alternative Syntax__\n",
        "\n",
        "        For those not familiar with Python list comprehensions, the code below shows how to use the instance to\n",
        "        create the summed profile using a for loop and numpy array:\n",
        "\n",
        "        model_data = np.zeros(shape=self.data.xvalues.shape[0])\n",
        "\n",
        "        for profile in instance:\n",
        "            model_data += profile.model_data_from(xvalues=self.data.xvalues)\n",
        "\n",
        "        return model_data\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        return sum([profile.model_data_from(xvalues=xvalues) for profile in instance])\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Collection__\n",
        "\n",
        "In the previous tutorial, we fitted a single `Gaussian` profile to the dataset by turning it into a model \n",
        "component using the `Model` class.\n",
        "\n",
        "In this tutorial, we will fit a model composed of five `Gaussian` profiles. To do this, we need to combine \n",
        "five `Gaussian` model components into a single model.\n",
        "\n",
        "This can be achieved using a `Collection` object, which was introduced in tutorial 1. The `Collection` object allows \n",
        "us to group together multiple model components\u2014in this case, five `Gaussian` profiles\u2014into one model that can be \n",
        "passed to the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(\n",
        "    gaussian_0=Gaussian,\n",
        "    gaussian_1=Gaussian,\n",
        "    gaussian_2=Gaussian,\n",
        "    gaussian_3=Gaussian,\n",
        "    gaussian_4=Gaussian,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `model.info` confirms the model is composed of 5 `Gaussian` profiles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "We again use the nested sampling algorithm Dynesty to fit the model to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    sample=\"rwalk\",  # This makes dynesty run faster, don't worry about what it means for now!\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "Perform the fit using our five `Gaussian` model, which has 15 free parameters.\n",
        "\n",
        "This means the non-linear parameter space has a dimensionality of N=15, making it significantly more complex \n",
        "than the simpler model we fitted in the previous tutorial.\n",
        "\n",
        "Consequently, the non-linear search takes slightly longer to run but still completes in under a minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "print(\n",
        "    \"\"\"\n",
        "    The non-linear search has begun running.\n",
        "    This Jupyter notebook cell with progress once the search has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"The search has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The `info` attribute shows the result in a readable format, which contains information on the full collection\n",
        "of all 5 model components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the result info, it is hard to assess if the model fit was good or not.\n",
        "\n",
        "A good way to evaluate the fit is through a visual inspection of the model data plotted over the actual data.\n",
        "\n",
        "If the model data (red line) consistently aligns with the data points (black error bars), the fit is good. \n",
        "However, if the model misses certain features of the data, such as peaks or regions of high intensity, \n",
        "the fit was not successful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = result.max_log_likelihood_instance\n",
        "\n",
        "model_data_0 = instance.gaussian_0.model_data_from(xvalues=np.arange(data.shape[0]))\n",
        "model_data_1 = instance.gaussian_1.model_data_from(xvalues=np.arange(data.shape[0]))\n",
        "model_data_2 = instance.gaussian_2.model_data_from(xvalues=np.arange(data.shape[0]))\n",
        "model_data_3 = instance.gaussian_3.model_data_from(xvalues=np.arange(data.shape[0]))\n",
        "model_data_4 = instance.gaussian_4.model_data_from(xvalues=np.arange(data.shape[0]))\n",
        "\n",
        "model_data_list = [model_data_0, model_data_1, model_data_2, model_data_3, model_data_4]\n",
        "\n",
        "model_data = sum(model_data_list)\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=data,\n",
        "    yerr=noise_map,\n",
        "    linestyle=\"\",\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        ")\n",
        "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
        "for model_data_1d_individual in model_data_list:\n",
        "    plt.plot(range(data.shape[0]), model_data_1d_individual, \"--\")\n",
        "plt.title(f\"Fit (log likelihood = {result.log_likelihood})\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.savefig(\"fit.png\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's challenging to determine from the plot whether the data and model data perfectly overlap across the entire dataset.\n",
        "\n",
        "To clarify this, the residual map introduced in tutorial 2 is useful. It provides a clear representation of where \n",
        "the differences between the model and data exceed the noise level.\n",
        "\n",
        "Regions where the black error bars do not align with the zero line in the residual map indicate areas where the model \n",
        "did not fit the data well and is inconsistent with the data above the noise level. Furthermore, regions where\n",
        "larger values of residuals are next to one another indicate that the model failed to accurate fit that\n",
        "region of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "residual_map = data - model_data\n",
        "plt.plot(range(data.shape[0]), np.zeros(data.shape[0]), \"--\", color=\"b\")\n",
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=residual_map,\n",
        "    yerr=noise_map,\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        "    linestyle=\"\",\n",
        ")\n",
        "plt.title(f\"Residuals (log likelihood = {result.log_likelihood})\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.savefig(\"residual_map.png\")\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The normalized residual map, as discussed in tutorial 2, provides an alternative visualization of the fit quality.\n",
        "\n",
        "Normalized residuals indicate the standard deviation (\u03c3) level at which the residuals could have been drawn from the \n",
        "noise. For instance, a normalized residual of 2.0 suggests that a residual value is 2.0\u03c3 away from the noise, \n",
        "implying there is a 5% chance such a residual would occur due to noise.\n",
        "\n",
        "Values of normalized residuals above 3.0 are particularly improbable (occurring only 0.3% of the time), which is \n",
        "generally considered a threshold where issues with the model-fit are likely the cause of the residual as opposed\n",
        "to it being a noise fluctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "residual_map = data - model_data\n",
        "normalized_residual_map = residual_map / noise_map\n",
        "plt.plot(xvalues, normalized_residual_map, color=\"k\")\n",
        "plt.title(f\"Normalized Residuals (log likelihood = {result.log_likelihood})\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Normalized Residuals ($\\sigma$)\")\n",
        "plt.savefig(\"normalized_residual_map.png\")\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, did you achieve a good fit? Maybe a bad one? Or just an okay one?\n",
        "\n",
        "The truth is, I don't know, and I can't tell you for sure. Modeling is inherently random. It's not uncommon to \n",
        "fit the same model to the same dataset using the same non-linear search and get a different result each time.\n",
        "\n",
        "When I ran the model fit above, that's exactly what happened. It produced a range of fits: some bad, some okay, and \n",
        "some good, as shown in the images below:\n",
        "\n",
        "<table><tr>\n",
        "<td> <img src=\"https://github.com/Jammy2211/autofit_workspace/blob/main/scripts/howtofit/chapter_1_introduction/images/bad_fit.png?raw=true\" width=\"400\" height=\"400\"></td>\n",
        "<td> <img src=\"https://github.com/Jammy2211/autofit_workspace/blob/main/scripts/howtofit/chapter_1_introduction/images/okay_fit.png?raw=true\" width=\"400\" height=\"400\">  </td>\n",
        "<td> <img src=\"https://github.com/Jammy2211/autofit_workspace/blob/main/scripts/howtofit/chapter_1_introduction/images/good_fit.png?raw=true\" width=\"400\" height=\"400\"> </td>\n",
        "</tr></table>\n",
        "\n",
        "Distinguishing between the good and okay fit is difficult, however the normalized residuals make this easier. They show\n",
        "that for the okay fit there are residuals above 3.0 sigma, indicating that the model did not perfectly fit the data.\n",
        "\n",
        "<table><tr>\n",
        "<td> <img src=\"https://github.com/Jammy2211/autofit_workspace/blob/main/scripts/howtofit/chapter_1_introduction/images/bad_normalized_residual_map.png?raw=true\" width=\"400\" height=\"400\"></td>\n",
        "<td> <img src=\"https://github.com/Jammy2211/autofit_workspace/blob/main/scripts/howtofit/chapter_1_introduction/images/okay_normalized_residual_map.png?raw=true\" width=\"400\" height=\"400\">  </td>\n",
        "<td> <img src=\"https://github.com/Jammy2211/autofit_workspace/blob/main/scripts/howtofit/chapter_1_introduction/images/good_normalized_residual_map.png?raw=true\" width=\"400\" height=\"400\"> </td>\n",
        "</tr></table>\n",
        "\n",
        "You should quickly rerun the code above a couple of times to see this variability for yourself.\n",
        "\n",
        "__Why Modeling is Hard__\n",
        "\n",
        "This variability is at the heart of why modeling is challenging. The process of model-fitting is stochastic, \n",
        "meaning it's hard as the scientist to determine if a better fit is possible or not.\n",
        "\n",
        "Why does modeling produce different results each time, and why might it sometimes infer solutions that are not good fits?\n",
        "\n",
        "In the previous tutorial, the non-linear search consistently found models that visually matched the data well, \n",
        "minimizing residuals and yielding high log likelihood values. These optimal solutions are called 'global maxima', \n",
        "they are where the model parameters correspond to the highest likelihood regions across the entire parameter space. \n",
        "This ideal scenario is illustrated in the `good_fit.png` image above.\n",
        "\n",
        "However, non-linear searches do not always find these global maxima. Instead, they might settle on 'local maxima' \n",
        "solutions, which have high log likelihood values relative to nearby models in parameter space but are significantly \n",
        "lower than the true global maxima found elsewhere.\n",
        "\n",
        "This is what happened for the okay and bad fits above. The non-linear search converged on solutions that were locally\n",
        "peaks on the likelihood surface but were not the global maximum solution. This is why the residuals were higher and\n",
        "the normalized residuals above 3.0 sigma.\n",
        "\n",
        "Why does a non-linear search end up at local maxima? As discussed, the search iterates through many models, \n",
        "focusing more on regions where previous guesses yielded higher likelihoods. It gradually converges around \n",
        "solutions with higher likelihoods compared to surrounding models. If the search isn't exhaustive enough, it might \n",
        "converge on a local maxima that appears good compared to nearby models but isn't the global maximum.\n",
        "\n",
        "Modeling is challenging because the parameter spaces of complex models are typically filled with local maxima, \n",
        "making it hard for a non-linear search to locate the global maximum.\n",
        "\n",
        "Fortunately, there are strategies to help non-linear searches find the global maxima, and we'll now explore three of \n",
        "them.\n",
        "\n",
        "__Prior Tuning__\n",
        "\n",
        "First, let's assist our non-linear search by tuning our priors. Priors provide guidance to the search on where to \n",
        "explore in the parameter space. By setting more accurate priors ('tuning' them), we can help the search find the \n",
        "global solution instead of settling for a local maximum.\n",
        "\n",
        "For instance, from the data itself, it's evident that all `Gaussian` profiles are centered around pixel 50. In our \n",
        "previous fit, the `centre` parameter of each `Gaussian` had a `UniformPrior` spanning from 0.0 to 100.0, which is \n",
        "much broader than necessary given the data's range.\n",
        "\n",
        "Additionally, the peak value of the data's `normalization` parameter was around 17.5. This indicates that \n",
        "the `normalization` values of our `Gaussians` do not exceed 500.0, allowing us to refine our prior accordingly.\n",
        "\n",
        "The following code snippet adjusts these priors for the `centre` and `normalization` parameters of \n",
        "each `Gaussian` using **PyAutoFit**'s API for model and prior customization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0 = af.Model(Gaussian)\n",
        "\n",
        "gaussian_0.centre = af.UniformPrior(lower_limit=45.0, upper_limit=55.0)\n",
        "gaussian_0.normalization = af.LogUniformPrior(lower_limit=0.1, upper_limit=500.0)\n",
        "\n",
        "gaussian_1 = af.Model(Gaussian)\n",
        "\n",
        "gaussian_1.centre = af.UniformPrior(lower_limit=45.0, upper_limit=55.0)\n",
        "gaussian_1.normalization = af.LogUniformPrior(lower_limit=0.1, upper_limit=500.0)\n",
        "\n",
        "gaussian_2 = af.Model(Gaussian)\n",
        "\n",
        "gaussian_2.centre = af.UniformPrior(lower_limit=45.0, upper_limit=55.0)\n",
        "gaussian_2.normalization = af.LogUniformPrior(lower_limit=0.1, upper_limit=500.0)\n",
        "\n",
        "gaussian_3 = af.Model(Gaussian)\n",
        "\n",
        "gaussian_3.centre = af.UniformPrior(lower_limit=45.0, upper_limit=55.0)\n",
        "gaussian_3.normalization = af.LogUniformPrior(lower_limit=0.1, upper_limit=500.0)\n",
        "\n",
        "gaussian_4 = af.Model(Gaussian)\n",
        "\n",
        "gaussian_4.centre = af.UniformPrior(lower_limit=45.0, upper_limit=55.0)\n",
        "gaussian_4.normalization = af.LogUniformPrior(lower_limit=0.1, upper_limit=500.0)\n",
        "\n",
        "model = af.Collection(\n",
        "    gaussian_0=gaussian_0,\n",
        "    gaussian_1=gaussian_1,\n",
        "    gaussian_2=gaussian_2,\n",
        "    gaussian_3=gaussian_3,\n",
        "    gaussian_4=gaussian_4,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model is now using the priors specified above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now repeat the model-fit using these updated priors.\n",
        "\n",
        "First, you should note that the run time of the fit is significantly faster than the previous fit. This is because\n",
        "the prior is telling the non-linear search where to look, meaning it converges on solutions more quickly\n",
        "and spends less time searching regions of parameter space that do not contain solutions.\n",
        "\n",
        "Second, the model-fit consistently produces a good model-fit more often, because our tuned priors are centred\n",
        "on the global maxima solution ensuring the non-linear search is less likely to converge on a local maxima."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"\"\"\n",
        "    The non-linear search has begun running.\n",
        "    This Jupyter notebook cell with progress once the search has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"The search has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets print the result info and plot the fit to the dataset to confirm the priors have provided a better model-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=data,\n",
        "    yerr=noise_map,\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        "    linestyle=\"\",\n",
        ")\n",
        "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
        "for model_data_1d_individual in model_data_list:\n",
        "    plt.plot(range(data.shape[0]), model_data_1d_individual, \"--\")\n",
        "plt.title(f\"Fit (log likelihood = {result.log_likelihood})\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.savefig(\"fit.png\")\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.close()\n",
        "\n",
        "residual_map = data - model_data\n",
        "normalized_residual_map = residual_map / noise_map\n",
        "plt.plot(xvalues, normalized_residual_map, color=\"k\")\n",
        "plt.title(f\"Normalized Residuals (log likelihood = {result.log_likelihood})\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Normalized Residuals ($\\sigma$)\")\n",
        "plt.savefig(\"normalized_residual_map.png\")\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets consider the advantages and disadvantages of prior tuning:\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "- Higher likelihood of finding the global maximum log likelihood solutions in parameter space.\n",
        "\n",
        "- Faster search times, as the non-linear search explores less of the parameter space.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "- Incorrectly specified priors could lead the non-linear search to an incorrect solution.\n",
        "\n",
        "- It is not always clear how the priors should be tuned, especially for complex models.\n",
        "\n",
        "- Priors tuning must be applied to each dataset fitted. For large datasets, this process would be very time-consuming.\n",
        "\n",
        "__Reducing Complexity__\n",
        "\n",
        "The non-linear search may fail because the model is too complex, making its parameter space too difficult to \n",
        "sample accurately consistent. To address this, we may be able to simplify the model while ensuring it remains \n",
        "realistic enough for our scientific study. By making certain assumptions, we can reduce the number of model \n",
        "parameters, thereby lowering the dimensionality of the parameter space and improving the search's performance.\n",
        "\n",
        "For example, we may know that the `Gaussian`'s in our model are aligned at the same `centre`. We can therefore\n",
        "compose a model that assumes that the `centre` of each `Gaussian` is the same, reducing the dimensionality of the\n",
        "model from N=15 to N=11.\n",
        "\n",
        "The code below shows how we can customize the model components to ensure the `centre` of each `Gaussian` is the same:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0 = af.Model(Gaussian)\n",
        "gaussian_1 = af.Model(Gaussian)\n",
        "gaussian_2 = af.Model(Gaussian)\n",
        "gaussian_3 = af.Model(Gaussian)\n",
        "gaussian_4 = af.Model(Gaussian)\n",
        "\n",
        "gaussian_1.centre = gaussian_0.centre\n",
        "gaussian_2.centre = gaussian_0.centre\n",
        "gaussian_3.centre = gaussian_0.centre\n",
        "gaussian_4.centre = gaussian_0.centre\n",
        "\n",
        "model = af.Collection(\n",
        "    gaussian_0=gaussian_0,\n",
        "    gaussian_1=gaussian_1,\n",
        "    gaussian_2=gaussian_2,\n",
        "    gaussian_3=gaussian_3,\n",
        "    gaussian_4=gaussian_4,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model is now using the same `centre` for all `Gaussian`'s and has 11 free parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now repeat the model-fit using this updated model where the `centre` of each `Gaussian` is the same.\n",
        "\n",
        "You should again note that the run time of the fit is significantly faster than the previous fits\n",
        "and that it consistently produces a good model-fit more often. This is because the model is less complex,\n",
        "non-linear parameter space is less difficult to sample and the search is less likely to converge on a local maxima."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"\"\"\n",
        "    The non-linear search has begun running.\n",
        "    This Jupyter notebook cell with progress once the search has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"The search has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets print the result info and plot the fit to the dataset to confirm the reduced model complexity has \n",
        "provided a better model-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=data,\n",
        "    yerr=noise_map,\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        "    linestyle=\"\",\n",
        ")\n",
        "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
        "for model_data_1d_individual in model_data_list:\n",
        "    plt.plot(range(data.shape[0]), model_data_1d_individual, \"--\")\n",
        "plt.title(f\"Fit (log likelihood = {result.log_likelihood})\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.savefig(\"fit.png\")\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.close()\n",
        "\n",
        "residual_map = data - model_data\n",
        "normalized_residual_map = residual_map / noise_map\n",
        "plt.plot(xvalues, normalized_residual_map, color=\"k\")\n",
        "plt.title(f\"Normalized Residuals (log likelihood = {result.log_likelihood})\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Normalized Residuals ($\\sigma$)\")\n",
        "plt.savefig(\"normalized_residual_map.png\")\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u2019s consider the advantages and disadvantages of simplifying the model:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "- By reducing the complexity of the parameter space, we increase the chances of finding the global maximum log \n",
        "likelihood, and the search requires less time to do so.\n",
        "\n",
        "- Unlike with tuned priors, this approach is not specific to a single dataset, allowing us to use it on many datasets.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "- Our model is less realistic, which may negatively impact the accuracy of our fit and the scientific results we\n",
        "derive from it.\n",
        "\n",
        "__Search More Thoroughly__\n",
        "\n",
        "In approaches 1 and 2, we assisted our non-linear search to find the highest log likelihood regions of parameter \n",
        "space. In approach 3, we're simply going to tell the search to look more thoroughly through parameter space.\n",
        "\n",
        "Every non-linear search has settings that control how thoroughly it explores parameter space. For Dynesty, the \n",
        "primary setting is the number of live points `nlive`. The more thoroughly the search examines the space, the more \n",
        "likely it is to find the global maximum model. However, this also means the search will take longer!\n",
        "\n",
        "Below, we configure a more thorough Dynesty search with `nlive=500`. It is currently unclear what changing\n",
        "this setting actually does and what the number of live points actually means. These will be covered in chapter 2\n",
        "of the **HowToFit** lectures, where we will also expand on how a non-linear search actually works and the different\n",
        "types of methods that can be used to search parameter space. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(\n",
        "    gaussian_0=Gaussian,\n",
        "    gaussian_1=Gaussian,\n",
        "    gaussian_2=Gaussian,\n",
        "    gaussian_3=Gaussian,\n",
        "    gaussian_4=Gaussian,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `model.info` confirms the model is the same model fitted initially, composed of 5 `Gaussian` profiles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "We again use the nested sampling algorithm Dynesty to fit the model to the data, but now increase the number of live \n",
        "points to 300 meaning it will search parameter space more thoroughly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    nlive=300,\n",
        "    sample=\"rwalk\",  # This makes dynesty run faster, don't worry about what it means for now!\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "Perform the fit using our five `Gaussian` model, which has 15 free parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "print(\n",
        "    \"\"\"\n",
        "    The non-linear search has begun running.\n",
        "    This Jupyter notebook cell with progress once the search has completed - this could take a few minutes!\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"The search has finished run - you may now continue the notebook.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets print the result info and plot the fit to the dataset to confirm the more thorough search has provided a better\n",
        "model-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=data,\n",
        "    yerr=noise_map,\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        "    linestyle=\"\",\n",
        ")\n",
        "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
        "for model_data_1d_individual in model_data_list:\n",
        "    plt.plot(range(data.shape[0]), model_data_1d_individual, \"--\")\n",
        "plt.title(f\"Fit (log likelihood = {result.log_likelihood})\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.savefig(\"fit.png\")\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.close()\n",
        "\n",
        "residual_map = data - model_data\n",
        "normalized_residual_map = residual_map / noise_map\n",
        "plt.plot(xvalues, normalized_residual_map, color=\"k\")\n",
        "plt.title(f\"Normalized Residuals (log likelihood = {result.log_likelihood})\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Normalized Residuals ($\\sigma$)\")\n",
        "plt.savefig(\"normalized_residual_map.png\")\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you repeat the fit multiple times, you will find that the model-fit is more likely to produce a good fit than\n",
        "previously. \n",
        "\n",
        "However, the run-time of the search is noticeably longer, taking a few minutes to complete, owining\n",
        "to the increased number of live points and fact it is searching parameter space more thoroughly.\n",
        "\n",
        "Let's list the advantages and disadvantages of simply adjusting the non-linear search:\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "- It\u2019s easy to set up; just change the settings of the non-linear search.\n",
        "\n",
        "- It generalizes to any dataset.\n",
        "\n",
        "- We can retain a more complex model.\n",
        "\n",
        "**Disadvantage:**\n",
        "\n",
        "- It can be very expensive in terms of run time, producing run-times that are five, tens or even longer than the\n",
        "  original run-time.\n",
        "\n",
        "__Summary__\n",
        "\n",
        "We have covered three strategies for ensuring the non-linear search estimates the correct solution:\n",
        "\n",
        "1) Prior Tuning: By setting our priors more accurately, we can help the search find the global solution instead of\n",
        "    getting stuck at a local maxima.\n",
        "    \n",
        "2) Reducing Complexity: By making certain assumptions, we can reduce the number of model parameters, thereby lowering\n",
        "    the dimensionality of the parameter space and improving the search's performance.\n",
        "    \n",
        "3) Search More Thoroughly: By increasing the number of live points, we can make the search explore parameter space\n",
        "    more thoroughly, increasing the likelihood of finding the global maximum model.\n",
        "    \n",
        "Each of these strategies has its advantages and disadvantages, and your ability to fit models successfully will\n",
        "likely rely on a combination of these strategies. Which one works best depends on the specific model, dataset and\n",
        "scientific question you are trying to answer. Therefore, when you begin your own model-fitting, it is a good idea to\n",
        "try each of these strategies and assess which works best for your model-fit.\n",
        "\n",
        "__Run Times__\n",
        "\n",
        "One challenging aspect of model-fitting which was not properly covered in this tutorial is the run-time of a model-fit.\n",
        "This example fits simple 1D datasets, which are computationally inexpensive to fit. That is, the `log_likelihood_function`\n",
        "is evaluated in a fraction of a second, meaning the non-linear search fitted the model in mere minutes.\n",
        "\n",
        "Many model-fitting tasks are not as fast. For example, when fitting a model to a 2D image, the `log_likelihood_function`\n",
        "may take of order seconds, or longer, because it comprises a number of expensive calculations (e.g. a Fourier transform,\n",
        "2D convolution, etc.). Depending on the model complexity, this means that the non-linear search may take hours, days \n",
        "or even weeks to fit the model.\n",
        "\n",
        "Run times are also dictated by the complexity of the model and the nature of the log likelihood function. For models\n",
        "with many more dimensions than the simple 1D model used in this tutorial (e.g. hundreds or thousands of free parameters),\n",
        "non-linear search may take tens or hundreds of more iterations to converge on a solution. This is because the parameter\n",
        "space is significantly more complex and difficult to sample accurately. More iterations mean longer run times,\n",
        "which in combination with a slow likelihood function can make model-fitting infeasible.\n",
        "\n",
        "Whether or not run times will pose a challenge to your model-fitting task depends on the complexity of the model and\n",
        "nature of the log likelihood function. If your problem is computationally expensive, **PyAutoFit** provides many\n",
        "tools to help, which will be the topic of tutorials in chapter 2 of the **HowToFit** lectures. \n",
        "\n",
        "**PyAutoFit** provides tools to profile the run-time of your log likelihood function, which can be used to\n",
        "assess the computational expense of your model-fit and plan accordingly. Below is an example of the simplest use\n",
        "of these tools, an estimate of the run-time of the log likelihood function using one random instance of the model.\n",
        "\n",
        "\n",
        "Feature still being developed, IGNORE FOR NOW.\n",
        "\n",
        "run_time_dict, info_dict = analysis.profile_log_likelihood_function(\n",
        "    instance=model.random_instance()\n",
        ")\n",
        "print(f\"Log Likelihood Evaluation Time (second) = {run_time_dict['fit_time']}\")\n",
        "\n",
        "__Model Mismatch__\n",
        "\n",
        "In this example, interpreting how well the model fitted the data, and whether it found the global maxima, was\n",
        "relatively straightforward. This is because the same model was used to simulate the data and fit it, meaning the \n",
        "global maxima fit corresponded to one where the normalized residuals were minimized and consistent with the noise\n",
        "(e.g. they went to sigma values below 3.0 or so).\n",
        "\n",
        "In many scientific studies, the data that one is fitting may have come from an instrument or simulation where the\n",
        "exact physical processes that generate the data are not perfectly known. This then means that the model is\n",
        "not a perfect representation of the data, and it may not ever be possible to fit the data perfectly. In this case,\n",
        "we might infer a fit with significant residuals, but it may still correspond to the global maxima solution,\n",
        "at least for that particular model.\n",
        "\n",
        "This makes it even more difficult to be certain if the non-linear search is sampling parameter space correctly,\n",
        "and therefore requires even more care and attention to the strategies we have discussed above.\n",
        "\n",
        "Whether or not this is the case for your model-fitting task is something you will have to determine yourself. \n",
        "**PyAutoFit** provides many tools to help assess the quality of a model-fit, which will be the topic of tutorials\n",
        "in chapter 2 of the **HowToFit** lectures.\n",
        "\n",
        "__Astronomy Example__\n",
        "\n",
        "At the end of chapter 1, we will fit a complex model to a real astronomical dataset in order to quantify\n",
        "the distribution of stars in 2D images of galaxies.\n",
        "\n",
        "This example will illustrate many of the challenges discussed in this tutorial, including:\n",
        "\n",
        "- Fits using more complex models consisting of 15-20 parameters often infer local maxima, unless we assist the\n",
        "  non-linear search with tuned priors, reduced complexity or a more thorough search.\n",
        "\n",
        "- Fitting 2D imaging data requires a 2D convolution, which is somewhat computationally expensive and means run times\n",
        "  become something we must balance with model complexity.\n",
        "\n",
        "- The model is not a perfect representation of the data. For example, the model assumes the galaxy is elliptically\n",
        "  symmetric, whereas the real galaxy may not be. In certain examples, this means that the global maxima solution\n",
        "  actually leaves significant residuals, above 3.0 $\\sigma$, in the data.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "Now is a good time to assess how straightforward or difficult you think your model-fitting task will be. \n",
        "\n",
        "Are the models you will be fitting made up of tens of parameters? or thousands? Are there ways you can simplify\n",
        "the model parameter or tune priors to make the model-fitting task more feasible? Will run times be an issue, or is\n",
        "your likelihood function computationally cheap? And how confident are you that the model you are fitting is a good\n",
        "representation of the data?\n",
        "\n",
        "These are all questions you should be asking yourself before beginning your model-fitting task, but they will\n",
        "become easier to answer as you gain experience with model-fitting and **PyAutoFit**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}