{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 5: Complex Models\n",
        "==========================\n",
        "\n",
        "Up to now, we've fitted a very simple model, a 1D `Gaussian` with 3 free parameters. In this tutorial, we'll look at\n",
        "how **PyAutoFit** allows us to compose and fit models of arbitrary complexity.\n",
        "\n",
        "To begin, you should check out the module `autofit_workspace/howtofit/chapter_1_introduction/profiles.py`.\n",
        "\n",
        "In previous tutorials we used the module `gaussian.py` which contained only the `Gaussian` class. The `profiles.py`\n",
        "includes a second profile, `Exponential`, which like the `Gaussian` class is a model-component that can be fitted to\n",
        "data.\n",
        "\n",
        "Up to now, our data has always been generated using a single `Gaussian` profile. Thus, we have only needed to fit\n",
        "it with a single `Gaussian`. In this tutorial, our `dataset` is now a superpositions of multiple profiles. The models\n",
        "we compose and fit are therefore composed of multiple profiles, such that when we generate the model-data we\n",
        "generate it as the sum of all individual profiles in our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "import os\n",
        "from os import path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Lets quickly recap tutorial 1, where using `Models` we created a `Gaussian` as a model component and used it to \n",
        "map a list of parameters to a model `instance`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import profiles as p\n",
        "\n",
        "model = af.Model(p.Gaussian)\n",
        "\n",
        "print(\"Model `Gaussian` object: \\n\")\n",
        "print(model)\n",
        "\n",
        "instance = model.instance_from_vector(vector=[0.1, 0.2, 0.3])\n",
        "\n",
        "print(\"Model Instance: \\n\")\n",
        "print(instance)\n",
        "\n",
        "print(\"Instance Parameters \\n\")\n",
        "print(\"x = \", instance.centre)\n",
        "print(\"normalization = \", instance.normalization)\n",
        "print(\"sigma = \", instance.sigma)\n",
        "\n",
        "print()\n",
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Collection__\n",
        "\n",
        "Defining a model using multiple model components is straight forward in **PyAutoFit**, using a `Collection`\n",
        "object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(\n",
        "    gaussian=af.Model(p.Gaussian), exponential=af.Model(p.Exponential)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A `Collection` behaves like a `Model` but contains a collection of model components. For example, it\n",
        "creates a model instance by mapping a list of parameters, which in this case is 6 (3 for the `Gaussian` (centre,\n",
        "normalization, sigma) and 3 for the `Exponential` (centre, normalization, rate))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = model.instance_from_vector(vector=[0.1, 0.2, 0.3, 0.4, 0.5, 0.01])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This `instance` contains each of the model components we defined above, using the input argument name of the\n",
        "`Collection` to define the attributes in the `instance`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Instance Parameters \\n\")\n",
        "print(\"x (Gaussian) = \", instance.gaussian.centre)\n",
        "print(\"normalization (Gaussian) = \", instance.gaussian.normalization)\n",
        "print(\"sigma (Gaussian) = \", instance.gaussian.sigma)\n",
        "print(\"x (Exponential) = \", instance.exponential.centre)\n",
        "print(\"normalization (Exponential) = \", instance.exponential.normalization)\n",
        "print(\"sigma (Exponential) = \", instance.exponential.rate)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All of the information about the collection can be printed at once using its `info` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can call the components of a `Collection` whatever we want, and the mapped `instance` will use those names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_custom_names = af.Collection(\n",
        "    custom_name=af.Model(p.Gaussian), another_custom_name=af.Model(p.Exponential)\n",
        ")\n",
        "\n",
        "instance = model_custom_names.instance_from_vector(\n",
        "    vector=[0.1, 0.2, 0.3, 0.4, 0.5, 0.01]\n",
        ")\n",
        "\n",
        "print(\"Instance Parameters \\n\")\n",
        "print(\"x (Gaussian) = \", instance.custom_name.centre)\n",
        "print(\"normalization (Gaussian) = \", instance.custom_name.normalization)\n",
        "print(\"sigma (Gaussian) = \", instance.custom_name.sigma)\n",
        "print(\"x (Exponential) = \", instance.another_custom_name.centre)\n",
        "print(\"normalization (Exponential) = \", instance.another_custom_name.normalization)\n",
        "print(\"sigma (Exponential) = \", instance.another_custom_name.rate)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These names are also seen in the `info` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Plot Function__\n",
        "\n",
        "To perform visualization we'll again use the plot_profile_1d function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def plot_profile_1d(\n",
        "    xvalues,\n",
        "    profile_1d,\n",
        "    title=None,\n",
        "    ylabel=None,\n",
        "    errors=None,\n",
        "    color=\"k\",\n",
        "    output_path=None,\n",
        "    output_filename=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot 1D data on a plot of x versus y, where the x-axis is the x coordinate of the profile and the y-axis\n",
        "    is the normalization of the profile at that coordinate.\n",
        "\n",
        "    The function include options to output the image to the hard-disk as a .png.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    xvalues\n",
        "        The x-coordinates the profile is defined on.\n",
        "    profile_1d\n",
        "        The normalization values of the profile which are plotted.\n",
        "    ylabel\n",
        "        The y-label of the plot.\n",
        "    output_path\n",
        "        The path the image is to be output to hard-disk as a .png.\n",
        "    output_filename\n",
        "        The filename of the file if it is output as a .png.\n",
        "    output_format\n",
        "        Determines where the plot is displayed on your screen (\"show\") or output to the hard-disk as a png (\"png\").\n",
        "    \"\"\"\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=profile_1d,\n",
        "        yerr=errors,\n",
        "        color=color,\n",
        "        ecolor=\"k\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x value of profile\")\n",
        "    plt.ylabel(ylabel)\n",
        "    if not path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    plt.savefig(path.join(output_path, f\"{output_filename}.png\"))\n",
        "    plt.clf()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "Now we can create a model composed of multiple components we need to fit it to our data. To do this, we use updated \n",
        "`Analysis` class that creates the `model_data` as a super position of all of the model's individual `Profile`'s. For \n",
        "example, in the model above, the `model_data` is the sum of the `Gaussian`'s  individual profile and `Exponential`'s \n",
        "individual profile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        Returns the log likelihood of a list of Profiles (Gaussians, Exponentials, etc.) to the dataset, using a\n",
        "        model instance.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance\n",
        "            The list of Profile model instance (e.g. the Gaussians, Exponentials, etc.).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The log likelihood value indicating how well this model fit the `MaskedDataset`.\n",
        "\n",
        "        In tutorials 3 & 4, the instance was an instance of a single `Gaussian` profile. PyAutoFit knew this instance\n",
        "        would contain just one Gaussian, because when the model was created we used a Model object in PyAutoFit\n",
        "        to make the Gaussian. This meant we could create the model data using the line:\n",
        "\n",
        "            model_data = instance.gaussian.model_data_1d_via_xvalues_from(xvalues=self.masked_dataset.xvalues)\n",
        "\n",
        "        In this tutorial our instance is comprised of multiple Profile objects, because we used a Collection:\n",
        "\n",
        "            model = Collection(gaussian=profiles.Gaussian, exponential=profiles.Exponential).\n",
        "\n",
        "        By using a Collection, this means the instance parameter input into the fit function is a\n",
        "        dictionary where individual profiles (and their parameters) can be accessed as followed:\n",
        "\n",
        "            print(instance.gaussian)\n",
        "            print(instance.exponential)\n",
        "            print(instance.exponential.centre)\n",
        "\n",
        "        The names of the attributes of the instance correspond to what we input into the Collection. Lets\n",
        "        look at a second example:\n",
        "\n",
        "        model = Collection(\n",
        "                      gaussian_0=af.Model(profiles.Gaussian),\n",
        "                      gaussian_1=af.Model(profiles.Gaussian),\n",
        "                      whatever_i_want=af.Model(profiles.Exponential)\n",
        "                 ).\n",
        "\n",
        "        print(instance.gaussian_0)\n",
        "        print(instance.gaussian_1)\n",
        "        print(instance.whatever_i_want.centre)\n",
        "\n",
        "        A Collection allows us to name our model components whatever we want!\n",
        "\n",
        "        In this tutorial, we want our `fit` function to fit the data with a profile which is the summed profile\n",
        "        of all individual profiles in the model. Look at `model_data_from_instance` to see how we do this.\n",
        "        \"\"\"\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def model_data_from_instance(self, instance):\n",
        "        \"\"\"\n",
        "        To create the summed profile of all individual profiles in an instance, we can use a dictionary comprehension\n",
        "        to iterate over all profiles in the instance.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        return sum(\n",
        "            [\n",
        "                profile.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "                for profile in instance\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        For those not familiar with dictionary comprehensions, below I've included how one would use the instance to \n",
        "        create the summed profile using a more simple for loop.\n",
        "\n",
        "        model_data = np.zeros(shape=self.masked_dataset.xvalues.shape[0])\n",
        "\n",
        "        for profile in instance:\n",
        "            model_data += profile.model_data_1d_via_xvalues_from(xvalues=self.masked_dataset.xvalues)\n",
        "\n",
        "        return model_data\n",
        "        \"\"\"\n",
        "\n",
        "    def visualize(self, paths, instance, during_analysis):\n",
        "        \"\"\"\n",
        "        This method is identical to the previous tutorial, except it now uses the `model_data_from_instance` method\n",
        "        to create the profile.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "\n",
        "        \"\"\"\n",
        "        The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\n",
        "        \"\"\"\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=self.data,\n",
        "            title=\"Data\",\n",
        "            ylabel=\"Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"data\",\n",
        "        )\n",
        "\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=model_data,\n",
        "            title=\"Model Data\",\n",
        "            ylabel=\"Model Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"model_data\",\n",
        "        )\n",
        "\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=residual_map,\n",
        "            title=\"Residual Map\",\n",
        "            ylabel=\"Residuals\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"residual_map\",\n",
        "        )\n",
        "\n",
        "        plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=chi_squared_map,\n",
        "            title=\"Chi-Squared Map\",\n",
        "            ylabel=\"Chi-Squareds\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"chi_squared_map\",\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data (Complex)__\n",
        "\n",
        "Load the dataset from the `autofit_workspace/dataset` folder. This uses a new `dataset` that is a sum of a \n",
        "`Gaussian` and `Exponential` profile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1__exponential_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "Lets now perform the fit using our model which is composed of two profile's. You'll note that the `Emcee`\n",
        "dimensionality has increased from N=3 to N=6, given that we are now fitting two `Profile`'s each with 3 free parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee(\n",
        "    path_prefix=path.join(\"howtofit\", \"chapter_1\"),\n",
        "    name=\"tutorial_5__gaussian_x1__exponential_x1\",\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running. \\n\"\n",
        "    \"Checkout the autofit_workspace/output/howtofit/tutorial_5__gaussian_x1__exponential_x1 \\n\"\n",
        "    \"folder for live output of the results.\\n\"\n",
        "    \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The `info` attribute shows the result in a readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect the results of the fit by going to the folder \n",
        "\n",
        "`autofit_workspace/output/howtofit/tutorial_5__gaussian_x1__exponential_x1`. The fit takes longer to run than \n",
        "the fits performed in previous tutorials, because the dimensionality of the model we fit increases from 3 to 6.\n",
        "\n",
        "__Triple Profile Fit__\n",
        "\n",
        "With the `Collection`, **PyAutoFit** provides all the tools needed to compose and fit any model imaginable!\n",
        "Lets fit a model composed of two `Gaussian`. and and an `Exponential`, which will have a dimensionality of N=9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x2__exponential_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "model = af.Collection(\n",
        "    gaussian_0=af.Model(p.Gaussian),\n",
        "    gaussian_1=af.Model(p.Gaussian),\n",
        "    exponential=af.Model(p.Exponential),\n",
        ")\n",
        "\n",
        "search = af.Emcee(\n",
        "    name=\"tutorial_5__gaussian_x2__exponential_x1\",\n",
        "    path_prefix=path.join(\"howtofit\", \"chapter_1\"),\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running.\\n\"\n",
        "    \"checkout the autofit_workspace/output/howtofit/tutorial_5__gaussian_x2__exponential_x1\\n\"\n",
        "    \" folder for live output of the results.\\n\"\n",
        "    \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Customization__\n",
        "\n",
        "We can fully customize the model that we fit. Lets suppose we have a dataset` that consists of three `Gaussian` \n",
        "profiles, but we also know the following information about the dataset:\n",
        "\n",
        "- All 3 `Gaussian`'s are centrally aligned.\n",
        "- The `sigma` of one `Gaussian` is equal to 1.0.\n",
        "- The sigma of another `Gaussian` is above 3.0.\n",
        "\n",
        "We can edit the `Model` components we pass into the `Collection` to meet these constraints accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0 = af.Model(p.Gaussian)\n",
        "gaussian_1 = af.Model(p.Gaussian)\n",
        "gaussian_2 = af.Model(p.Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This aligns the `centre`'s of the 3 `Gaussian`'s reducing the dimensionality of the model from N=9 to N=7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0.centre = gaussian_1.centre\n",
        "gaussian_1.centre = gaussian_2.centre"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This fixes the `sigma` value of one `Gaussian` to 1.0, further reducing the dimensionality from N=7 to N=6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0.sigma = 1.0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This assertion forces all values of the `sigma` value of the third `Gaussian` to  be above 3.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_2.add_assertion(gaussian_2.sigma > 3.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now input these model components into the `Collection`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(\n",
        "    gaussian_0=gaussian_0, gaussian_1=gaussian_1, gaussian_2=gaussian_2\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now fit this model as per usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x3\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee(\n",
        "    path_prefix=path.join(\"howtofit\", \"chapter_1\"), name=\"tutorial_5__gaussian_x3\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running. \"\n",
        "    \"Checkout the autofit_workspace/output/howtofit/tutorial_5__gaussian_x3\"\n",
        "    \" folder for live output of the results.\"\n",
        "    \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can again quickly inspect the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Cookbooks__\n",
        "\n",
        "This tutorial illustrates how to compose model out of multiple components, using a `Collection`.\n",
        "\n",
        "**PyAutoFit** has many advanced model composition tools, which offer more customization of `Collection` objects,\n",
        "allow models to be composed and fitted to multiple datasets and for multi-level models to be created out of\n",
        "hierarchies of Python classes.\n",
        "\n",
        "Checkout the `autofit_workspace/*/model` package for these cookbooks with give a full run through of all of\n",
        "**PyAutoFit**'s model composition tools, or read them on the readthedocs:\n",
        "\n",
        " - `cookbook 1: Basics  <https://pyautofit.readthedocs.io/en/latest/cookbooks/cookbook_1_basics.html>`_\n",
        "\n",
        " - `cookbook 2: Collections  <https://pyautofit.readthedocs.io/en/latest/cookbooks/cookbook_2_collections.html>`_\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "And with that, we are complete. In this tutorial, we learned how to compose and fit complex models in **PyAutoFit**.\n",
        " \n",
        "To end, you should think again in more detail about your model fitting problem:\n",
        "\n",
        " Are there many different model components you may wish to define and fit?\n",
        "\n",
        " Is your data the super position of many different model components, like the profiles in this tutorial?\n",
        "\n",
        " In this tutorial, all components of our model did the same thing, represent a 1D profile. In your model, you may\n",
        "have model components that represent different parts of your model, which need to be combined in more complicated ways\n",
        "in order to create your model-fit. You now have all the tools you need to define, compose and fit very complex models!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}