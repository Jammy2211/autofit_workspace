{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 4: Complex Models\n",
        "==========================\n",
        "\n",
        "In this tutorial, we will fix more complex models with N=10, N=20 and more parameters. We will consider the following:\n",
        "\n",
        " - Why more complex model are more difficult to fit, and may lead the non-linear search to incorrectly infer\n",
        "   models with significantly lower likelihoods than the true maximum likelihood model.\n",
        "\n",
        " - Strategies for ensuring the non-linear search correctly estimates the maximum likelihood model.\n",
        "\n",
        " - What drives the run-times of a model-fit, and how one must carefully balance run-times with model complexity.\n",
        "for mitigating this:\n",
        "\n",
        "WHAT I NEED TO WRITE:\n",
        "\n",
        "- Example which fits an N=15 model and gets an incorrect result, concepts like \"local maxima\", model complexity,\n",
        "using composition API to simplify model, etc, using priors to do this.\n",
        "\n",
        "- Sections on run times.\n",
        "\n",
        "- Sections on non-linear search settings.\n",
        "\n",
        "Can rewrite and borrow from HowToLens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "import os\n",
        "from os import path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "We first load the dataset we will fit, which is a new `dataset` where the underlying signal is a sum of two  `Gaussian` \n",
        "profiles which share the same centre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x2\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the data shows the noisy signal is more complicated than just a 1D Gaussian.\n",
        "\n",
        "Note that both Gaussians are centred at the same point (x = 50). We will compose a model that reflects this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = np.arange(data.shape[0])\n",
        "plt.errorbar(\n",
        "    xvalues, data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.title(\"1D Gaussian dataset with errors from the noise-map.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Signal Value\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Models__\n",
        "\n",
        "We create the `Gaussian` class which will form our model components using the standard **PyAutoFit** format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "        normalization=1.0,  # <- are the Gaussian`s model parameters.\n",
        "        sigma=5.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Gaussian profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to fit example datasets\n",
        "        via a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues: np.ndarray):\n",
        "        \"\"\"\n",
        "\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We now define the  `Analysis` class for this model-fit. \n",
        "\n",
        "The `log_likelihood_function` of this analysis now assumes that the `instance` that is input into it will contain\n",
        "multiple 1D profiles.\n",
        " \n",
        " The way the `model_data` is computed is updating accordingly (the sum of each individual Gaussian's `model_data`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        Returns the log likelihood of the fit of an `instance` containing many 1D\n",
        "        Profiles (e.g. Gaussians) to the dataset, using a model instance.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance\n",
        "            A list of 1D profiles with parameters set via the non-linear search.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The log likelihood value indicating how well this model fit the `MaskedDataset`.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        In the previous tutorial the instance was a single `Gaussian` profile, meaning we could create the model data \n",
        "        using the line:\n",
        "\n",
        "            model_data = instance.gaussian.model_data_1d_via_xvalues_from(xvalues=self.data.xvalues)\n",
        "\n",
        "        In this tutorial our instance is comprised of multiple 1D Gaussians, because we will use a `Collection` to\n",
        "        compose the model:\n",
        "\n",
        "            model = Collection(gaussian_0=Gaussian, gaussian_1=Gaussian).\n",
        "\n",
        "        By using a Collection, this means the instance parameter input into the fit function is a\n",
        "        dictionary where individual profiles (and their parameters) can be accessed as followed:\n",
        "\n",
        "            print(instance.gaussian_0)\n",
        "            print(instance.gaussian_1)\n",
        "            print(instance.gaussian_0.centre)\n",
        "\n",
        "        In this tutorial, the `model_data` is therefore the summed `model_data` of all individual Gaussians in the \n",
        "        model. The function `model_data_from_instance` performs this summation. \n",
        "        \"\"\"\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def model_data_from_instance(self, instance):\n",
        "        \"\"\"\n",
        "        To create the summed profile of all individual profiles, we use a list comprehension to iterate over\n",
        "        all profiles in the instance.\n",
        "\n",
        "        The key point to understand is that the `instance` has the properties of a Python `iterator` and therefore\n",
        "        can be looped over using the standard Python for syntax (e.g. `for profile in instance`).\n",
        "\n",
        "        __Alternative Syntax__\n",
        "\n",
        "        For those not familiar with list comprehensions, the code below shows how to use the instance to create the\n",
        "        summed profile using a more simple for loop.\n",
        "\n",
        "        model_data = np.zeros(shape=self.data.xvalues.shape[0])\n",
        "\n",
        "        for profile in instance:\n",
        "            model_data += profile.model_data_1d_via_xvalues_from(xvalues=self.data.xvalues)\n",
        "\n",
        "        return model_data\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        return sum(\n",
        "            [\n",
        "                profile.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "                for profile in instance\n",
        "            ]\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Collection__\n",
        "\n",
        "Use a `Collection` to compose the model we fit, consisting of two `Gaussian`'s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian_0=Gaussian, gaussian_1=Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Customization__\n",
        "\n",
        "We can fully customize the model that we fit. \n",
        "\n",
        "First, lets align the centres of the two `Gaussian`'s (given we know they are aligned in the data). Note that\n",
        "doing so reduces the number of free parameters in the model by 1, from N=6 to N=5.\n",
        "\n",
        "Lets suppose we have a `dataset` that consists of three `Gaussian` \n",
        "profiles, but we also know the following information about the dataset:\n",
        "\n",
        "- The 2 `Gaussian`'s are centrally aligned.\n",
        "- The `sigma` of one `Gaussian` is equal to 1.0.\n",
        "- The sigma of another `Gaussian` is above 3.0.\n",
        "\n",
        "We can edit the `Model` components we pass into the `Collection` to meet these constraints accordingly.\n",
        "\n",
        "Lets first create the model `Gaussian`'s as we did in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0 = af.Model(Gaussian)\n",
        "gaussian_1 = af.Model(Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can centrally align the two `Gaussian`'s by setting the `centre` of the first `Gaussian` to the `centre` of the\n",
        "second `Gaussian`.\n",
        "\n",
        "This removes a free parameter from the model reducing the dimensionality by 1 (from N=6 to N=5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0.centre = gaussian_1.centre"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can follow the same API to set the `sigma` of the first `Gaussian` to 1.0.\n",
        "\n",
        "This again removes another free parameter from the model (from N=5 to N=4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0.sigma = 1.0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can add assertions, for example requiring that  the `sigma` value of the second `Gaussian` is above 2.0.\n",
        "\n",
        "Assertions do not change the dimensionality of the model, because we are not fixing or removing any free parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_1.add_assertion(gaussian_1.sigma > 3.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We again input these newly customized model components into the `Collection`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(\n",
        "    gaussian_0=gaussian_0,\n",
        "    gaussian_1=gaussian_1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The customized model can be printed via the `info` attribute, where the customizes discussed above can be seen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "Lets now perform the fit using our model which is composed of two profile's in a non-linear parameter space of\n",
        "dimensionality N=4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search = af.Emcee()\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running. \\n\"\n",
        "    \"Checkout the autofit_workspace/output/howtofit/tutorial_5__gaussian_x1__exponential_x1 \\n\"\n",
        "    \"folder for live output of the results.\\n\"\n",
        "    \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The `info` attribute shows the result in a readable format, which contains informaiton on the full collection\n",
        "of model components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Cookbooks__\n",
        "\n",
        "This tutorial illustrates how to compose model out of multiple components, using a `Collection`.\n",
        "\n",
        "**PyAutoFit** has many advanced model composition tools, which offer more customization of `Collection` objects,\n",
        "allow models to be composed and fitted to multiple datasets and for multi-level models to be created out of\n",
        "hierarchies of Python classes.\n",
        "\n",
        "Checkout the `autofit_workspace/*/model` package for these cookbooks with give a full run through of all of\n",
        "**PyAutoFit**'s model composition tools, or read them on the readthedocs:\n",
        "\n",
        " - `cookbook 1: Basics  <https://pyautofit.readthedocs.io/en/latest/cookbooks/cookbook_1_basics.html>`_\n",
        "\n",
        " - `cookbook 2: Collections  <https://pyautofit.readthedocs.io/en/latest/cookbooks/cookbook_2_collections.html>`_\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "And with that, we are complete. In this tutorial, we learned how to compose and fit complex models in **PyAutoFit**.\n",
        " \n",
        "To end, you should think again in more detail about your model fitting problem:\n",
        "\n",
        " Are there many different model components you may wish to define and fit?\n",
        "\n",
        " Is your data the super position of many different model components, like the profiles in this tutorial?\n",
        "\n",
        " In this tutorial, all components of our model did the same thing, represent a 1D profile. In your model, you may\n",
        "have model components that represent different parts of your model, which need to be combined in more complicated ways\n",
        "in order to create your model-fit. You now have all the tools you need to define, compose and fit very complex models!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}