{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 3: Parameter Space And Priors\n",
        "======================================\n",
        "\n",
        "In the previous tutorial, we define fitting functions that allowed us to create model-data using realizations of a\n",
        "1D `Gaussian` model and fit it to the data. We achieved a good fit, but only by guessing values of parameters. In the\n",
        "next two tutorials we are going to learn how to fit a model to data properly, which means we first need to define\n",
        "concepts of a parameter space and priors.\n",
        "\n",
        "__Parameter Space__\n",
        "\n",
        "If mathematics, you will have learnt that we can write a simple function as follows:\n",
        "\n",
        "$f(x) = x^2$\n",
        "\n",
        "In this function, when we input the parameter $x`$ in to the function $f$, it returns a value $f(x)$. The mappings\n",
        "between values of $x$ and $f(x)$ define what we can call the parameter space of this function (and if you remember\n",
        "your math classes, the parameter space of the function $f(x) = x^2$ is a parabola).\n",
        "\n",
        "A function can of course have multiple parameters:\n",
        "\n",
        "$f(x, y, z) = x + y^2 - z^3$\n",
        "\n",
        "This function has 3 parameters, $x$, $y$ and $z$. The mappings between $x$, $y$ and $z$ and $f(x, y, z)$ define another\n",
        "parameter space, albeit this parameter space now has 3 dimensions. Nevertheless, just like we could plot a parabola to\n",
        "visualize the parameter space $f(x) = x^2$, we could visualize this parameter space as 3 dimensional surface.\n",
        "\n",
        "In the previous tutorial, we used realizations of the `Gaussian` class to fit data with a model so as to return a log\n",
        "likelihood.\n",
        "\n",
        "This process can be thought of as us computing a likelihood from a function, just like our functions $f(x)$ above.\n",
        "However, the log likelihood function is not something that we can write down analytically as an equation and its\n",
        "behaviour is inherently non-linear. Nevertheless, it is a function, and if we put the same values of model\n",
        "parameters into this function the same value of log likelihood will be returned.\n",
        "\n",
        "Therefore, we can write this log likelihood function as follows, where the parameters $(x, N, \\sigma)$ are again\n",
        "the parameters of our `Gaussian`:\n",
        "\n",
        "$f(x, N, \\sigma) = log_likelihood$\n",
        "\n",
        "By expressing the likelihood in this way we have defined a parameter space! The solutions to this function cannot be\n",
        "written analytically and it is highly complex and non-linear. However, we have already learnt how we can use this\n",
        "function to determine a log likelihood, by creating realizations of the Gaussian and comparing them to the data.\n",
        "\n",
        "__Priors__\n",
        "\n",
        "We are now thinking about our model and log likelihood function as a parameter space, which is crucial for\n",
        "understanding how we will fit the model to data in the next tutorial. Before we do that, we need to consider one more\n",
        "concept, how do we define where in parameter space we search for solutions? What values of model parameters do we\n",
        "consider viable solutions?\n",
        "\n",
        "A parameter, say, the `centre` of the `Gaussian`, could in principle take any value between negative and positive\n",
        "infinity. However, when we inspect the data it is clearly confined to values between 0.0 and 100.0, therefore we should\n",
        "define a parameter space that only contains these solutions as these are the only physically plausible values\n",
        "of `centre` (e.g. between 0.0 --> 100.0).\n",
        "\n",
        "These are called the 'priors'. Our priors define where parameter space has valid solutions, and throughout these\n",
        "tutorials we will use three types of prior:\n",
        "\n",
        "- UniformPrior: The permitted values of a parameter are between a `lower_limit` and `upper_limit` and we assign equal\n",
        "probability to all solutions between these limits. For example, the `centre` of the `Gaussian` will typically assume\n",
        "a uniform prior between 0.0 and 100.0.\n",
        "\n",
        "- LogUniformPrior: Like a `UniformPrior` this confines the values of a parameter between a `limit_limit`\n",
        "and `upper_limit`, but we assign the probability of solutions with a log distribution with base 10. For example,\n",
        "the `normalization` of the `Gaussian` will typically assume a log uniform prior between 1e-2 and 100.0.\n",
        "\n",
        "- GaussianPrior: The permitted values of a parameter whose probability is tied to a Gaussian distribution with\n",
        "a `mean` and width `sigma`. For example, the `sigma` of the `Gaussian` will typically assume Gaussian prior with mean\n",
        "10.0 and sigma 5.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import autofit as af\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Again, load and plot the dataset from the `autofit_workspace/dataset` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "xvalues = np.arange(data.shape[0])\n",
        "print(xvalues)\n",
        "\n",
        "plt.errorbar(\n",
        "    xvalues, data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.title(\"1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile Normalization\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Lets again define our 1D `Gaussian` model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre: float = 30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "        normalization: float = 1.0,  # <- are the Gaussian`s model parameters.3\n",
        "        sigma: float = 5.0,\n",
        "    ):\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate the normalization of the light profile on a line of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, using its centre.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Mapping via Priors__\n",
        "\n",
        "We can again use **PyAutoFit** to set the `Gaussian` as a model and map it to instances of the `Gaussian`, however\n",
        "we can now do this via priors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)\n",
        "print(\"Model `Gaussian` object: \\n\")\n",
        "print(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now set the prior for each parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.normalization = af.LogUniformPrior(lower_limit=0.1, upper_limit=10.0)\n",
        "model.sigma = af.GaussianPrior(mean=10.0, sigma=5.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The updated priors are reflected in the model's `info` attribute.\n",
        "\n",
        "[The `info` below may not display optimally on your computer screen, for example the whitespace between parameter\n",
        "names on the left and parameter priors on the right may lead them to appear across multiple lines. This is a\n",
        "common issue in Jupyter notebooks.\n",
        "\n",
        "The`info_whitespace_length` parameter in the file `config/general.yaml` in the [output] section can be changed to \n",
        "increase or decrease the amount of whitespace (The Jupyter notebook kernel will need to be reset for this change to \n",
        "appear in a notebook).]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a quick reminder, we have seen that using this `Model` we can create an `instance` of the model, by mapping a \n",
        "list of physical values of each parameter as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = model.instance_from_vector(vector=[1.0, 2.0, 3.0])\n",
        "print(\"Instance Parameters \\n\")\n",
        "print(\"x = \", instance.centre)\n",
        "print(\"normalization = \", instance.normalization)\n",
        "print(\"sigma = \", instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Priors are used to create model instances via a mapping analogous to the one above, but from a unit-vector of values.\n",
        "\n",
        "This vector is defined in the same way as the vector above but with values spanning from 0 -> 1, where the unit values \n",
        "are mapped to physical values via the prior, for example:\n",
        "\n",
        "For the `UniformPrior` defined between 0.0 and 100.0:\n",
        "\n",
        "- An input unit value of 0.5 will give the physical value 5.0.\n",
        "- An input unit value of 0.8 will give te physical value 80.0.\n",
        "\n",
        "For the `LogUniformPrior` (base 10) defined between 0.1 and 10.0:\n",
        "\n",
        "- An input unit value of 0.5 will give the physical value 1.0.\n",
        "- An input unit value of 1.0 will give te physical value 10.0.\n",
        "\n",
        "For a `GaussianPrior `defined with mean 10.0 and sigma 5.0:\n",
        "\n",
        "- An input unit value of 0.5 (e.g. the centre of the Gaussian) will give the physical value 10.0.\n",
        "- An input unit value of 0.8173 (e.g. 1 sigma confidence) will give te physical value 14.5256.\n",
        "\n",
        "Lets take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = model.instance_from_unit_vector(unit_vector=[0.5, 0.3, 0.8173])\n",
        "\n",
        "print(\"Model Instance: \\n\")\n",
        "print(instance)\n",
        "\n",
        "print(\"Instance Parameters \\n\")\n",
        "print(\"x = \", instance.centre)\n",
        "print(\"normalization = \", instance.normalization)\n",
        "print(\"sigma = \", instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__How Are Priors Actually Used?__\n",
        "\n",
        "Priors allow us to map unit vectors to physical parameters and therefore define a parameter space. However, the actual\n",
        "process of mapping unit-values to physical values in this way is pretty much all handled by **PyAutoFit** \n",
        "\"behind ths scenes\" and is not something you'll explicitly do yourself. Nevertheless, this is core concept of any\n",
        "model-fitting exercise and is why we have covered it in this tutorial. \n",
        "\n",
        "In the next tutorial, we'll see how this mapping between unit and physical values is built-in to the algorithms we \n",
        "use to perform model-fitting!\n",
        "\n",
        "__Limits__\n",
        "\n",
        "We can also set physical limits on parameters, such that a model instance cannot generate parameters outside of a\n",
        "specified range.\n",
        "\n",
        "For example, a `Gaussian` cannot have a negative normalization, so we can set its lower limit to a value of 0.0.\n",
        "\n",
        "This is what the `gaussian_limits` section in the priors config files sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.normalization = af.GaussianPrior(\n",
        "    mean=0.0, sigma=1.0, lower_limit=0.0, upper_limit=1000.0\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The unit vector input below creates a negative normalization value, such that if you uncomment the line \n",
        "below **PyAutoFit** raises an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# instance = model.instance_from_unit_vector(unit_vector=[0.01, 0.01, 0.01])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Prior Configs__\n",
        "\n",
        "For highly complex models with many parameters, it is cumbersome to have to manually define the prior on every \n",
        "parameter and would likely lead to input mistakes.\n",
        "\n",
        "**PyAutoFit** allows one to define all of the default prior values in a configuration file, such that they are loaded\n",
        "automatically. This means we do not need manually define the priors ourselves.\n",
        "\n",
        "To do this, we define the `Gaussian` class in a standalone Python \n",
        "module `autofit_workspace/*/howtofit/chapter_1_introduction/gaussian.py` (as opposed to this Python script). \n",
        "The name of this module is used to look for a file `gaussian.json` in the directory `autofit_workspace/config/priors` \n",
        "such that the default priors of the model are loaded from the file `gaussian.json`. \n",
        "\n",
        "For example, because our `Gaussian` is in the module `gaussian.py`, its priors are loaded from the priors config\n",
        "file `gaussian.json`. Check this file out now to see the default priors; we'll discuss what the different inputs\n",
        "mean later on.\n",
        "\n",
        "This is illustrated below, where we are using the `Gaussian` defined in `gaussian.py` and inspect its prior to see\n",
        "they have been automatically set up via the config file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gaussian as g\n",
        "\n",
        "model = af.Model(g.Gaussian)\n",
        "print(\"Model `Gaussian` object via priors configs: \\n\")\n",
        "print(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "In this tutorial, we introduce the notion of a parameter space and priors, which **PyAutoFit**'s model mapping \n",
        "utilities map between. We are now in a position to perform a model-fit, which will be the subject of the next tutorial.\n",
        " \n",
        "The description of priors in this tutorial was somewhat of a simplification; we viewed them as a means to map a \n",
        "unit values of parameters to physical values. In Bayesian inference, priors play a far more important role, as they\n",
        "define one's previous knowledge of the model before performing the fit. They directly impact the solution that one\n",
        "infers and ultimately dictate how the model-fitting is performed.\n",
        "\n",
        "The aim of the **HowToFit** tutorials is not to teach the reader the details of Bayesian inference but instead set\n",
        "you up with the tools necessary to perform a model-fit. Nevertheless, it is worth reading up on Bayesian inference and \n",
        "priors at any of the following links:\n",
        "\n",
        "https://seeing-theory.brown.edu/bayesian-inference/index.html\n",
        "\n",
        "https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}