{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 2: Model Fitting\n",
        "=========================\n",
        "\n",
        "In this tutorial, we'll fit the 1D `Gaussian` from the previous tutorial to the data containing that Gaussian, and\n",
        "learn how to quantify the goodness-of-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Load the dataset we will fit from the `autofit_workspace/dataset` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The shape of the data gives us its xvalues, the x coordinates we evaluate our model 1D `Gaussian` on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = np.arange(data.shape[0])\n",
        "print(xvalues)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets plot the data and noise-map we're going to fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.errorbar(\n",
        "    xvalues, data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.title(\"1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile Normalization\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Data__\n",
        "\n",
        "So, how do we actually go about fitting our `Gaussian` model to this data? First, we need to be able to generate\n",
        "an image of our 1D `Gaussian` model. \n",
        "\n",
        "As we did in tutorial 1, we define the `Gaussian` as a Python class with the format required for **PyAutoFit** to use \n",
        "it as a model-component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "        normalization=1.0,  # <- are the Gaussian`s model parameters.3\n",
        "        sigma=5.0,\n",
        "    ):\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate the normalization of the 1D Gaussian profile on uniform Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, using its centre.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've again extended the `Gaussian` class to have a method `model_data_1d_via_xvalues_from`. Given an input set of x coordinates\n",
        "this computes the normalization of the `Gaussian` at every point, which we again plot a realization of our Gaussian. \n",
        "Note how we are now referring to this as our `model_data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)\n",
        "\n",
        "gaussian = model.instance_from_vector(vector=[60.0, 20.0, 15.0])\n",
        "\n",
        "model_data = gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"1D Gaussian model.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile Normalization\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is often more informative to plot the `data` and `model_data` on the same plot for comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"Model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Residuals__\n",
        "\n",
        "Different values of `centre`, `normalization` and `sigma` change the `Gaussian``s appearance, have a go at editing some \n",
        "of the values input into `instance_from_vector()` above to see this behaviour.\n",
        "\n",
        "Lets recap. We've defined a model which is a 1D `Gaussian` and given a set of parameters for that model $(x, N, \\sigma)$ \n",
        "we can create `model_data` of the `Gaussian`. We have some data of a 1D `Gaussian` we want to fit this model with, \n",
        "so as to determine the values of $(x, N, \\sigma)$ from which it was created. So how do we do that?\n",
        "\n",
        "Simple, we take the image from our `data` and our `model_data` of the `Gaussian` and subtract the two to get a\n",
        "residual-map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "residual_map = data - model_data\n",
        "plt.plot(xvalues, residual_map, color=\"r\")\n",
        "plt.title(\"Residuals of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Normalized Residuals__\n",
        "\n",
        "Clearly, this model is not a good fit to the data -- which was to be expected as they looked nothing alike!\n",
        "\n",
        "Next, we want to quantify how good (or bad) the fit actually was, via some goodness-of-fit measure. This measure\n",
        "needs to account for noise in the data, after all if we fit a pixel badly simply because it was very noisy we want\n",
        "our goodness-of-fit to account for that.\n",
        "\n",
        "To account for noise, we take our `residual_map` and divide it by the `noise_map`, to get the normalized residual-map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "normalized_residual_map = residual_map / noise_map\n",
        "plt.plot(xvalues, normalized_residual_map, color=\"r\")\n",
        "plt.title(\"Normalized residuals of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Normalized Residuals\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Chi Squared__\n",
        "\n",
        "We're getting close to a goodness-of-fit measure, but there is still a problem, we have negative and positive values\n",
        "in the normalized residual-map. A value of -0.2 represents just as good of a fit as a value of 0.2, therefore we want \n",
        "them to both be the same value.\n",
        "\n",
        "Thus, we next define a chi-squared-map, which is the `normalized_residual_map` squared. This makes negative and\n",
        "positive values both positive and thus defines them on a common overall scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared_map = (normalized_residual_map) ** 2\n",
        "plt.plot(xvalues, chi_squared_map, color=\"r\")\n",
        "plt.title(\"Chi-Squareds of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Chi-Squareds\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great, even when looking at a chi-squared-map its clear that our model gives a rubbish fit to the data!\n",
        "\n",
        "Finally, we want to reduce all the information in our `chi_squared_map` into a single goodness-of-fit measure. To do\n",
        "this we define the `chi_squared`, which is the sum of all values on the chi-squared-map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared = np.sum(chi_squared_map)\n",
        "print(\"Chi-squared = \", chi_squared)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The lower our chi-squared, the fewer residuals in the fit between our model and the data and therefore the better our \n",
        "fit!\n",
        "\n",
        "__Likelihood__\n",
        "\n",
        "From the chi-squared we can then define our final goodness-of-fit measure, the `log_likelihood`, which is the\n",
        "chi-squared value multiplied by -0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_likelihood = -0.5 * chi_squared\n",
        "print(\"Log Likelihood = \", log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why is the log likelihood the chi-squared multiplied by -0.5? Lets not worry about. This is simply the standard definition of a\n",
        "log_likelihood in statistics (it relates to the specific case of the noise properties in our dataset being Gaussian). \n",
        "For now, just accept that this is  what a log likelihood is and if we want to fit a model to data our goal is to thus \n",
        "find the combination of model parameters that maximize the `log_likelihood`.\n",
        "\n",
        "There is a second quantity that enters the log likelihood, called the `noise_normalization`. This is the log sum of all\n",
        "noise values squared in our data. Ggiven the noise-map is fixed, the noise_normalization retains the same value for\n",
        "all models that we fit. Nevertheless, it is good practise to include it in the log likelihood. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, like the definition of a log likelihood, lets not worry about why a noise normalization is defined in this way \n",
        "or why its in our goodness-of-fit (it again relates to the noise properties of our dataset being Gaussian). Lets just \n",
        "accept for now that this is how it is in statistics.\n",
        "\n",
        "Thus, we now have the definition of a log likelihood that we'll use hereafter in all **PyAutoFit** tutorials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "print(\"Log Likelihood = \", log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Fitting Functions__\n",
        "\n",
        "If you are familiar with model-fitting, you'll have probably heard of terms like 'residuals', 'chi-squared' and\n",
        "'log_likelihood' before. These are the standard metrics by which a model-fit`s quality is quantified. They are used for\n",
        "model fitting in general, so not just when your data is 1D but when its a 2D image, 3D datacube or something else\n",
        "entirely!\n",
        "\n",
        "If you have not performed model fitting before and these terms are new to you, make sure you are clear on exactly what\n",
        "they all mean as they are at the core of all model-fitting performed in **PyAutoFit** (and statistical inference in\n",
        "general)!\n",
        "\n",
        "So to recap the previous tutorial and this one:\n",
        "    \n",
        " - We can define a model components in **PyAutoFit**, like our `Gaussian`, using Python classes that follow a certain \n",
        " format.\n",
        " \n",
        " - The model component can be set up as a `Model` and have its parameters mapped to an instance of the \n",
        " `Gaussian` class via `instance_from_vector` function.\n",
        " \n",
        " - We can use this model instance to create model-data of our `Gaussian` and compare it to data and quantify the\n",
        " goodness-of-fit via a log likelihood.\n",
        "\n",
        "Thus we have everything we need to fit our model to our data! So, how do we go about finding the best-fit model?\n",
        "That is, the model which maximizes the log likelihood.\n",
        "\n",
        "The most simple thing we can do is guess parameters, and when we guess parameters that give a good fit, guess another\n",
        "set of parameters near those values. We can then repeat this process, over and over, until we find a really good model!\n",
        "\n",
        "For our `Gaussian` this works pretty well, below I've fitted 5 different `Gaussian` models and ended up landing on\n",
        "the best-fit model (the model I used to create the dataset in the first place!).\n",
        "\n",
        "For convenience, I've create functions which compute the chi-squared-map and log likelihood of a model-fit, alongside a\n",
        "method to plot a profile, residual-map or chi-squared-map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def chi_squared_map_from_data_and_model_data(data, noise_map, model_data):\n",
        "    residual_map = data - model_data\n",
        "    normalized_residual_map = residual_map / noise_map\n",
        "\n",
        "    return (normalized_residual_map) ** 2\n",
        "\n",
        "\n",
        "def log_likelihood_from_data_and_model_data(data, noise_map, model_data):\n",
        "    chi_squared_map = chi_squared_map_from_data_and_model_data(\n",
        "        data=data, noise_map=noise_map, model_data=model_data\n",
        "    )\n",
        "    chi_squared = sum(chi_squared_map)\n",
        "    noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "    log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "    return log_likelihood\n",
        "\n",
        "\n",
        "def plot_profile_1d(xvalues, profile_1d, color=\"k\", errors=None, ylabel=None):\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=profile_1d,\n",
        "        yerr=errors,\n",
        "        color=color,\n",
        "        ecolor=\"k\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.title(\"Chi-Squared of model-data fit to 1D Gaussian data.\")\n",
        "    plt.xlabel(\"x values of profile\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()\n",
        "    plt.clf()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Guess 1__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gaussian = model.instance_from_vector(vector=[50.0, 10.0, 5.0])\n",
        "model_data = gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "chi_squared_map = chi_squared_map_from_data_and_model_data(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "plot_profile_1d(\n",
        "    xvalues=xvalues, profile_1d=chi_squared_map, color=\"r\", ylabel=\"Chi-Squareds\"\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from_data_and_model_data(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(\"Log Likelihood:\")\n",
        "print(log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Guess 2__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gaussian = model.instance_from_vector(vector=[50.0, 25.0, 5.0])\n",
        "model_data = gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "chi_squared_map = chi_squared_map_from_data_and_model_data(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "plot_profile_1d(\n",
        "    xvalues=xvalues, profile_1d=chi_squared_map, color=\"r\", ylabel=\"Chi-Squareds\"\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from_data_and_model_data(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(\"Log Likelihood:\")\n",
        "print(log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Guess 3__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gaussian = model.instance_from_vector(vector=[50.0, 25.0, 10.0])\n",
        "model_data = gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "chi_squared_map = chi_squared_map_from_data_and_model_data(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "plot_profile_1d(\n",
        "    xvalues=xvalues, profile_1d=chi_squared_map, color=\"r\", ylabel=\"Chi-Squareds\"\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from_data_and_model_data(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(\"Log Likelihood:\")\n",
        "print(log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "You can now perform model-fitting with **PyAutoFit**! All we have to do is guess lots of parameters, over and over and\n",
        "over again, until we hit a model with a high log_likelihood. Yay!\n",
        "\n",
        "Of course, you're probably thinking, is that really it? Should we really be guessing models to find the best-fit?\n",
        "\n",
        "Obviously, the answer is no. Imagine our model was more complex, that it had many more parameters than just 3.\n",
        "Our approach of guessing parameters won't work -- it could take days, maybe years, to find models with a high\n",
        "log likelihood, and how could you even be sure they ware the best-fit models? Maybe a set of parameters you never tried\n",
        "provide an even better fit?\n",
        "\n",
        "Of course, there is a much better way to perform model-fitting, and in the next tutorials we'll take you through how\n",
        "to do such fitting in **PyAutoFit**, by setting up our model with priors and performing a non-linear search.\n",
        "\n",
        "__Your Model__\n",
        "\n",
        "To end, its worth quickly again thinking about the model you ultimately want to fit with **PyAutoFit**. In this example,\n",
        "we extended the `Gaussian` class to contain the function we needed to generate an image of the `Gaussian` and thus\n",
        "generate the model-image we need to fit our data. For your model fitting problem can you do something similar?\n",
        "Or is your model-fitting task a bit more complicated than this? Maybe there are more model component you want to\n",
        "combine or there is an inter-dependency between models?\n",
        "\n",
        "**PyAutoFit** provides a lot of flexibility in how you use your model instances, so whatever your problem you should \n",
        "find that it is straight forward to find a solution. But, whatever you need to do at its core your modeling problem \n",
        "will break down into the tasks we did in this tutorial:\n",
        "\n",
        "- Use your model to create some model data.\n",
        "- Subtract it from the data to create residuals.\n",
        "- Use these residuals in conjunction with your noise-map to define a log likelihood.\n",
        "- Find the highest log likelihood models.\n",
        "\n",
        "So, get thinking about how these steps would be performed for your model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}