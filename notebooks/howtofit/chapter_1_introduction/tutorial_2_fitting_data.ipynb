{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 2: Fitting Data\n",
        "========================\n",
        "\n",
        "We have now learnt that a model is a set of equations, numerical processes and assumptions describing\n",
        "a physical system. We defined a couple of simple models made of 1D equations like a Gaussian, composed them as models\n",
        "in **PyAutoFit** using the `Model` and `Collection` objects, and used these models to create model data for different\n",
        "values of their parameters.\n",
        "\n",
        "For our model to inform us about a real physical system, we need to fit it to data. By fitting it to the data,\n",
        "we can determine whether the model provides a good or bad fit to the data. If it is a good fit, we will learn which\n",
        "parameter values best describe the data and therefore the physical system as a whole. If it is a bad fit, we will\n",
        "learn that our model is not representative of the physical system and therefore that we need to change it.\n",
        "\n",
        "The process of defining a model, fitting it to data and using it to learn about the system we are modeling is at the\n",
        "heart of model-fitting. One would typically repeat this process many times, making the model more complex to better\n",
        "fit more data, better describing the physical system we are interested in.\n",
        "\n",
        "In Astronomy, this is the process that was followed to learn about the distributions of stars in galaxies. Fitting\n",
        "high quality images of galaxies with ever more complex models, allowed astronomers to determine that the stars in\n",
        "galaxies are distributed in structures like disks, bars and bulges, and it taught them that stars appear differently\n",
        "in red and blue images due to their age.\n",
        "\n",
        "In this tutorial, we will learn how to fit the `model_data` created by a model to data, and we will in particular:\n",
        "\n",
        " - Load data of a 1D Gaussian signal which is the data we will fit.\n",
        "\n",
        " - Subtract the model data from the data to compute quantities like the residuals of the fit.\n",
        "\n",
        " - Quantify the goodness-of-fit of a model to the data quantitatively using a key quantity in model-fitting called the\n",
        "   `log_likelihood`.\n",
        "\n",
        "This will all be performed using the **PyAutoFit** API for model composition, which forms the basis of all model\n",
        "fitting performed by **PyAutoFit**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Our data is noisy 1D data containing a signal, where the underlying signal is generated using the equation of \n",
        "a 1D Gaussian, a 1D Exponential or a sum of multiple 1D profiles.\n",
        " \n",
        "We now load this data from .json files, where:\n",
        "\n",
        " - The `data` is a 1D numpy array of values corresponding to the observed signal.\n",
        " - The `noise_map` is a 1D numpy array of values corresponding to the estimate noise value in every data point.\n",
        " \n",
        "These datasets are created via the scripts `autofit_workspace/howtofit/simulators`, feel free to check them out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now plot the 1D signal via `matplotlib`.\n",
        "\n",
        "The 1D signal is observed on uniformly spaced `xvalues`, which are computed using the `arange` function \n",
        "and `data.shape[0]` method.\n",
        "\n",
        "These x values will be used again below, when we create model data from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = np.arange(data.shape[0])\n",
        "plt.plot(xvalues, data, color=\"k\")\n",
        "plt.title(\"1D Dataset Containing a Gaussian.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Signal Value\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot above only showed the signal, and did not show the noise estimated in every data point. \n",
        "\n",
        "We can plot the signal, including its `noise_map`, using the `matplotlib` `errorbar` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.errorbar(\n",
        "    xvalues, data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.title(\"1D Gaussian dataset with errors from the noise-map.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Signal Value\")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Data__\n",
        "\n",
        "How do we actually fit our `Gaussian` model to this data? First, we generate `model_data` of the 1D `Gaussian` model,\n",
        "following the same steps as the previous tutorial. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "        normalization=1.0,  # <- are the Gaussian`s model parameters.\n",
        "        sigma=5.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Gaussian profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to fit example datasets\n",
        "        via a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues: np.ndarray):\n",
        "        \"\"\"\n",
        "\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this model to create `model_data` of the `Gaussian` by passing it an input `xvalues` of the observed\n",
        "data.\n",
        "\n",
        "We do this below, and plot the resulting model-data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)\n",
        "\n",
        "gaussian = model.instance_from_vector(vector=[60.0, 20.0, 15.0])\n",
        "\n",
        "model_data = gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"1D Gaussian model.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile Normalization\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is often more informative to plot the `data` and `model_data` on the same plot for comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"Model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Different values of `centre`, `normalization` and `sigma` change the `Gaussian``s appearance. \n",
        "\n",
        "Have a go at editing some of the values input into `instance_from_vector()`, recomputing the `model_data` and\n",
        "plotting it above to see this behaviour.\n",
        "\n",
        "__Residuals__\n",
        "\n",
        "The comparison of the `data` and `model_data` above is informative, but it can be more useful to show the\n",
        "residuals, which are calculated as `data - model_data` in 1D:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "residual_map = data - model_data\n",
        "plt.plot(xvalues, residual_map, color=\"r\")\n",
        "plt.title(\"Residuals of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Are these residuals a good fit to the data? Without considering the noise in the data, we can't be sure.\n",
        "\n",
        "We can plot the residual-map with error-bars for the noise-map, which below shows that the model is a pretty bad fit,\n",
        "because many of the residuals are far away from 0 even after accounting for the noise in every data point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "residual_map = data - model_data\n",
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=residual_map,\n",
        "    yerr=noise_map,\n",
        "    color=\"r\",\n",
        "    ecolor=\"r\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        ")\n",
        "plt.title(\"Residuals of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Normalized Residuals__\n",
        "\n",
        "A different way to quantify and visualize how good (or bad) the fit is, is using the normalized residual-map (sometimes\n",
        "called the standardized residuals).\n",
        "\n",
        "This is defined as the residual-map divided by the noise-map. \n",
        "\n",
        "If you are familiar with the concept of `sigma` variancdes in statistics, the normalized residual-map is equivalent\n",
        "to the number of `sigma` the residual is from zero. For example, a normalized residual of 2.0 (which has confidence\n",
        "internals for 95%) means that the probability that the model under-estimates the data by that value is just 5.0%.\n",
        "\n",
        "The residual map with error bars and normalized residual map portray the same information, but the normalized\n",
        "residual map is better for visualization for problems with more than 1 dimension, as plotting the error bars in\n",
        "2D or more dimensions is not straight forward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "normalized_residual_map = residual_map / noise_map\n",
        "plt.plot(xvalues, normalized_residual_map, color=\"r\")\n",
        "plt.title(\"Normalized residuals of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Normalized Residuals\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Chi Squared__\n",
        "\n",
        "We now define the `chi_squared_map`, which is the `normalized_residual_map` squared, and will be used to compute the\n",
        "the final goodness of fit measure.\n",
        "\n",
        "The normalized residual map has both positive and negative values. When we square it, we therefore get only positive\n",
        "values. This means that a normalized residual of -0.2 and 0.2 both become 0.04, and therefore in the context of a\n",
        "`chi_squared` signify the same goodness-of-fit.\n",
        "\n",
        "Again, it is clear that the model gives a poor fit to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared_map = (normalized_residual_map) ** 2\n",
        "plt.plot(xvalues, chi_squared_map, color=\"r\")\n",
        "plt.title(\"Chi-Squared Map of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Chi-Squareds\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we now reduce all the information in our `chi_squared_map` into a single goodness-of-fit measure by defining the \n",
        "`chi_squared`: the sum of all values in the `chi_squared_map`.\n",
        "\n",
        "This is why having all positive and negative values in the normalized residual map become positive is important,\n",
        "as this summed measure would otherwise cancel out the positive and negative values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared = np.sum(chi_squared_map)\n",
        "print(\"Chi-squared = \", chi_squared)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The lower the chi-squared, the fewer residuals in the model's fit to the data and therefore the better our fit as\n",
        "a whole!\n",
        "\n",
        "__Noise Normalization__\n",
        "\n",
        "We now define a second quantity that will enter our final quantification of the goodness-of-fit, called the\n",
        "`noise_normalization`.\n",
        "\n",
        "This is the log sum of all noise values squared in our data. Given the noise-map is fixed, the `noise_normalization`\n",
        "retains the same value for all models that we fit, and therefore could be omitted. Nevertheless, its good practise\n",
        "to include it as it has an important meaning statistically.\n",
        "\n",
        "Lets not worry about what a `noise_normalization` actually means, because its not important for us to successfully\n",
        "get a model to fit a dataset. In a nutshell, it relates the noise in the dataset being drawn from a Gaussian\n",
        "distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Likelihood__\n",
        "\n",
        "From the `chi_squared` and `noise_normalization` we can define a final goodness-of-fit measure, the `log_likelihood`. \n",
        "\n",
        "This is the sum of the `chi_squared` and `noise_normalization` multiplied by -0.5. Why -0.5? Again, lets not worry\n",
        "about this for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "print(\"Log Likelihood = \", log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we stated that a lower `chi_squared` corresponds to a better model-data fit. \n",
        "\n",
        "When computing the `log_likelihood` we multiplied the `chi_squared` by -0.5. Therefore, a higher log likelihood\n",
        "corresponds to a better model fit, as one would hope!\n",
        "\n",
        "__Fitting Functions__\n",
        "\n",
        "If you are familiar with model-fitting, you'll have probably heard of terms like 'residuals', 'chi-squared' and\n",
        "'log_likelihood' before. These are the standard metrics by which a model-fit`s quality is quantified. They are used for\n",
        "model fitting in general, so not just when your data is 1D but when its a 2D image, 3D datacube or something else\n",
        "entirely!\n",
        "\n",
        "If you have not performed model fitting before and these terms are new to you, make sure you are clear on exactly what\n",
        "they all mean as they are at the core of all model-fitting performed in **PyAutoFit** (and statistical inference in\n",
        "general)!\n",
        "\n",
        "Lets recap everything we've learnt so far:\n",
        "    \n",
        " - We can define a model, like a 1D `Gaussian`, using Python classes that follow a certain format.\n",
        " \n",
        " - The model can be set up as a `Collection` and `Model`, having its parameters mapped to an instance of the\n",
        "   model class (e.g the `Gaussian`).  \n",
        "\n",
        " - Using this model instance, we can create model-data and compare it to data and quantify the goodness-of-fit via a \n",
        "   log likelihood.\n",
        "\n",
        "We now have everything we need to fit our model to our data! \n",
        "\n",
        "So, how do we go about finding the best-fit model? That is, what model which maximizes the log likelihood?\n",
        "\n",
        "The most simple thing we can do is guess parameters. When we guess parameters that give a good fit (e.g. a higher \n",
        "log likelihood), we then guess new parameters with values near those previous vlaues. We can repeat this process, \n",
        "over and over, until we find a really good model!\n",
        "\n",
        "For a 1D  `Gaussian` this works pretty well. Below, we fit 3 different `Gaussian` models and end up landing on\n",
        "the best-fit model (the model I used to create the dataset in the first place!).\n",
        "\n",
        "For convenience, I've create functions which compute the `log_likelihood` of a model-fit and plot the data and model\n",
        "data with errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def log_likelihood_from(data, noise_map, model_data):\n",
        "    residual_map = data - model_data\n",
        "    normalized_residual_map = residual_map / noise_map\n",
        "    chi_squared_map = (normalized_residual_map) ** 2\n",
        "    chi_squared = sum(chi_squared_map)\n",
        "    noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "    log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "    return log_likelihood\n",
        "\n",
        "\n",
        "def plot_model_fit(xvalues, data, noise_map, model_data, color=\"k\"):\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=data,\n",
        "        yerr=noise_map,\n",
        "        color=color,\n",
        "        ecolor=\"k\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.plot(xvalues, model_data, color=\"r\")\n",
        "    plt.title(\"Fit of model-data to data.\")\n",
        "    plt.xlabel(\"x values of profile\")\n",
        "    plt.ylabel(\"Profile Value\")\n",
        "    plt.show()\n",
        "    plt.clf()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Guess 1__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gaussian = model.instance_from_vector(vector=[50.0, 10.0, 5.0])\n",
        "model_data = gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "plot_model_fit(\n",
        "    xvalues=xvalues,\n",
        "    data=data,\n",
        "    noise_map=noise_map,\n",
        "    model_data=model_data,\n",
        "    color=\"r\",\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(f\"Log Likelihood: {log_likelihood}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Guess 2__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gaussian = model.instance_from_vector(vector=[50.0, 25.0, 5.0])\n",
        "model_data = gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "plot_model_fit(\n",
        "    xvalues=xvalues,\n",
        "    data=data,\n",
        "    noise_map=noise_map,\n",
        "    model_data=model_data,\n",
        "    color=\"r\",\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(f\"Log Likelihood: {log_likelihood}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Guess 3__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gaussian = model.instance_from_vector(vector=[50.0, 25.0, 10.0])\n",
        "model_data = gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "plot_model_fit(\n",
        "    xvalues=xvalues,\n",
        "    data=data,\n",
        "    noise_map=noise_map,\n",
        "    model_data=model_data,\n",
        "    color=\"r\",\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(f\"Log Likelihood: {log_likelihood}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Extensibility__\n",
        "\n",
        "Fitting models made of multiple components is straight forward. \n",
        "\n",
        "We again simply create the model via  the `Collection` object, use it to generate `model_data` and fit it to the \n",
        "data in order to compute the log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian_0=Gaussian, gaussian_1=Gaussian)\n",
        "\n",
        "instance = model.instance_from_vector(vector=[40.0, 0.2, 0.3, 60.0, 0.5, 1.0])\n",
        "\n",
        "model_data_0 = instance.gaussian_0.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "model_data_1 = instance.gaussian_1.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "\n",
        "model_data = model_data_0 + model_data_1"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot the data and model data below, showing that we get a bad fit (a low log likelihood) for this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_model_fit(\n",
        "    xvalues=xvalues,\n",
        "    data=data,\n",
        "    noise_map=noise_map,\n",
        "    model_data=model_data,\n",
        "    color=\"r\",\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(f\"Log Likelihood: {log_likelihood}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the model had just 3 parameters, it was feasible to guess values by eye and find a good fit. \n",
        "\n",
        "With six parameters, this approach becomes inefficient, and doing it with even more parameters would be impossible!\n",
        "\n",
        "In the next turorial, we will learn a more efficient and automated approach for fitting models to data.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "To end, have another quick think about the model you ultimately want to fit with **PyAutoFit**. What does the\n",
        "data look like? Is it one dimension? two dimensions? Can you easily define a model which generates realizations of\n",
        "this data? Can you picture what a residual map would look like and how you would infer a log likelihood from it?\n",
        "\n",
        "If not, don't worry about it for now, because you first need to learn how to fit a model to data using **PyAutoFit**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}