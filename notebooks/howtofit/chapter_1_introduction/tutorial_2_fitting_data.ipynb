{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 2: Fitting Data\n",
        "========================\n",
        "\n",
        "We've learned that a model consists of equations, numerical processes, and assumptions that describe a physical system.\n",
        "Using **PyAutoFit**, we defined simple 1D models like the Gaussian, composed them into models using `Model` and\n",
        "`Collection` objects, and generated model data by varying their parameters.\n",
        "\n",
        "To apply our model to real-world situations, we must fit it to data. Fitting involves assessing how well the model\n",
        "matches observed data. A good fit indicates that the model's parameter values accurately describe the physical system.\n",
        "Conversely, a poor fit suggests that adjustments are needed to better reflect reality.\n",
        "\n",
        "Model-fitting is a cyclical process: define the model, fit it to data, and refine the model based on insights gained.\n",
        "Iteratively improving the model's complexity enhances its ability to accurately represent the system under study.\n",
        "This iterative process lies at the core of model-fitting in scientific analysis.\n",
        "\n",
        "__Astronomy Example__\n",
        "\n",
        "In Astronomy, this process has been crucial for understanding the distributions of stars within galaxies. By\n",
        "fitting high-quality images of galaxies with increasingly sophisticated models, astronomers have determined that\n",
        "stars within galaxies are organized into structures such as disks, bars, and bulges. This approach has also revealed\n",
        "that stars appear differently in red and blue images due to variations in their age and composition.\n",
        "\n",
        "__Overview__\n",
        "\n",
        "In this tutorial, we will explore how to fit the `model_data` generated by a model to actual data. Specifically, we will:\n",
        "\n",
        "- Load data representing a 1D Gaussian signal, which serves as our target dataset for fitting.\n",
        "\n",
        "- Compute quantities such as residuals by subtracting the model data from the observed data.\n",
        "\n",
        "- Quantitatively assess the goodness-of-fit using a critical measure in model-fitting known as the `log_likelihood`.\n",
        "\n",
        "All these steps will utilize the **PyAutoFit** API for model composition, introduced in the previous tutorial.\n",
        "\n",
        "__Contents__\n",
        "\n",
        "This tutorial is split into the following sections:\n",
        "\n",
        "- **Data**: Load and plot the 1D Gaussian dataset we will fit.\n",
        "- **Model Data**: Generate model data of the `Gaussian` model using a forward model.\n",
        "- **Residuals**: Compute and visualize residuals between the model data and observed data.\n",
        "- **Normalized Residuals**: Compute and visualize normalized residuals, which account for the noise properties of the data.\n",
        "- **Chi Squared**: Compute and visualize the chi-squared map, a measure of the overall goodness-of-fit.\n",
        "- **Noise Normalization**: Compute the noise normalization term which describes the noise properties of the data.\n",
        "- **Likelihood**: Compute the log likelihood, a key measure of the goodness-of-fit of the model to the data.\n",
        "- **Fitting Models**: Fit the `Gaussian` model to the 1D data and compute the log likelihood, by guessing parameters.\n",
        "- **Extensibility**: Use the `Collection` object for fitting models with multiple components.\n",
        "- **Wrap Up**: Summarize the key concepts of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Our dataset consists of noisy 1D data containing a signal, where the underlying signal can be modeled using \n",
        "equations such as a 1D Gaussian, a 1D Exponential, or a combination of multiple 1D profiles.\n",
        "\n",
        "We load this dataset from .json files, where:\n",
        "\n",
        "- `data` is a 1D NumPy array containing values representing the observed signal.\n",
        "\n",
        "- `noise_map` is a 1D NumPy array containing values representing the estimated root mean squared (RMS) noise level at \n",
        "  each data point.\n",
        "\n",
        "These datasets are generated using scripts located in `autofit_workspace/howtofit/simulators`. Feel free to explore \n",
        "these scripts for more details!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we visualize the 1D signal using `matplotlib`.\n",
        "\n",
        "The signal is observed over uniformly spaced `xvalues`, computed using the `arange` function and `data.shape[0]` method.\n",
        "\n",
        "We will reuse these `xvalues` shortly when generating model data from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = np.arange(data.shape[0])\n",
        "plt.plot(xvalues, data, color=\"k\")\n",
        "plt.title(\"1D Dataset Containing a Gaussian.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Signal Value\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The earlier plot depicted only the signal without indicating the estimated noise at each data point.\n",
        "\n",
        "To visualize both the signal and its `noise_map`, we can use `matplotlib`'s `errorbar` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.errorbar(\n",
        "    xvalues,\n",
        "    data,\n",
        "    yerr=noise_map,\n",
        "    linestyle=\"\",\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        ")\n",
        "plt.title(\"1D Gaussian dataset with errors from the noise-map.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Signal Value\")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Data__\n",
        "\n",
        "To fit our `Gaussian` model to this data, we start by generating `model_data` from the 1D `Gaussian` model, \n",
        "following the same steps as outlined in the previous tutorial.\n",
        "\n",
        "We begin by again defining the `Gaussian` class, following the **PyAutoFit** format for model components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre: float = 30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "        normalization: float = 1.0,  # <- are the Gaussian`s model parameters.\n",
        "        sigma: float = 5.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D Gaussian profile.\n",
        "\n",
        "        This is a model-component of example models in the **HowToFit** lectures and is used to perform model-fitting\n",
        "        of example datasets.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_from(self, xvalues: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns a 1D Gaussian on an input list of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, via its `centre`.\n",
        "\n",
        "        The output is referred to as the `model_data` to signify that it is a representation of the data from the\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.array\n",
        "            The Gaussian values at the input x coordinates.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To create `model_data` for the `Gaussian`, we use the model by providing it with `xvalues` corresponding to the \n",
        "observed data, as demonstrated in the previous tutorial.\n",
        "\n",
        "The following code essentially utilizes a forward model to generate the model data based on a specified set of \n",
        "parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)\n",
        "\n",
        "gaussian = model.instance_from_vector(vector=[60.0, 20.0, 15.0])\n",
        "\n",
        "model_data = gaussian.model_data_from(xvalues=xvalues)\n",
        "\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"1D Gaussian model.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile Normalization\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For comparison purposes, it is more informative to plot both the `data` and `model_data` on the same plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=data,\n",
        "    yerr=noise_map,\n",
        "    linestyle=\"\",\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"Model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Changing the values of `centre`, `normalization`, and `sigma` alters the appearance of the `Gaussian`.\n",
        "\n",
        "You can modify the parameters passed into `instance_from_vector()` above. After recomputing the `model_data`, plot \n",
        "it again to observe how these changes affect the Gaussian's appearance.\n",
        "\n",
        "__Residuals__\n",
        "\n",
        "While it's informative to compare the `data` and `model_data` above, gaining insights from the residuals can be even \n",
        "more useful. \n",
        "\n",
        "Residuals are calculated as `data - model_data` in 1D:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "residual_map = data - model_data\n",
        "plt.plot(xvalues, residual_map, color=\"k\")\n",
        "plt.title(\"Residuals of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Are these residuals indicative of a good fit to the data? Without considering the noise in the data, it's difficult \n",
        "to ascertain.\n",
        "\n",
        "We can plot the residuals with error bars based on the noise map. The plot below reveals that the model is a poor fit, \n",
        "as many residuals deviate significantly from zero even after accounting for the noise in each data point.\n",
        "\n",
        "A blue line through zero is included on the plot, to make it clear where residuals are not constent with zero\n",
        "above the noise level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "residual_map = data - model_data\n",
        "plt.plot(range(data.shape[0]), np.zeros(data.shape[0]), \"--\", color=\"b\")\n",
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=residual_map,\n",
        "    yerr=noise_map,\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        "    linestyle=\"\",\n",
        ")\n",
        "plt.title(\"Residuals of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Normalized Residuals__\n",
        "\n",
        "Another method to quantify and visualize the quality of the fit is using the normalized residual map, also known as \n",
        "standardized residuals.\n",
        "\n",
        "The normalized residual map is computed as the residual map divided by the noise map:\n",
        "\n",
        "\\[ \\text{normalized\\_residual} = \\frac{\\text{residual\\_map}}{\\text{noise\\_map}} = \\frac{\\text{data} - \\text{model\\_data}}{\\text{noise\\_map}} \\]\n",
        "\n",
        "If you're familiar with the concept of standard deviations (sigma) in statistics, the normalized residual map represents \n",
        "how many standard deviations the residual is from zero. For instance, a normalized residual of 2.0 (corresponding \n",
        "to a 95% confidence interval) means that the probability of the model underestimating the data by that amount is only 5%.\n",
        "\n",
        "Both the residual map with error bars and the normalized residual map convey the same information. However, \n",
        "the normalized residual map is particularly useful for visualization in multidimensional problems, as plotting \n",
        "error bars in 2D or higher dimensions is not straightforward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "normalized_residual_map = residual_map / noise_map\n",
        "plt.plot(xvalues, normalized_residual_map, color=\"k\")\n",
        "plt.title(\"Normalized residuals of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Normalized Residuals\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Chi Squared__\n",
        "\n",
        "Next, we define the `chi_squared_map`, which is obtained by squaring the `normalized_residual_map` and serves as a \n",
        "measure of goodness of fit.\n",
        "\n",
        "The chi-squared map is calculated as:\n",
        "\n",
        "\\[ \\chi^2 = \\left(\\frac{\\text{data} - \\text{model\\_data}}{\\text{noise\\_map}}\\right)^2 \\]\n",
        "\n",
        "The purpose of squaring the normalized residual map is to ensure all values are positive. For instance, both a \n",
        "normalized residual of -0.2 and 0.2 would square to 0.04, indicating the same level of fit in terms of `chi_squared`.\n",
        "\n",
        "As seen from the chi-squared map, it's evident that the model does not provide a good fit to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared_map = (normalized_residual_map) ** 2\n",
        "plt.plot(xvalues, chi_squared_map, color=\"k\")\n",
        "plt.title(\"Chi-Squared Map of model-data fit to 1D Gaussian data.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Chi-Squareds\")\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we consolidate all the information in our `chi_squared_map` into a single measure of goodness-of-fit \n",
        "called `chi_squared`. \n",
        "\n",
        "It is defined as the sum of all values in the `chi_squared_map` and is computed as:\n",
        "\n",
        "\\[ \\chi^2 = \\sum \\left(\\frac{\\text{data} - \\text{model\\_data}}{\\text{noise\\_map}}\\right)^2 \\]\n",
        "\n",
        "This summing process highlights why ensuring all values in the chi-squared map are positive is crucial. If we \n",
        "didn't square the values (making them positive), positive and negative residuals would cancel each other out, \n",
        "leading to an inaccurate assessment of the model's fit to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared = np.sum(chi_squared_map)\n",
        "print(\"Chi-squared = \", chi_squared)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The lower the `chi_squared`, the fewer residuals exist between the model's fit and the data, indicating a better \n",
        "overall fit!\n",
        "\n",
        "__Noise Normalization__\n",
        "\n",
        "Next, we introduce another quantity that contributes to our final assessment of the goodness-of-fit: \n",
        "the `noise_normalization`.\n",
        "\n",
        "The `noise_normalization` is computed as the logarithm of the sum of squared noise values in our data: \n",
        "\n",
        "\\[\n",
        "\\text{{noise\\_normalization}} = \\sum \\log(2 \\pi \\text{{noise\\_map}}^2)\n",
        "\\]\n",
        "\n",
        "This quantity is fixed because the noise-map remains constant throughout the fitting process. Despite this, \n",
        "including the `noise_normalization` is considered good practice due to its statistical significance.\n",
        "\n",
        "Understanding the exact meaning of `noise_normalization` isn't critical for our primary goal of successfully \n",
        "fitting a model to a dataset. Essentially, it provides a measure of how well the noise properties of our data align \n",
        "with a Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Likelihood__\n",
        "\n",
        "From the `chi_squared` and `noise_normalization`, we can define a final goodness-of-fit measure known as \n",
        "the `log_likelihood`. \n",
        "\n",
        "This measure is calculated by taking the sum of the `chi_squared` and `noise_normalization`, and then multiplying the \n",
        "result by -0.5:\n",
        "\n",
        "\\[ \\text{log\\_likelihood} = -0.5 \\times \\left( \\chi^2 + \\text{noise\\_normalization} \\right) \\]\n",
        "\n",
        "Why multiply by -0.5? The exact rationale behind this factor isn't critical for our current understanding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "print(\"Log Likelihood = \", log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Certainly! Here's a clearer version:\n",
        "\n",
        "Above, we mentioned that a lower `chi_squared` indicates a better fit of the model to the data. \n",
        "\n",
        "When calculating the `log_likelihood`, we multiply the `chi_squared` by -0.5. Therefore, a higher log likelihood \n",
        "corresponds to a better model fit. This is what we aim for when fitting models to data, we want to maximize the\n",
        "log likelihood!\n",
        "\n",
        "__Recap__\n",
        "\n",
        "If you're familiar with model-fitting, you've likely encountered terms like 'residuals', 'chi-squared', \n",
        "and 'log_likelihood' before. \n",
        "\n",
        "These metrics are standard ways to quantify the quality of a model fit. They are applicable not only to 1D data but \n",
        "also to more complex data structures like 2D images, 3D data cubes, or any other multidimensional datasets.\n",
        "\n",
        "If these terms are new to you, it's important to understand their meanings as they form the basis of all \n",
        "model-fitting operations in **PyAutoFit** (and in statistical inference more broadly).\n",
        "\n",
        "Let's recap what we've learned so far:\n",
        "\n",
        "- We can define models, such as a 1D `Gaussian`, using Python classes that follow a specific format.\n",
        "  \n",
        "- Models can be organized using `Collection` and `Model` objects, with parameters mapped to instances of their \n",
        "  respective model classes (e.g., `Gaussian`).\n",
        "  \n",
        "- Using these model instances, we can generate model data, compare it to observed data, and quantify the \n",
        "  goodness-of-fit using the log likelihood.\n",
        "\n",
        "__Fitting Models__\n",
        "\n",
        "Now, armed with this knowledge, we are ready to fit our model to our data!\n",
        "\n",
        "But how do we find the best-fit model, which maximizes the log likelihood?\n",
        "\n",
        "The simplest approach is to guess parameters. Starting with initial parameter values that yield a good \n",
        "fit (i.e., a higher log likelihood), we iteratively adjust these values to refine our model until we achieve an \n",
        "optimal fit.\n",
        "\n",
        "For a 1D `Gaussian`, this iterative process works effectively. Below, we fit three different `Gaussian` models and \n",
        "identify the best-fit model\u2014the one that matches the original dataset most closely.\n",
        "\n",
        "To streamline this process, I've developed functions that compute the `log_likelihood` of a model fit and visualize \n",
        "the data alongside the model predictions, complete with error bars."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def log_likelihood_from(\n",
        "    data: np.ndarray, noise_map: np.ndarray, model_data: np.ndarray\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Compute the log likelihood of a model fit to data given the noise map.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data\n",
        "        The observed data.\n",
        "    noise_map\n",
        "        The root mean square noise (or uncertainty) associated with each data point.\n",
        "    model_data\n",
        "        The model's predicted data for the given data x points.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The log likelihood of the model fit to the data.\n",
        "    \"\"\"\n",
        "    # Calculate residuals and normalized residuals\n",
        "    residual_map = data - model_data\n",
        "    normalized_residual_map = residual_map / noise_map\n",
        "\n",
        "    # Compute chi-squared and noise normalization\n",
        "    chi_squared_map = normalized_residual_map**2\n",
        "    chi_squared = np.sum(chi_squared_map)\n",
        "    noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "\n",
        "    # Compute log likelihood\n",
        "    log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "    return log_likelihood\n",
        "\n",
        "\n",
        "def plot_model_fit(\n",
        "    xvalues: np.ndarray,\n",
        "    data: np.ndarray,\n",
        "    noise_map: np.ndarray,\n",
        "    model_data: np.ndarray,\n",
        "    color: str = \"k\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot the observed data, model predictions, and error bars.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    xvalues\n",
        "        The x-axis values where the data is observed and model is predicted.\n",
        "    data\n",
        "        The observed data points.\n",
        "    noise_map\n",
        "        The root mean squared noise (or uncertainty) associated with each data point.\n",
        "    model_data\n",
        "        The model's predicted data for the given data x points.\n",
        "    color\n",
        "        The color for plotting (default is \"k\" for black).\n",
        "    \"\"\"\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=data,\n",
        "        yerr=noise_map,\n",
        "        linestyle=\"\",\n",
        "        color=color,\n",
        "        ecolor=\"k\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.plot(xvalues, model_data, color=\"r\")\n",
        "    plt.title(\"Fit of model-data to data\")\n",
        "    plt.xlabel(\"x values of profile\")\n",
        "    plt.ylabel(\"Profile Value\")\n",
        "    plt.show()\n",
        "    plt.clf()  # Clear figure to prevent overlapping plots\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Guess 1__\n",
        "\n",
        "The first guess correctly pinpoints that the Gaussian's peak is at 50.0, but the width and normalization are off.\n",
        "\n",
        "The `log_likelihood` is computed and printed, however because we don't have a value to compare it to yet, its hard\n",
        "to assess if it is a large or small value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gaussian = model.instance_from_vector(vector=[50.0, 10.0, 5.0])\n",
        "model_data = gaussian.model_data_from(xvalues=xvalues)\n",
        "plot_model_fit(\n",
        "    xvalues=xvalues,\n",
        "    data=data,\n",
        "    noise_map=noise_map,\n",
        "    model_data=model_data,\n",
        "    color=\"r\",\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(f\"Log Likelihood: {log_likelihood}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Guess 2__\n",
        "\n",
        "The second guess refines the width and normalization, but the size of the Gaussian is still off.\n",
        "\n",
        "The `log_likelihood` is computed and printed, and increases a lot compared to the previous guess, indicating that\n",
        "the fit is better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gaussian = model.instance_from_vector(vector=[50.0, 25.0, 5.0])\n",
        "model_data = gaussian.model_data_from(xvalues=xvalues)\n",
        "plot_model_fit(\n",
        "    xvalues=xvalues,\n",
        "    data=data,\n",
        "    noise_map=noise_map,\n",
        "    model_data=model_data,\n",
        "    color=\"r\",\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(f\"Log Likelihood: {log_likelihood}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Guess 3__\n",
        "\n",
        "The third guess provides a good fit to the data, with the Gaussian's peak, width, and normalization all accurately\n",
        "representing the observed signal.\n",
        "\n",
        "The `log_likelihood` is computed and printed, and is the highest value yet, indicating that this model provides the\n",
        "best fit to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "gaussian = model.instance_from_vector(vector=[50.0, 25.0, 10.0])\n",
        "model_data = gaussian.model_data_from(xvalues=xvalues)\n",
        "plot_model_fit(\n",
        "    xvalues=xvalues,\n",
        "    data=data,\n",
        "    noise_map=noise_map,\n",
        "    model_data=model_data,\n",
        "    color=\"r\",\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(f\"Log Likelihood: {log_likelihood}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Extensibility__\n",
        "\n",
        "Fitting models composed of multiple components is straightforward with PyAutoFit. Using the `Collection` object, \n",
        "we can define complex models consisting of several components. Once defined, we generate `model_data` \n",
        "from this collection and fit it to the observed data to compute the log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian_0=Gaussian, gaussian_1=Gaussian)\n",
        "\n",
        "instance = model.instance_from_vector(vector=[40.0, 0.2, 0.3, 60.0, 0.5, 1.0])\n",
        "\n",
        "model_data_0 = instance.gaussian_0.model_data_from(xvalues=xvalues)\n",
        "model_data_1 = instance.gaussian_1.model_data_from(xvalues=xvalues)\n",
        "\n",
        "model_data = model_data_0 + model_data_1"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot the data and model data below, showing that we get a bad fit (a low log likelihood) for this model.\n",
        "\n",
        "We could attempt to improve the model-fit and find a higher log likelihood solution by varying the parameters of\n",
        "the two Gaussians. However, with 6 parameters, this would be a challenging and cumbersome task to perform by eye."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_model_fit(\n",
        "    xvalues=xvalues,\n",
        "    data=data,\n",
        "    noise_map=noise_map,\n",
        "    model_data=model_data,\n",
        "    color=\"r\",\n",
        ")\n",
        "\n",
        "log_likelihood = log_likelihood_from(\n",
        "    data=data, noise_map=noise_map, model_data=model_data\n",
        ")\n",
        "print(f\"Log Likelihood: {log_likelihood}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When our model consisted of only 3 parameters, it was manageable to visually guess their values and achieve a good \n",
        "fit to the data. However, as we expanded our model to include six parameters, this approach quickly became \n",
        "inefficient. Attempting to manually optimize models with even more parameters would effectively become impossible,\n",
        "and a more systematic approach is required.\n",
        "\n",
        "In the next tutorial, we will introduce an automated approach for fitting models to data. This method will enable \n",
        "us to systematically determine the optimal values of model parameters that best describe the observed data, without \n",
        "relying on manual guesswork.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "To conclude, take a moment to reflect on the model you ultimately aim to fit using **PyAutoFit**. What does your \n",
        "data look like? Is it one-dimensional, like a spectrum or a time series? Or is it two-dimensional, such as an image \n",
        "or a map? Visualize the nature of your data and consider whether you can define a mathematical model that \n",
        "accurately generates similar data.\n",
        "\n",
        "Can you imagine what a residual map would look like if you were to compare your model's predictions against this \n",
        "data? A residual map shows the differences between observed data and the model's predictions, often revealing \n",
        "patterns or areas where the model fits well or poorly.\n",
        "\n",
        "Furthermore, can you foresee how you would calculate a log likelihood from this residual map? The log likelihood q\n",
        "uantifies how well your model fits the data, incorporating both the residual values and the noise characteristics of \n",
        "your observations.\n",
        "\n",
        "If you find it challenging to visualize these aspects right now, that's perfectly fine. The first step is to \n",
        "grasp the fundamentals of fitting a model to data using **PyAutoFit**, which will provide you with the tools \n",
        "and understanding needed to address these questions effectively in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}