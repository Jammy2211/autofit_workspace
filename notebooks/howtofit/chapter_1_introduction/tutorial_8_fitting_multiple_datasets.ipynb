{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 8: Fitting Multiple Datasets\n",
        "=====================================\n",
        "\n",
        "In this tutorial, we'll fit multiple dataset's with the same `NonLinearSearch`, producing multiple sets of results on\n",
        "our hard-disc. In the following tutorials, we then use these results and the `Aggregator` to load the results into\n",
        "our Jupyter notebook to interpret, inspect and plot the output results.\n",
        "\n",
        "we'll fit 3 different dataset's, each with a single `Gaussian` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "import os\n",
        "from os import path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll reuse the `plot_line` and `Analysis` classes of the previous tutorial.\n",
        "\n",
        "Note that the `Analysis` class has a new method, `save_attributes_for_aggregator`. This method specifies which properties of the\n",
        "fit are output to hard-disc so that we can load them using the `Aggregator` in the next tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def plot_line(\n",
        "    xvalues,\n",
        "    line,\n",
        "    title=None,\n",
        "    ylabel=None,\n",
        "    errors=None,\n",
        "    color=\"k\",\n",
        "    output_path=None,\n",
        "    output_filename=None,\n",
        "):\n",
        "    plt.errorbar(\n",
        "        x=xvalues, y=line, yerr=errors, color=color, ecolor=\"k\", elinewidth=1, capsize=2\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x value of profile\")\n",
        "    plt.ylabel(ylabel)\n",
        "    if not path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    plt.savefig(path.join(output_path, f\"{output_filename}.png\"))\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def model_data_from_instance(self, instance):\n",
        "        \"\"\"\n",
        "        To create the summed profile of all individual profiles in an instance, we can use a dictionary comprehension\n",
        "        to iterate over all profiles in the instance.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        return sum(\n",
        "            [profile.profile_from_xvalues(xvalues=xvalues) for profile in instance]\n",
        "        )\n",
        "\n",
        "    def visualize(self, paths, instance, during_analysis):\n",
        "        \"\"\"\n",
        "        This method is identical to the previous tutorial, except it now uses the `model_data_from_instance` method\n",
        "        to create the profile.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "\n",
        "        \"\"\"The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\"\"\"\n",
        "        plot_line(\n",
        "            xvalues=xvalues,\n",
        "            line=self.data,\n",
        "            title=\"Data\",\n",
        "            ylabel=\"Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"data\",\n",
        "        )\n",
        "\n",
        "        plot_line(\n",
        "            xvalues=xvalues,\n",
        "            line=model_data,\n",
        "            title=\"Model Data\",\n",
        "            ylabel=\"Model Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"model_data\",\n",
        "        )\n",
        "\n",
        "        plot_line(\n",
        "            xvalues=xvalues,\n",
        "            line=residual_map,\n",
        "            title=\"Residual Map\",\n",
        "            ylabel=\"Residuals\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"residual_map\",\n",
        "        )\n",
        "\n",
        "        plot_line(\n",
        "            xvalues=xvalues,\n",
        "            line=chi_squared_map,\n",
        "            title=\"Chi-Squared Map\",\n",
        "            ylabel=\"Chi-Squareds\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"chi_squared_map\",\n",
        "        )\n",
        "\n",
        "    def save_attributes_for_aggregator(self, paths):\n",
        "        \"\"\"Save files like the data and noise-map as pickle files so they can be loaded in the `Aggregator`\"\"\"\n",
        "        # These functions save the objects we will later access using the aggregator. They are saved via the `pickle`\n",
        "        # module in Python, which serializes the data on to the hard-disk.\n",
        "\n",
        "        with open(path.join(f\"{paths.pickle_path}\", \"data.pickle\"), \"wb\") as f:\n",
        "            pickle.dump(self.data, f)\n",
        "\n",
        "        with open(path.join(f\"{paths.pickle_path}\", \"noise_map.pickle\"), \"wb\") as f:\n",
        "            pickle.dump(self.noise_map, f)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also fit the same model as the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import profiles as p\n",
        "\n",
        "model = af.CollectionPriorModel(gaussian=p.Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, for each dataset we are going to set up the correct path, load it, and fit it using a `NonLinearSearch`.\n",
        "\n",
        "We want our results to be in a folder specific to the dataset. we'll use the dataset`'s name string to do this. Lets\n",
        "create a list of all 3 of our dataset names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_names = [\"gaussian_x1_0\", \"gaussian_x1_1\", \"gaussian_x1_2\"]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also attach information to the model-fit, by setting up an info dictionary. \n",
        "\n",
        "Information about our model-fit (e.g. the data of observation) that isn't part of the model-fit is made accessible to \n",
        "the `Aggregator`. For example, below we write info on the dataset's data of observation and exposure time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "info = {\"date_of_observation\": \"01-02-18\", \"exposure_time\": 1000.0}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This for loop runs over every dataset, checkout the comments below for how we set up the path structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name in dataset_names:\n",
        "\n",
        "    \"\"\"Load the dataset from the `autofit_workspace/dataset` folder.\"\"\"\n",
        "    dataset_path = path.join(\"dataset\", \"example_1d\", dataset_name)\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    Here, we create the `Emcee` `NonLinear` as normal. However, we also includethe data name in the `path_prefix`.\n",
        "    \n",
        "    Note that we pass the info to the fit, so that the `Aggregator` can make it accessible.\n",
        "    \"\"\"\n",
        "    analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    emcee = af.Emcee(\n",
        "        name=\"tutorial_8_fitting_multiple_datasets\",\n",
        "        path_prefix=path.join(\"howtofit\", \"database\", dataset_name),\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Emcee has begun running, checkout \\n\"\n",
        "        f\"autofit_workspace/output/howtofit/database/{dataset_name}/tutorial_7_multi folder for live \\n\"\n",
        "        f\"output of the results. This Jupyter notebook cell with progress once Emcee has completed, this could take a \\n\"\n",
        "        f\"few minutes!\"\n",
        "    )\n",
        "\n",
        "    emcee.fit(model=model, analysis=analysis, info=info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkout the output folder, you should see three new sets of results corresponding to our 3 `Gaussian` datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}