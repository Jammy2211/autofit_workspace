{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 2: Aggregator\n",
        "======================\n",
        "\n",
        "In the previous tutorial, we fitted 3 datasets with an identical `NonLinearSearch`, outputting the results of each to a\n",
        "unique folder on our hard disk.\n",
        "\n",
        "In this tutorial, we'll use the `Aggregator` to load the `Result`'s and manipulate them using our Jupyter\n",
        "notebook. The API for using a `Result` is described fully in tutorial 6 of chapter 1 of **HowToFit**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        " - `plot_profile_1d`: a function for plotting 1D profile datasets including their noise.\n",
        "\n",
        "These are functionally identical to the `Gaussian` and `plot_profile_1d` objects and functions you \n",
        "have seen and used elsewhere throughout the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# %%\n",
        "'''\n",
        "__Building a Database File From an Output Folder__\n",
        "\n",
        "In the previous tutorials, we built the database file `chapter_database.sqlite` via the results output to\n",
        "hard-disk.\n",
        "\n",
        "We can therefore simply load this database from the hard-disk in order to use the aggregator.\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "database_name = \"chapter_database\"\n",
        "\n",
        "agg = af.Aggregator.from_database(\n",
        "   filename=f\"{database_name}.sqlite\", completed_only=False\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Generators__\n",
        "\n",
        "Before using the aggregator to inspect results, let me quickly cover Python generators. A generator is an object that \n",
        "iterates over a function when it is called. The aggregator creates all of the objects that it loads from the database \n",
        "as generators (as opposed to a list, or dictionary, or other Python type).\n",
        "\n",
        "Why? Because lists and dictionaries store every entry in memory simultaneously. If you fit many datasets, this will use \n",
        "a lot of memory and crash your laptop! On the other hand, a generator only stores the object in memory when it is used; \n",
        "Python is then free to overwrite it afterwards. Thus, your laptop won't crash!\n",
        "\n",
        "There are two things to bare in mind with generators:\n",
        "\n",
        "1) A generator has no length and to determine how many entries it contains you first must turn it into a list.\n",
        "\n",
        "2) Once we use a generator, we cannot use it again and need to remake it. For this reason, we typically avoid \n",
        " storing the generator as a variable and instead use the aggregator to create them on use.\n",
        "\n",
        "We can now create a `samples` generator of every fit. The `results` example scripts show how , an instance of \n",
        "the `Samples` class acts as an interface to the results of the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_gen = agg.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we print this list of outputs you should see over 3 different `SamplesNest` instances, corresponding to the 3\n",
        "model-fits we performed in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Emcee Samples:\\n\")\n",
        "print(samples_gen)\n",
        "print(\"Total Samples Objects = \", len(agg), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples__\n",
        "\n",
        "We've encountered the `Samples` class in previous tutorials. As we saw in chapter 1, the `Samples` class contains all \n",
        "the accepted parameter samples of the `NonLinearSearch`, which is a list of lists where:\n",
        "\n",
        " - The outer list is the size of the total number of samples.\n",
        " - The inner list is the size of the number of free parameters in the fit.\n",
        "\n",
        "With the `Aggregator` we can now get information on the `Samples` of all 3 model-fits, as opposed to just 1 fit using \n",
        "its `Result` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(\"All parameters of the very first sample\")\n",
        "    print(samples.parameter_lists[0])\n",
        "    print(\"The tenth sample`s third parameter\")\n",
        "    print(samples.parameter_lists[9][2])\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the `Aggregator` to get information on the `log_likelihood_list`, log_prior_list`, `weight_list`, etc. of \n",
        "every fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(\"log(likelihood), log(prior), log(posterior) and weight of the tenth sample.\")\n",
        "    print(samples.log_likelihood_list[9])\n",
        "    print(samples.log_prior_list[9])\n",
        "    print(samples.log_posterior_list[9])\n",
        "    print(samples.weight_list[9])\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Instances__\n",
        "\n",
        "We can use the `Aggregator` to create a list of the `max_log_likelihood` instances of every fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_lh_instance_list = [samps.max_log_likelihood() for samps in agg.values(\"samples\")]\n",
        "\n",
        "print(\"Maximum Log Likelihood Model Instances:\\n\")\n",
        "print(max_lh_instance_list, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model instance contains all the model components of our fit which for the fits above was a single `Gaussian`\n",
        "profile (the word `gaussian` comes from what we called it in the `Collection` above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(max_lh_instance_list[0].gaussian)\n",
        "print(max_lh_instance_list[1].gaussian)\n",
        "print(max_lh_instance_list[2].gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This, of course, gives us access to any individual parameter of our maximum log likelihood `instance`. Below, we see \n",
        "that the 3 `Gaussian`s were simulated using `sigma` values of 1.0, 5.0 and 10.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(max_lh_instance_list[0].gaussian.sigma)\n",
        "print(max_lh_instance_list[1].gaussian.sigma)\n",
        "print(max_lh_instance_list[2].gaussian.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Vectors__\n",
        "\n",
        "We can use the outputs to create a list of the maximum log likelihood model of each fit to our three images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vector_list = [\n",
        "    samps.max_log_likelihood(as_instance=False) for samps in agg.values(\"samples\")\n",
        "]\n",
        "print(\"Maximum Log Likelihood Parameter Lists:\\n\")\n",
        "print(vector_list, \"\\n\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Median PDF__\n",
        "\n",
        "We can also access the `median_pdf` model via the `Aggregator`, as we saw for the `Samples` object in chapter 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "median_pdf_instance_list = [samps.median_pdf() for samps in agg.values(\"samples\")]\n",
        "\n",
        "print(\"Median PDF Model Instances:\\n\")\n",
        "print(median_pdf_instance_list, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Ordering__\n",
        "\n",
        "The default ordering of the results can be a bit random, as it depends on how the sqlite database is built. \n",
        "\n",
        "The `order_by` method can be used to order by a property of the database that is a string, for example by ordering \n",
        "using the `unique_tag` (which we set up in the search as the `dataset_name`) the database orders results alphabetically\n",
        "according to dataset name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg = agg.order_by(agg.search.unique_tag)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also order by a bool, for example making it so all completed results are at the front of the aggregator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg = agg.order_by(agg.search.is_complete)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Errors__\n",
        "\n",
        "Lets try something more ambitious and create a plot of the inferred `sigma` values vs `normalization` of each `Gaussian` \n",
        "profile, including error bars at $3\\sigma$ confidence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "median_pdf_instance_list = [samps.median_pdf() for samps in agg.values(\"samples\")]\n",
        "ue3_instance_list = [\n",
        "    samp.errors_at_upper_sigma(sigma=3.0) for samp in agg.values(\"samples\")\n",
        "]\n",
        "le3_instance_list = [\n",
        "    samp.errors_at_lower_sigma(sigma=3.0) for samp in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "mp_sigma_list = [instance.gaussian.sigma for instance in median_pdf_instance_list]\n",
        "ue3_sigma_list = [instance.gaussian.sigma for instance in ue3_instance_list]\n",
        "le3_sigma_list = [instance.gaussian.sigma for instance in le3_instance_list]\n",
        "mp_normalization_list = [\n",
        "    instance.gaussian.normalization for instance in median_pdf_instance_list\n",
        "]\n",
        "ue3_normalization_list = [\n",
        "    instance.gaussian.normalization for instance in ue3_instance_list\n",
        "]\n",
        "le3_normalization_list = [\n",
        "    instance.gaussian.normalization for instance in le3_instance_list\n",
        "]\n",
        "\n",
        "plt.errorbar(\n",
        "    x=mp_sigma_list,\n",
        "    y=mp_normalization_list,\n",
        "    marker=\".\",\n",
        "    linestyle=\"\",\n",
        "    xerr=[le3_sigma_list, ue3_sigma_list],\n",
        "    yerr=[le3_normalization_list, ue3_normalization_list],\n",
        ")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples Filtering__\n",
        "\n",
        "Our samples object has the results for all three parameters in our model. However, we might only be interested in the\n",
        "results of a specific parameter.\n",
        "\n",
        "The basic form of filtering specifies parameters via their path, which was printed above via the model and is printed \n",
        "again below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = list(agg.values(\"samples\"))[0]\n",
        "\n",
        "print(\"Parameter paths in the model which are used for filtering:\")\n",
        "print(samples.model.paths)\n",
        "\n",
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "samples = samples.with_paths([(\"gaussian\", \"centre\")])\n",
        "\n",
        "print(\"All parameters of the very first sample (containing only the Gaussian centre.\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "print(\"Maximum Log Likelihood Model Instances (containing only the Gaussian centre):\\n\")\n",
        "print(samples.max_log_likelihood(as_instance=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we specified each path as a list of tuples of strings. \n",
        "\n",
        "This is how the source code internally stores the path to different components of the model, but it is not in-line \n",
        "with the PyAutoFIT API used to compose a model.\n",
        "\n",
        "We can alternatively use the following API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = list(agg.values(\"samples\"))[0]\n",
        "\n",
        "samples = samples.with_paths([\"gaussian.centre\"])\n",
        "\n",
        "print(\"All parameters of the very first sample (containing only the Gaussian centre.\")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we filtered the `Samples` but asking for all parameters which included the path (\"gaussian\", \"centre\").\n",
        "\n",
        "We can alternatively filter the `Samples` object by removing all parameters with a certain path. Below, we remove\n",
        "the Gaussian's `centre` to be left with 2 parameters; the `normalization` and `sigma`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = list(agg.values(\"samples\"))[0]\n",
        "\n",
        "print(\"Parameter paths in the model which are used for filtering:\")\n",
        "print(samples.model.paths)\n",
        "\n",
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "samples = samples.without_paths([\"gaussian.centre\"])\n",
        "\n",
        "print(\n",
        "    \"All parameters of the very first sample (containing only the Gaussian normalization and sigma).\"\n",
        ")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can keep and remove entire paths of the samples, for example keeping only the parameters of the `Gaussian`.\n",
        "\n",
        "For this example this is somewhat trivial, given the model only contains the `Gaussian`, but for models with\n",
        "multiply components (e.g. a `Collection`) and multi-level models this can be a powerful way to extract samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = list(agg.values(\"samples\"))[0]\n",
        "samples = samples.with_paths([(\"gaussian\",)])\n",
        "print(\"Parameters of the first sample of the Gaussian model component\")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___Wrap Up__\n",
        "\n",
        "With that, tutorial 2 is complete. \n",
        "\n",
        "The take home point of this tutorial is that everything that is available in a `Result` or `Samples` object is \n",
        "accessible via the `Aggregator`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}