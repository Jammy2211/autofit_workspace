{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 1: Fitting Multiple Datasets\n",
        "=====================================\n",
        "\n",
        "The default behaviour of **PyAutoFit** is for model-fitting results to be output to hard-disc in folders, which are\n",
        "straight forward to navigate and manually check the model-fitting results and visualization. For small model-fitting\n",
        "tasks this is sufficient, however many users have a need to perform many model fits to very large datasets, making\n",
        "the manual inspection of results time consuming.\n",
        "\n",
        "PyAutoFit's database feature outputs all model-fitting results as a\n",
        "sqlite3 (https://docs.python.org/3/library/sqlite3.html) relational database, such that all results\n",
        "can be efficiently loaded into a Jupyter notebook or Python script for inspection, analysis and interpretation. This\n",
        "database supports advanced querying, so that specific model-fits (e.g., which fit a certain model or dataset) can be\n",
        "loaded.\n",
        "\n",
        "In this tutorial, we fit multiple dataset's with a `NonLinearSearch`, producing multiple sets of results on our\n",
        "hard-disc. In the following tutorials, we load these results using the database into our Jupyter notebook and\n",
        "interpret, inspect and plot the results.\n",
        "\n",
        "we'll fit 3 different dataset's, each with a single `Gaussian` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import numpy as np\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        " - `plot_profile_1d`: a function for plotting 1D profile datasets including their noise.\n",
        "\n",
        "These are functionally identical to the `Gaussian` and `plot_profile_1d` objects and functions you \n",
        "have seen and used elsewhere throughout the workspace.\n",
        "\n",
        "__Analysis__\n",
        "\n",
        "The `Analysis` class below has a new method, `save_attributes_for_aggregator`, which specifies the properties of the \n",
        "fit that are output to hard-disc so that we can load them using the database in the next tutorials.\n",
        "\n",
        "In particular, note how we output the `data` and `noise_map`, these will be loaded in tutorial 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def model_data_from_instance(self, instance):\n",
        "        \"\"\"\n",
        "        To create the summed profile of all individual profiles in an instance, we can use a dictionary comprehension\n",
        "        to iterate over all profiles in the instance.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        return sum(\n",
        "            [\n",
        "                profile.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "                for profile in instance\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def visualize(self, paths, instance, during_analysis):\n",
        "        \"\"\"\n",
        "        This method is identical to the previous tutorial, except it now uses the `model_data_from_instance` method\n",
        "        to create the profile.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "\n",
        "        \"\"\"\n",
        "        The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\n",
        "        \"\"\"\n",
        "        af.ex.plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=self.data,\n",
        "            title=\"Data\",\n",
        "            ylabel=\"Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"data\",\n",
        "        )\n",
        "\n",
        "        af.ex.plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=model_data,\n",
        "            title=\"Model Data\",\n",
        "            ylabel=\"Model Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"model_data\",\n",
        "        )\n",
        "\n",
        "        af.ex.plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=residual_map,\n",
        "            title=\"Residual Map\",\n",
        "            ylabel=\"Residuals\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"residual_map\",\n",
        "        )\n",
        "\n",
        "        af.ex.plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=chi_squared_map,\n",
        "            title=\"Chi-Squared Map\",\n",
        "            ylabel=\"Chi-Squareds\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"chi_squared_map\",\n",
        "        )\n",
        "\n",
        "    def save_attributes_for_aggregator(self, paths):\n",
        "        \"\"\"\n",
        "        Save files like the data and noise-map as pickle files so they can be loaded in the `Aggregator`\n",
        "        \"\"\"\n",
        "\n",
        "        # These functions save the objects we will later access using the aggregator. They are saved via the `pickle`\n",
        "        # module in Python, which serializes the data on to the hard-disk.\n",
        "\n",
        "        paths.save_object(\"data\", self.data)\n",
        "\n",
        "        paths.save_object(\"noise_map\", self.noise_map)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "We'll fit the single `Gaussian` model used in chapter 1 of **HowToFit**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.ex.Gaussian)\n",
        "\n",
        "model.gaussian.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.gaussian.normalization = af.LogUniformPrior(lower_limit=1e-2, upper_limit=1e2)\n",
        "model.gaussian.sigma = af.GaussianPrior(\n",
        "    mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each dataset we load it from hard-disc, set up its `Analysis` class and fit it with a non-linear search. \n",
        "\n",
        "The 3 datasets are in the `autofit_workspace/dataset/example_1d` folder.\n",
        "\n",
        "We want each results to be stored in the database with an entry specific to the dataset. We'll use the `Dataset`'s name \n",
        "string to do this, so lets create a list of the 3 dataset names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name_list = [\"gaussian_x1_0\", \"gaussian_x1_1\", \"gaussian_x1_2\"]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Info__\n",
        "\n",
        "We can also attach information to the model-fit, by setting up an info dictionary. \n",
        "\n",
        "Information about our model-fit (e.g. the dataset) that isn't part of the model-fit is made accessible to the \n",
        "database. For example, below we write info on the dataset`s (hypothetical) data of observation and exposure time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "info = {\"date_of_observation\": \"01-02-18\", \"exposure_time\": 1000.0}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___Session__\n",
        "\n",
        "To output results directly to the database, we start a session, which includes the name of the database `.sqlite` file\n",
        "where results are stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "session = af.db.open_database(\"database_howtofit.sqlite\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "This for loop runs over every dataset, checkout the comments below for how we set up the path structure.\n",
        "\n",
        "Note how the `session` is passed to the `DynestyStatic` search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name in dataset_name_list:\n",
        "    \"\"\"\n",
        "    The code below loads the dataset and sets up the Analysis class.\n",
        "    \"\"\"\n",
        "    dataset_path = path.join(\"dataset\", \"example_1d\", dataset_name)\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    \"\"\"\n",
        "    In all examples so far, results were wrriten to the `autofit_workspace/output` folder with a path and folder \n",
        "    named after the non-linear search.\n",
        "\n",
        "    In this example, results are written directly to the `database.sqlite` file after the model-fit is complete and \n",
        "    only stored in the output folder during the model-fit. This can be important for performing large model-fitting \n",
        "    tasks on high performance computing facilities where there may be limits on the number of files allowed, or there\n",
        "    are too many results to make navigating the output folder manually feasible.\n",
        "\n",
        "    The `unique_tag` uses the `dataset_name` name below to generate the unique identifier, which in other examples we \n",
        "    have seen is also generated depending on the search settings and model. In this example, all three model fits\n",
        "    use an identical search and model, so this `unique_tag` is key in ensuring 3 separate sets of results for each\n",
        "    model-fit are stored in the output folder and written to the .sqlite database. \n",
        "    \"\"\"\n",
        "    dynesty = af.DynestyStatic(\n",
        "        name=\"database_example\",\n",
        "        path_prefix=path.join(\"howtofit\", \"chapter_database\", dataset_name),\n",
        "        unique_tag=dataset_name,  # This makes the unique identifier use the dataset name\n",
        "        session=session,  # This instructs the search to write to the .sqlite database.\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"The non-linear search has begun running, checkout \\n\"\n",
        "        f\"autofit_workspace/output/howtofit/database/{dataset_name}/tutorial_1_fitting_multiple_datasets folder for live \\n\"\n",
        "        f\"output of the results. This Jupyter notebook cell with progress once the search has completed, this could take a \\n\"\n",
        "        f\"few minutes!\"\n",
        "    )\n",
        "\n",
        "    dynesty.fit(model=model, analysis=analysis, info=info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "Checkout the output folder, during the model-fits you should see three new sets of results corresponding to \n",
        "our 3 `Gaussian` datasets. If the model-fits are already complete, you will only see the .sqlite database file.\n",
        "\n",
        "This completes tutorial 1, which was less of a tutorial and more a quick exercise in getting the results of three \n",
        "model-fits onto our hard-disc to demonstrate **PyAutoFit**'s database feature!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}