{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial 1: Fitting Multiple Datasets\n",
    "=====================================\n",
    "\n",
    "The default behaviour of **PyAutoFit** is for model-fitting results to be output to hard-disc in folders, which are\n",
    "straight forward to navigate and manually check the model-fitting results and visualization. For small model-fitting\n",
    "tasks this is sufficient, however many users have a need to perform many model fits to very large datasets, making\n",
    "the manual inspection of results time consuming.\n",
    "\n",
    "PyAutoFit's database feature outputs all model-fitting results as a\n",
    "sqlite3 (https://docs.python.org/3/library/sqlite3.html) relational database, such that all results\n",
    "can be efficiently loaded into a Jupyter notebook or Python script for inspection, analysis and interpretation. This\n",
    "database supports advanced querying, so that specific model-fits (e.g., which fit a certain model or dataset) can be\n",
    "loaded.\n",
    "\n",
    "In this tutorial, we fit multiple dataset's with a `NonLinearSearch`, producing multiple sets of results on our\n",
    "hard-disc. In the following tutorials, we load these results using the database into our Jupyter notebook and\n",
    "interpret, inspect and plot the results.\n",
    "\n",
    "we'll fit 3 different dataset's, each with a single `Gaussian` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T13:36:55.808800Z",
     "iopub.status.busy": "2021-03-06T13:36:55.808448Z",
     "iopub.status.idle": "2021-03-06T13:36:56.581919Z",
     "shell.execute_reply": "2021-03-06T13:36:56.581565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Jammy/Code/PyAuto/autofit_workspace\n",
      "Working Directory has been set to `/mnt/c/Users/Jammy/Code/PyAuto/autofit_workspace`\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pyprojroot import here\n",
    "workspace_path = str(here())\n",
    "%cd $workspace_path\n",
    "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
    "\n",
    "import autofit as af\n",
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll reuse the `plot_line` and `Analysis` classes of the previous tutorial.\n",
    "\n",
    "Note that the `Analysis` class has a new method, `save_attributes_for_aggregator`. This method specifies which properties of the\n",
    "fit are output to hard-disc so that we can load them using the `Aggregator` in the next tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T13:36:56.589573Z",
     "iopub.status.busy": "2021-03-06T13:36:56.589204Z",
     "iopub.status.idle": "2021-03-06T13:36:56.591046Z",
     "shell.execute_reply": "2021-03-06T13:36:56.590729Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_line(\n",
    "    xvalues,\n",
    "    line,\n",
    "    title=None,\n",
    "    ylabel=None,\n",
    "    errors=None,\n",
    "    color=\"k\",\n",
    "    output_path=None,\n",
    "    output_filename=None,\n",
    "):\n",
    "    plt.errorbar(\n",
    "        x=xvalues, y=line, yerr=errors, color=color, ecolor=\"k\", elinewidth=1, capsize=2\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x value of profile\")\n",
    "    plt.ylabel(ylabel)\n",
    "    if not path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    plt.savefig(path.join(output_path, f\"{output_filename}.png\"))\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "class Analysis(af.Analysis):\n",
    "    def __init__(self, data, noise_map):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.noise_map = noise_map\n",
    "\n",
    "    def log_likelihood_function(self, instance):\n",
    "\n",
    "        model_data = self.model_data_from_instance(instance=instance)\n",
    "\n",
    "        residual_map = self.data - model_data\n",
    "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
    "        chi_squared = sum(chi_squared_map)\n",
    "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))\n",
    "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def model_data_from_instance(self, instance):\n",
    "        \"\"\"\n",
    "        To create the summed profile of all individual profiles in an instance, we can use a dictionary comprehension\n",
    "        to iterate over all profiles in the instance.\n",
    "        \"\"\"\n",
    "        xvalues = np.arange(self.data.shape[0])\n",
    "\n",
    "        return sum(\n",
    "            [profile.profile_from_xvalues(xvalues=xvalues) for profile in instance]\n",
    "        )\n",
    "\n",
    "    def visualize(self, paths, instance, during_analysis):\n",
    "        \"\"\"\n",
    "        This method is identical to the previous tutorial, except it now uses the `model_data_from_instance` method\n",
    "        to create the profile.\n",
    "        \"\"\"\n",
    "        xvalues = np.arange(self.data.shape[0])\n",
    "\n",
    "        model_data = self.model_data_from_instance(instance=instance)\n",
    "\n",
    "        residual_map = self.data - model_data\n",
    "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
    "\n",
    "        \"\"\"\n",
    "        The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\n",
    "        \"\"\"\n",
    "        plot_line(\n",
    "            xvalues=xvalues,\n",
    "            line=self.data,\n",
    "            title=\"Data\",\n",
    "            ylabel=\"Data Values\",\n",
    "            color=\"k\",\n",
    "            output_path=paths.image_path,\n",
    "            output_filename=\"data\",\n",
    "        )\n",
    "\n",
    "        plot_line(\n",
    "            xvalues=xvalues,\n",
    "            line=model_data,\n",
    "            title=\"Model Data\",\n",
    "            ylabel=\"Model Data Values\",\n",
    "            color=\"k\",\n",
    "            output_path=paths.image_path,\n",
    "            output_filename=\"model_data\",\n",
    "        )\n",
    "\n",
    "        plot_line(\n",
    "            xvalues=xvalues,\n",
    "            line=residual_map,\n",
    "            title=\"Residual Map\",\n",
    "            ylabel=\"Residuals\",\n",
    "            color=\"k\",\n",
    "            output_path=paths.image_path,\n",
    "            output_filename=\"residual_map\",\n",
    "        )\n",
    "\n",
    "        plot_line(\n",
    "            xvalues=xvalues,\n",
    "            line=chi_squared_map,\n",
    "            title=\"Chi-Squared Map\",\n",
    "            ylabel=\"Chi-Squareds\",\n",
    "            color=\"k\",\n",
    "            output_path=paths.image_path,\n",
    "            output_filename=\"chi_squared_map\",\n",
    "        )\n",
    "\n",
    "    def save_attributes_for_aggregator(self, paths):\n",
    "        \"\"\"\n",
    "        Save files like the data and noise-map as pickle files so they can be loaded in the `Aggregator`\n",
    "        \"\"\"\n",
    "\n",
    "        # These functions save the objects we will later access using the aggregator. They are saved via the `pickle`\n",
    "        # module in Python, which serializes the data on to the hard-disk.\n",
    "\n",
    "        with open(path.join(f\"{paths.pickle_path}\", \"data.pickle\"), \"wb\") as f:\n",
    "            pickle.dump(self.data, f)\n",
    "\n",
    "        with open(path.join(f\"{paths.pickle_path}\", \"noise_map.pickle\"), \"wb\") as f:\n",
    "            pickle.dump(self.noise_map, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit the single `Gaussian` model used in chapter 1 of **HowToFit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T13:36:56.593751Z",
     "iopub.status.busy": "2021-03-06T13:36:56.593369Z",
     "iopub.status.idle": "2021-03-06T13:36:56.605201Z",
     "shell.execute_reply": "2021-03-06T13:36:56.604938Z"
    }
   },
   "outputs": [],
   "source": [
    "import profiles as p\n",
    "\n",
    "model = af.CollectionPriorModel(gaussian=p.Gaussian)\n",
    "\n",
    "model.gaussian.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
    "model.gaussian.intensity = af.LogUniformPrior(lower_limit=1e-2, upper_limit=1e2)\n",
    "model.gaussian.sigma = af.GaussianPrior(\n",
    "    mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset we load it from hard-disc, set up its `Analysis` class and fit it with a non-linear search. \n",
    "\n",
    "The 3 datasets are in the `autofit_workspace/dataset/example_1d` folder.\n",
    "\n",
    "We want each results to be stored in the database with an entry specific to the dataset. We'll use the `Dataset`'s name \n",
    "string to do this, so lets create a list of the 3 dataset names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T13:36:56.607384Z",
     "iopub.status.busy": "2021-03-06T13:36:56.607087Z",
     "iopub.status.idle": "2021-03-06T13:36:56.608985Z",
     "shell.execute_reply": "2021-03-06T13:36:56.608682Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_names = [\"gaussian_x1_0\", \"gaussian_x1_1\", \"gaussian_x1_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also attach information to the model-fit, by setting up an info dictionary. \n",
    "\n",
    "Information about our model-fit (e.g. the dataset) that isn't part of the model-fit is made accessible to the \n",
    "database. For example, below we write info on the dataset`s (hypothetical) data of observation and exposure time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T13:36:56.611250Z",
     "iopub.status.busy": "2021-03-06T13:36:56.610958Z",
     "iopub.status.idle": "2021-03-06T13:36:56.612492Z",
     "shell.execute_reply": "2021-03-06T13:36:56.612709Z"
    }
   },
   "outputs": [],
   "source": [
    "info = {\"date_of_observation\": \"01-02-18\", \"exposure_time\": 1000.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for loop runs over every dataset, checkout the comments below for how we set up the path structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-06T13:36:56.615722Z",
     "iopub.status.busy": "2021-03-06T13:36:56.615416Z",
     "iopub.status.idle": "2021-03-06T13:36:59.478601Z",
     "shell.execute_reply": "2021-03-06T13:36:59.478287Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Generating initial samples of model, which are subject to prior limits and other constraints.\n",
      "INFO:root:No Dynesty samples found, beginning new non-linear search. \n",
      "2it [00:00, 738.24it/s, +50 | bound: 0 | nc: 1 | ncall: 52 | eff(%): 100.000 | loglstar:   -inf < -45870.059 <    inf | logz: -45874.723 +/-  0.521 | dlogz:  1.099 >  0.059]\n",
      "INFO:root:5000 Iterations: Performing update (Visualization, outputting samples, etc.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emcee has begun running, checkout \n",
      "autofit_workspace/output/howtofit/database/gaussian_x1_0/tutorial_7_multi folder for live \n",
      "output of the results. This Jupyter notebook cell with progress once Emcee has completed, this could take a \n",
      "few minutes!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:10000 Iterations: Performing update (Visualization, outputting samples, etc.).\n",
      "INFO:root:15000 Iterations: Performing update (Visualization, outputting samples, etc.).\n",
      "INFO:root:Generating initial samples of model, which are subject to prior limits and other constraints.\n",
      "INFO:root:No Dynesty samples found, beginning new non-linear search. \n",
      "2it [00:00, 617.81it/s, +50 | bound: 0 | nc: 1 | ncall: 52 | eff(%): 100.000 | loglstar:   -inf < -3093.549 <    inf | logz: -3098.214 +/-  0.547 | dlogz:  1.099 >  0.059]\n",
      "INFO:root:5000 Iterations: Performing update (Visualization, outputting samples, etc.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emcee has begun running, checkout \n",
      "autofit_workspace/output/howtofit/database/gaussian_x1_1/tutorial_7_multi folder for live \n",
      "output of the results. This Jupyter notebook cell with progress once Emcee has completed, this could take a \n",
      "few minutes!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:10000 Iterations: Performing update (Visualization, outputting samples, etc.).\n",
      "INFO:root:15000 Iterations: Performing update (Visualization, outputting samples, etc.).\n",
      "INFO:root:Generating initial samples of model, which are subject to prior limits and other constraints.\n",
      "INFO:root:No Dynesty samples found, beginning new non-linear search. \n",
      "2it [00:00, 730.14it/s, +50 | bound: 0 | nc: 1 | ncall: 52 | eff(%): 100.000 | loglstar:   -inf < -401.685 <    inf | logz: -406.349 +/-  0.556 | dlogz:  1.099 >  0.059]\n",
      "INFO:root:5000 Iterations: Performing update (Visualization, outputting samples, etc.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emcee has begun running, checkout \n",
      "autofit_workspace/output/howtofit/database/gaussian_x1_2/tutorial_7_multi folder for live \n",
      "output of the results. This Jupyter notebook cell with progress once Emcee has completed, this could take a \n",
      "few minutes!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:10000 Iterations: Performing update (Visualization, outputting samples, etc.).\n",
      "INFO:root:15000 Iterations: Performing update (Visualization, outputting samples, etc.).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset_name in dataset_names:\n",
    "\n",
    "    \"\"\"\n",
    "    The code below loads the dataset and sets up the Analysis class.\n",
    "    \"\"\"\n",
    "    dataset_path = path.join(\"dataset\", \"example_1d\", dataset_name)\n",
    "\n",
    "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
    "    noise_map = af.util.numpy_array_from_json(\n",
    "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
    "    )\n",
    "\n",
    "    analysis = Analysis(data=data, noise_map=noise_map)\n",
    "\n",
    "    \"\"\"\n",
    "    In all examples so far, results have gone to the default output path, which was the `autofit_workspace/output` \n",
    "    folder and a folder named after the non linear search. In this example, we will repeat this process and then load\n",
    "    these results into the database and a `database.sqlite` file.\n",
    "\n",
    "    However, results can be written directly to the `database.sqlite` file omitted hard-disc output entirely, which\n",
    "    can be important for performing large model-fitting tasks on high performance computing facilities where there\n",
    "    may be limits on the number of files allowed. The commented out code below shows how one would perform\n",
    "    direct output to the `.sqlite` file. \n",
    "\n",
    "    [NOTE: direct writing to .sqlite not supported yet, so this fit currently outputs to hard-disc as per usual and\n",
    "    these outputs will be used to make the database.]\n",
    "    \"\"\"\n",
    "    emcee = af.DynestyStatic(\n",
    "        path_prefix=path.join(\"howtofit\", \"database\", dataset_name),\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Emcee has begun running, checkout \\n\"\n",
    "        f\"autofit_workspace/output/howtofit/database/{dataset_name}/tutorial_7_multi folder for live \\n\"\n",
    "        f\"output of the results. This Jupyter notebook cell with progress once Emcee has completed, this could take a \\n\"\n",
    "        f\"few minutes!\"\n",
    "    )\n",
    "\n",
    "    emcee.fit(model=model, analysis=analysis, info=info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout the output folder, you should see three new sets of results corresponding to our 3 `Gaussian` datasets.\n",
    "\n",
    "This completes tutorial 1, which was less of a tutorial and more a quick exercise in getting the results of three \n",
    "model-fits onto our hard-disc to demonstrate **PyAutoFit**'s database feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
