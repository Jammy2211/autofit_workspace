{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 1: Fitting Multiple Datasets\n",
        "=====================================\n",
        "\n",
        "The default behaviour of **PyAutoFit** is for model-fitting results to be output to hard-disc in folders, which are\n",
        "straight forward to navigate and manually check the model-fitting results and visualization. For small model-fitting\n",
        "tasks this is sufficient, however many users have a need to perform many model fits to very large datasets, making\n",
        "the manual inspection of results time consuming.\n",
        "\n",
        "PyAutoFit's database feature outputs all model-fitting results as a\n",
        "sqlite3 (https://docs.python.org/3/library/sqlite3.html) relational database, such that all results\n",
        "can be efficiently loaded into a Jupyter notebook or Python script for inspection, analysis and interpretation. This\n",
        "database supports advanced querying, so that specific model-fits (e.g., which fit a certain model or dataset) can be\n",
        "loaded.\n",
        "\n",
        "In this tutorial, we fit multiple dataset's with a `NonLinearSearch`, producing multiple sets of results on our\n",
        "hard-disc. In the following tutorials, we load these results using the database into our Jupyter notebook and\n",
        "interpret, inspect and plot the results.\n",
        "\n",
        "we'll fit 3 different dataset's, each with a single `Gaussian` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import numpy as np\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        " - `plot_profile_1d`: a function for plotting 1D profile datasets including their noise.\n",
        "\n",
        "These are functionally identical to the `Gaussian` and `plot_profile_1d` objects and functions you \n",
        "have seen and used elsewhere throughout the workspace.\n",
        "\n",
        "__Analysis__\n",
        "\n",
        "The `Analysis` class below has a new method, `save_attributes`, which specifies the properties of the \n",
        "fit that are output to hard-disc so that we can load them using the database in the next tutorials.\n",
        "\n",
        "In particular, note how we output the `data` and `noise_map`, these will be loaded in tutorial 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def model_data_from_instance(self, instance):\n",
        "        \"\"\"\n",
        "        To create the summed profile of all individual profiles in an instance, we can use a dictionary comprehension\n",
        "        to iterate over all profiles in the instance.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        return sum(\n",
        "            [\n",
        "                profile.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "                for profile in instance\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def visualize(self, paths, instance, during_analysis):\n",
        "        \"\"\"\n",
        "        This method is identical to the previous tutorial, except it now uses the `model_data_from_instance` method\n",
        "        to create the profile.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = self.model_data_from_instance(instance=instance)\n",
        "\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "\n",
        "        \"\"\"\n",
        "        The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\n",
        "        \"\"\"\n",
        "        af.ex.plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=self.data,\n",
        "            title=\"Data\",\n",
        "            ylabel=\"Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"data\",\n",
        "        )\n",
        "\n",
        "        af.ex.plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=model_data,\n",
        "            title=\"Model Data\",\n",
        "            ylabel=\"Model Data Values\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"model_data\",\n",
        "        )\n",
        "\n",
        "        af.ex.plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=residual_map,\n",
        "            title=\"Residual Map\",\n",
        "            ylabel=\"Residuals\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"residual_map\",\n",
        "        )\n",
        "\n",
        "        af.ex.plot_profile_1d(\n",
        "            xvalues=xvalues,\n",
        "            profile_1d=chi_squared_map,\n",
        "            title=\"Chi-Squared Map\",\n",
        "            ylabel=\"Chi-Squareds\",\n",
        "            color=\"k\",\n",
        "            output_path=paths.image_path,\n",
        "            output_filename=\"chi_squared_map\",\n",
        "        )\n",
        "\n",
        "    def save_attributes(self, paths):\n",
        "        \"\"\"\n",
        "        Save files like the data and noise-map as pickle files so they can be loaded in the `Aggregator`\n",
        "        \"\"\"\n",
        "\n",
        "        # These functions save the objects we will later access using the aggregator. They are saved via the `pickle`\n",
        "        # module in Python, which serializes the data on to the hard-disk.\n",
        "\n",
        "        paths.save_object(\"data\", self.data)\n",
        "\n",
        "        paths.save_object(\"noise_map\", self.noise_map)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "We'll fit the single `Gaussian` model used in chapter 1 of **HowToFit**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.ex.Gaussian)\n",
        "\n",
        "model.gaussian.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.gaussian.normalization = af.LogUniformPrior(lower_limit=1e-2, upper_limit=1e2)\n",
        "model.gaussian.sigma = af.GaussianPrior(\n",
        "    mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each dataset we load it from hard-disc, set up its `Analysis` class and fit it with a non-linear search. \n",
        "\n",
        "The 3 datasets are in the `autofit_workspace/dataset/example_1d` folder.\n",
        "\n",
        "We want each results to be stored in the database with an entry specific to the dataset. We'll use the `Dataset`'s name \n",
        "string to do this, so lets create a list of the 3 dataset names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name_list = [\"gaussian_x1_0\", \"gaussian_x1_1\", \"gaussian_x1_2\"]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Info__\n",
        "\n",
        "We can also attach information to the model-fit, by setting up an info dictionary. \n",
        "\n",
        "Information about our model-fit (e.g. the dataset) that isn't part of the model-fit is made accessible to the \n",
        "database. For example, below we write info on the dataset`s (hypothetical) data of observation and exposure time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "info = {\"date_of_observation\": \"01-02-18\", \"exposure_time\": 1000.0}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Results From Hard Disk__\n",
        "\n",
        "We now perform a simple model-fit to 3 datasets, where the results are written to hard-disk using the standard \n",
        "output directory structure and we will then build the database from these results. This behaviour is governed \n",
        "by us inputting `session=None`.\n",
        "\n",
        "If you have existing results you wish to build a database for, you can therefore adapt this example you to do this.\n",
        "\n",
        "Later in this example we show how results can also also be output directly to an .sqlite database, saving on hard-disk \n",
        "space. This will be acheived by setting `session` to something that is not `None`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "session = None"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "This for loop runs over every dataset, checkout the comments below for how we set up the path structure.\n",
        "\n",
        "Note how the `session` is passed to the `DynestyStatic` search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name in dataset_name_list:\n",
        "    \"\"\"\n",
        "    The code below loads the dataset and sets up the Analysis class.\n",
        "    \"\"\"\n",
        "    dataset_path = path.join(\"dataset\", \"example_1d\", dataset_name)\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    \"\"\"\n",
        "    In all examples so far, results were wrriten to the `autofit_workspace/output` folder with a path and folder \n",
        "    named after the non-linear search.\n",
        "\n",
        "    In this example, results are written directly to the `database.sqlite` file after the model-fit is complete and \n",
        "    only stored in the output folder during the model-fit. This can be important for performing large model-fitting \n",
        "    tasks on high performance computing facilities where there may be limits on the number of files allowed, or there\n",
        "    are too many results to make navigating the output folder manually feasible.\n",
        "\n",
        "    The `unique_tag` uses the `dataset_name` name below to generate the unique identifier, which in other examples we \n",
        "    have seen is also generated depending on the search settings and model. In this example, all three model fits\n",
        "    use an identical search and model, so this `unique_tag` is key in ensuring 3 separate sets of results for each\n",
        "    model-fit are stored in the output folder and written to the .sqlite database. \n",
        "    \"\"\"\n",
        "    search = af.DynestyStatic(\n",
        "        name=\"database_example\",\n",
        "        path_prefix=path.join(\"howtofit\", \"chapter_database\", dataset_name),\n",
        "        unique_tag=dataset_name,  # This makes the unique identifier use the dataset name\n",
        "        session=session,  # This can instruct the search to write to the .sqlite database.\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"The non-linear search has begun running, checkout \\n\"\n",
        "        f\"autofit_workspace/output/howtofit/database/{dataset_name}/tutorial_1_fitting_multiple_datasets folder for live \\n\"\n",
        "        f\"output of the results. This Jupyter notebook cell with progress once the search has completed, this could take a \\n\"\n",
        "        f\"few minutes!\"\n",
        "    )\n",
        "\n",
        "    search.fit(model=model, analysis=analysis, info=info)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Building a Database File From an Output Folder__\n",
        "\n",
        "The fits above wrote the results to hard-disk in folders, not as an .sqlite database file. \n",
        "\n",
        "We build the database below, where the `database_name` corresponds to the name of your output folder and is also the \n",
        "name of the `.sqlite` database file that is created.\n",
        "\n",
        "If you are fitting a relatively small number of datasets (e.g. 10-100) having all results written to hard-disk (e.g. \n",
        "for quick visual inspection) and using the database for sample wide analysis is beneficial.\n",
        "\n",
        "We can optionally only include completed model-fits but setting `completed_only=True`.\n",
        "\n",
        "If you inspect the `output` folder, you will see a `database.sqlite` file which contains the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "database_name = \"chapter_database\"\n",
        "\n",
        "agg = af.Aggregator.from_database(\n",
        "    filename=f\"{database_name}.sqlite\", completed_only=False\n",
        ")\n",
        "\n",
        "agg.add_directory(directory=path.join(\"output\", \"howtofit\", database_name))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Writing Directly To Database__\n",
        "\n",
        "Results can be written directly to the .sqlite database file, skipping output to hard-disk entirely, by creating\n",
        "a session and passing this to the non-linear search.\n",
        "\n",
        "The code below shows how to do this, but it is commented out to avoid rerunning the non-linear searches.\n",
        "\n",
        "This is ideal for tasks where model-fits to hundreds or thousands of datasets are performed, as it becomes unfeasible\n",
        "to inspect the results of all fits on the hard-disk. \n",
        "\n",
        "Our recommended workflow is to set up database analysis scripts using ~10 model-fits, and then scaling these up\n",
        "to large samples by writing directly to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# session = af.db.open_database(\"chapter_database.sqlite\")\n",
        "#\n",
        "# search = af.Nautilus(\n",
        "#     path_prefix=path.join(\"database\"),\n",
        "#     name=\"database_example\",\n",
        "#     unique_tag=dataset_name,  # This makes the unique identifier use the dataset name\n",
        "#     session=session,  # This can instruct the search to write to the .sqlite database.\n",
        "#     n_live=100,\n",
        "# )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you run the above code and inspect the `output` folder, you will see a `database.sqlite` file which contains \n",
        "the results.\n",
        "\n",
        "We can load the database using the `Aggregator` as we did above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# agg = af.Aggregator.from_database(\"chapter_database.sqlite\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "Checkout the output folder, during the model-fits you should see three new sets of results corresponding to \n",
        "our 3 `Gaussian` datasets. If the model-fits are already complete, you will only see the .sqlite database file.\n",
        "\n",
        "This completes tutorial 1, which was less of a tutorial and more a quick exercise in getting the results of three \n",
        "model-fits onto our hard-disc to demonstrate **PyAutoFit**'s database feature!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}