{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overview: Scientific Workflow\n",
        "=============================\n",
        "\n",
        "A scientific workflow comprises the tasks you perform to conduct a scientific study. This includes fitting models to\n",
        "datasets, interpreting the results, and gaining insights into your scientific problem.\n",
        "\n",
        "Different problems require different scientific workflows, depending on factors such as model complexity, dataset size,\n",
        "and computational run times. For example, some problems involve fitting a single dataset with many models to gain\n",
        "scientific insights, while others involve fitting thousands of datasets with a single model for large-scale studies.\n",
        "\n",
        "The **PyAutoFit** API is flexible, customizable, and extensible, enabling users to develop scientific workflows\n",
        "tailored to their specific problems.\n",
        "\n",
        "This overview covers the key features of **PyAutoFit** that support the development of effective scientific workflows:\n",
        "\n",
        "- **On The Fly**: Display results immediately (e.g., in Jupyter notebooks) to provide instant feedback for adapting your workflow.\n",
        "- **Hard Disk Output**: Output results to hard disk with high customization, allowing quick and detailed inspection of fits to many datasets.\n",
        "- **Visualization**: Generate model-specific visualizations to create custom plots that streamline result inspection.\n",
        "- **Loading Results**: Load results from the hard disk to inspect and interpret the outcomes of a model fit.\n",
        "- **Result Customization**: Customize the returned results to simplify scientific interpretation.\n",
        "- **Model Composition**: Extensible model composition makes it easy to fit many models with different parameterizations and assumptions.\n",
        "- **Searches**: Support for various non-linear searches (e.g., nested sampling, MCMC), including gradient based fitting using JAX, to find the right method for your problem.\n",
        "- **Configs**: Configuration files that set default model, fitting, and visualization behaviors, streamlining model fitting.\n",
        "- **Database**: Store results in a relational SQLite3 database, enabling efficient management of large modeling results.\n",
        "- **Scaling Up**: Guidance on scaling up your scientific workflow from small to large datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "from os import path\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "To illustrate a few aspects of the scientific workflow, we'll fit a 1D Gaussian profile to data, which\n",
        "we load from hard-disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__On The Fly__\n",
        "\n",
        "The on-the-fly feature described below is not implemented yet, we are working on it currently.\n",
        "The best way to get on-the-fly output is to output to hard-disk, which is described in the next section.\n",
        "This feature is fully implemented and provides on-the-fly output of results to hard-disk.\n",
        "\n",
        "When a model fit is running, information about the fit is displayed at user-specified intervals.\n",
        "\n",
        "The frequency of this on-the-fly output is controlled by a search's `iterations_per_update` parameter, which\n",
        "specifies how often this information is output. The example code below outputs on-the-fly information every 1000 \n",
        "iterations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(iterations_per_update=1000)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a Jupyter notebook, the default behavior is for this information to appear in the cell being run and to include:\n",
        "\n",
        "- Text displaying the maximum likelihood model inferred so far and related information.\n",
        "- A visual showing how the search has sampled parameter space so far, providing intuition on how the search is \n",
        "performing.\n",
        "\n",
        "Here is an image of how this looks:\n",
        "\n",
        "![Example On-the-Fly Output](path/to/image.png)\n",
        "\n",
        "The most valuable on-the-fly output is often specific to the model and dataset you are fitting. For instance, it\n",
        "might be a ``matplotlib`` subplot showing the maximum likelihood model's fit to the dataset, complete with residuals\n",
        "and other diagnostic information.\n",
        "\n",
        "The on-the-fly output can be fully customized by extending the ``on_the_fly_output`` method of the ``Analysis``\n",
        "class being used to fit the model.\n",
        "\n",
        "The example below shows how this is done for the simple case of fitting a 1D Gaussian profile:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data: np.ndarray, noise_map: np.ndarray):\n",
        "        \"\"\"\n",
        "        Example Analysis class illustrating how to customize the on-the-fly output of a model-fit.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def on_the_fly_output(self, instance):\n",
        "        \"\"\"\n",
        "        During a model-fit, the `on_the_fly_output` method is called throughout the non-linear search.\n",
        "\n",
        "        The `instance` passed into the method is maximum log likelihood solution obtained by the model-fit so far and it can be\n",
        "        used to provide on-the-fly output showing how the model-fit is going.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_from(xvalues=xvalues)\n",
        "\n",
        "        \"\"\"\n",
        "        The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        plt.errorbar(\n",
        "            x=xvalues,\n",
        "            y=self.data,\n",
        "            yerr=self.noise_map,\n",
        "            color=\"k\",\n",
        "            ecolor=\"k\",\n",
        "            elinewidth=1,\n",
        "            capsize=2,\n",
        "        )\n",
        "        plt.plot(xvalues, model_data, color=\"r\")\n",
        "        plt.title(\"Maximum Likelihood Fit\")\n",
        "        plt.xlabel(\"x value of profile\")\n",
        "        plt.ylabel(\"Profile Normalization\")\n",
        "        plt.show()  # By using `plt.show()` the plot will be displayed in the Jupyter notebook.\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's how the visuals appear in a Jupyter Notebook:\n",
        "\n",
        "![Example On-the-Fly Output](path/to/image.png)\n",
        "\n",
        "In the early stages of setting up a scientific workflow, on-the-fly output is invaluable. It provides immediate\n",
        "feedback on how your model fitting is performing, which is often crucial at the beginning of a project when things\n",
        "might not be going well. It also encourages you to prioritize visualizing your fit and diagnosing whether the process\n",
        "is working correctly.\n",
        "\n",
        "We highly recommend users starting a new model-fitting problem begin by setting up on-the-fly output!\n",
        "\n",
        "__Hard Disk Output__\n",
        "\n",
        "By default, a non-linear search does not save its results to the hard disk; the results can only be inspected in \n",
        "a Jupyter Notebook or Python script via the returned `result`.\n",
        "\n",
        "However, you can enable the output of non-linear search results to the hard disk by specifying \n",
        "the `name` and/or `path_prefix` attributes. These attributes determine how files are named and where results \n",
        "are saved on your hard disk.\n",
        "\n",
        "Benefits of saving results to the hard disk include:\n",
        "\n",
        "- More efficient inspection of results for multiple datasets compared to using a Jupyter Notebook.\n",
        "- Results are saved on-the-fly, allowing you to check the progress of a fit midway.\n",
        "- Additional information about a fit, such as visualizations, can be saved (see below).\n",
        "- Unfinished runs can be resumed from where they left off if they are terminated.\n",
        "- On high-performance supercomputers, results often need to be saved in this manner.\n",
        "\n",
        "Here's how to enable the output of results to the hard disk:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Emcee(path_prefix=path.join(\"folder_0\", \"folder_1\"), name=\"my_search_name\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The screenshot below shows the output folder where all output is enabled:\n",
        "\n",
        ".. image:: https://raw.githubusercontent.com/Jammy2211/PyAutoFit/main/docs/overview/image/output_example.png\n",
        "  :width: 400\n",
        "  :alt: Alternative text\n",
        "\n",
        "Let's break down the output folder generated by **PyAutoFit**:\n",
        "\n",
        "- **Unique Identifier**: Results are saved in a folder named with a unique identifier composed of random characters. \n",
        "  This identifier is automatically generated based on the specific model fit. For scientific workflows involving \n",
        "  numerous model fits, this ensures that each fit is uniquely identified without requiring manual updates to output paths.\n",
        "\n",
        "- **Info Files**: These files contain valuable information about the fit. For instance, `model.info` provides the \n",
        "  complete model composition used in the fit, while `search.summary` details how long the search has been running \n",
        "  and other relevant search-specific information.\n",
        "\n",
        "- **Files Folder**: Within the output folder, the `files` directory contains detailed information saved as `.json` \n",
        "  files. For example, `model.json` stores the model configuration used in the fit. This enables researchers to \n",
        "  revisit the results later and review how the fit was performed.\n",
        "\n",
        "**PyAutoFit** offers extensive tools for customizing hard-disk output. This includes using configuration files to \n",
        "control what information is saved, which helps manage disk space utilization. Additionally, specific `.json` files \n",
        "tailored to different models can be utilized for more detailed output.\n",
        "\n",
        "For many scientific workflows, having detailed output for each fit is crucial for thorough inspection and accurate\n",
        "interpretation of results. However, in scenarios where the volume of output data might overwhelm users or impede\n",
        "scientific study, this feature can be easily disabled by omitting the `name` or `path prefix` when initiating the search.\n",
        "\n",
        "__Visualization__\n",
        "\n",
        "When search hard-disk output is enabled in **PyAutoFit**, the visualization of model fits can also be saved directly\n",
        "to disk. This capability is crucial for many scientific workflows as it allows for quick and effective assessment of\n",
        "fit quality.\n",
        "\n",
        "To accomplish this, you can customize the `Visualizer` object of an `Analysis` class with a custom `Visualizer` class.\n",
        "This custom class is responsible for generating and saving visual representations of the model fits. By leveraging\n",
        "this approach, scientists can efficiently visualize and analyze the outcomes of model fitting processes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Visualizer(af.Visualizer):\n",
        "    @staticmethod\n",
        "    def visualize_before_fit(\n",
        "        analysis, paths: af.DirectoryPaths, model: af.AbstractPriorModel\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Before a model-fit, the `visualize_before_fit` method is called to perform visualization.\n",
        "\n",
        "        The function receives as input an instance of the `Analysis` class which is being used to perform the fit,\n",
        "        which is used to perform the visualization (e.g. it contains the data and noise map which are plotted).\n",
        "\n",
        "        This can output visualization of quantities which do not change during the model-fit, for example the\n",
        "        data and noise-map.\n",
        "\n",
        "        The `paths` object contains the path to the folder where the visualization should be output, which is determined\n",
        "        by the non-linear search `name` and other inputs.\n",
        "        \"\"\"\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        xvalues = np.arange(analysis.data.shape[0])\n",
        "\n",
        "        plt.errorbar(\n",
        "            x=xvalues,\n",
        "            y=analysis.data,\n",
        "            yerr=analysis.noise_map,\n",
        "            color=\"k\",\n",
        "            ecolor=\"k\",\n",
        "            elinewidth=1,\n",
        "            capsize=2,\n",
        "        )\n",
        "        plt.title(\"Maximum Likelihood Fit\")\n",
        "        plt.xlabel(\"x value of profile\")\n",
        "        plt.ylabel(\"Profile Normalization\")\n",
        "        plt.savefig(path.join(paths.image_path, f\"data.png\"))\n",
        "        plt.clf()\n",
        "\n",
        "    @staticmethod\n",
        "    def visualize(analysis, paths: af.DirectoryPaths, instance, during_analysis):\n",
        "        \"\"\"\n",
        "        During a model-fit, the `visualize` method is called throughout the non-linear search.\n",
        "\n",
        "        The function receives as input an instance of the `Analysis` class which is being used to perform the fit,\n",
        "        which is used to perform the visualization (e.g. it generates the model data which is plotted).\n",
        "\n",
        "        The `instance` passed into the visualize method is maximum log likelihood solution obtained by the model-fit\n",
        "        so far and it can be used to provide on-the-fly images showing how the model-fit is going.\n",
        "\n",
        "        The `paths` object contains the path to the folder where the visualization should be output, which is determined\n",
        "        by the non-linear search `name` and other inputs.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(analysis.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_from(xvalues=xvalues)\n",
        "        residual_map = analysis.data - model_data\n",
        "\n",
        "        \"\"\"\n",
        "        The visualizer now outputs images of the best-fit results to hard-disk (checkout `visualizer.py`).\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        plt.errorbar(\n",
        "            x=xvalues,\n",
        "            y=analysis.data,\n",
        "            yerr=analysis.noise_map,\n",
        "            color=\"k\",\n",
        "            ecolor=\"k\",\n",
        "            elinewidth=1,\n",
        "            capsize=2,\n",
        "        )\n",
        "        plt.plot(xvalues, model_data, color=\"r\")\n",
        "        plt.title(\"Maximum Likelihood Fit\")\n",
        "        plt.xlabel(\"x value of profile\")\n",
        "        plt.ylabel(\"Profile Normalization\")\n",
        "        plt.savefig(path.join(paths.image_path, f\"model_fit.png\"))\n",
        "        plt.clf()\n",
        "\n",
        "        plt.errorbar(\n",
        "            x=xvalues,\n",
        "            y=residual_map,\n",
        "            yerr=analysis.noise_map,\n",
        "            color=\"k\",\n",
        "            ecolor=\"k\",\n",
        "            elinewidth=1,\n",
        "            capsize=2,\n",
        "        )\n",
        "        plt.title(\"Residuals of Maximum Likelihood Fit\")\n",
        "        plt.xlabel(\"x value of profile\")\n",
        "        plt.ylabel(\"Residual\")\n",
        "        plt.savefig(path.join(paths.image_path, f\"model_fit.png\"))\n",
        "        plt.clf()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``Analysis`` class is defined following the same API as before, but now with its `Visualizer` class attribute\n",
        "overwritten with the ``Visualizer`` class above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "\n",
        "    \"\"\"\n",
        "    This over-write means the `Visualizer` class is used for visualization throughout the model-fit.\n",
        "\n",
        "    This `VisualizerExample` object is in the `autofit.example.visualize` module and is used to customize the\n",
        "    plots output during the model-fit.\n",
        "\n",
        "    It has been extended with visualize methods that output visuals specific to the fitting of `1D` data.\n",
        "    \"\"\"\n",
        "\n",
        "    Visualizer = Visualizer\n",
        "\n",
        "    def __init__(self, data, noise_map):\n",
        "        \"\"\"\n",
        "        An Analysis class which illustrates visualization.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        The `log_likelihood_function` is identical to the example above\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_from(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualization of the results of the non-linear search, for example the \"Probability Density\n",
        "Function\", are also automatically output during the model-fit on the fly.\n",
        "\n",
        "We now perform a quick fit, outputting the results to the hard disk and visualizing the model-fit,\n",
        "so you can see how the results are output and the visualizations produced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "model = af.Model(af.Gaussian)\n",
        "\n",
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"result_folder\"), name=\"overview_2_scientific_workflow\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Loading Results__\n",
        "\n",
        "In your scientific workflow, you'll likely conduct numerous model fits, each generating outputs stored in individual\n",
        "folders on your hard disk.\n",
        "\n",
        "To efficiently work with these results in Python scripts or Jupyter notebooks, **PyAutoFit** provides\n",
        "the `aggregator` API. This tool simplifies the process of loading results from hard disk into Python variables.\n",
        "By pointing the aggregator at the folder containing your results, it automatically loads all relevant information\n",
        "from each model fit.\n",
        "\n",
        "This capability streamlines the workflow by enabling easy manipulation and inspection of model-fit results directly\n",
        "within your Python environment. It's particularly useful for managing and analyzing large-scale studies where\n",
        "handling multiple model fits and their associated outputs is essential."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.aggregator.aggregator import Aggregator\n",
        "\n",
        "agg = Aggregator.from_directory(\n",
        "    directory=path.join(\"result_folder\"),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``values`` method is used to specify the information that is loaded from the hard-disk, for example the\n",
        "``samples`` of the model-fit.\n",
        "\n",
        "The for loop below iterates over all results in the folder passed to the aggregator above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Result loading uses Python generators to ensure that memory use is minimized, meaning that even when loading\n",
        "thousands of results from hard-disk the memory use of your machine is not exceeded.\n",
        "\n",
        "The `result cookbook <https://pyautofit.readthedocs.io/en/latest/cookbooks/model.html>`_ gives a full run-through of\n",
        "the tools that allow results to be loaded and inspected.\n",
        "\n",
        "__Result Customization__\n",
        "\n",
        "An effective scientific workflow ensures that this object contains all information a user needs to quickly inspect\n",
        "the quality of a model-fit and undertake scientific interpretation.\n",
        "\n",
        "The result can be can be customized to include additional information about the model-fit that is specific to your\n",
        "model-fitting problem.\n",
        "\n",
        "For example, for fitting 1D profiles, the ``Result`` could include the maximum log likelihood model 1D data,\n",
        "which would enable the following code to be used after the model-fit:\n",
        "\n",
        "print(result.max_log_likelihood_model_data_1d)\n",
        "\n",
        "To do this we use the custom result API, where we first define a custom ``Result`` class which includes the\n",
        "property ``max_log_likelihood_model_data_1d``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class ResultExample(af.Result):\n",
        "    @property\n",
        "    def max_log_likelihood_model_data_1d(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns the maximum log likelihood model's 1D model data.\n",
        "\n",
        "        This is an example of how we can pass the `Analysis` class a custom `Result` object and extend this result\n",
        "        object with new properties that are specific to the model-fit we are performing.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.analysis.data.shape[0])\n",
        "\n",
        "        return self.instance.model_data_from(xvalues=xvalues)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The custom result has access to the analysis class, meaning that we can use any of its methods or properties to\n",
        "compute custom result properties.\n",
        "\n",
        "To make it so that the ``ResultExample`` object above is returned by the search we overwrite the ``Result`` class attribute\n",
        "of the ``Analysis`` and define a ``make_result`` object describing what we want it to contain:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "\n",
        "    \"\"\"\n",
        "    This overwrite means the `ResultExample` class is returned after the model-fit.\n",
        "    \"\"\"\n",
        "\n",
        "    Result = ResultExample\n",
        "\n",
        "    def __init__(self, data, noise_map):\n",
        "        \"\"\"\n",
        "        An Analysis class which illustrates custom results.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        The `log_likelihood_function` is identical to the example above\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_from(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def make_result(\n",
        "        self,\n",
        "        samples_summary: af.SamplesSummary,\n",
        "        paths: af.AbstractPaths,\n",
        "        samples: Optional[af.SamplesPDF] = None,\n",
        "        search_internal: Optional[object] = None,\n",
        "        analysis: Optional[object] = None,\n",
        "    ) -> Result:\n",
        "        \"\"\"\n",
        "        Returns the `Result` of the non-linear search after it is completed.\n",
        "\n",
        "        The result type is defined as a class variable in the `Analysis` class (see top of code under the python code\n",
        "        `class Analysis(af.Analysis)`.\n",
        "\n",
        "        The result can be manually overwritten by a user to return a user-defined result object, which can be extended\n",
        "        with additional methods and attribute specific to the model-fit.\n",
        "\n",
        "        This example class does example this, whereby the analysis result has been overwritten with the `ResultExample`\n",
        "        class, which contains a property `max_log_likelihood_model_data_1d` that returns the model data of the\n",
        "        best-fit model. This API means you can customize your result object to include whatever attributes you want\n",
        "        and therefore make a result object specific to your model-fit and model-fitting problem.\n",
        "\n",
        "        The `Result` object you return can be customized to include:\n",
        "\n",
        "        - The samples summary, which contains the maximum log likelihood instance and median PDF model.\n",
        "\n",
        "        - The paths of the search, which are used for loading the samples and search internal below when a search\n",
        "        is resumed.\n",
        "\n",
        "        - The samples of the non-linear search (e.g. MCMC chains) also stored in `samples.csv`.\n",
        "\n",
        "        - The non-linear search used for the fit in its internal representation, which is used for resuming a search\n",
        "        and making bespoke visualization using the search's internal results.\n",
        "\n",
        "        - The analysis used to fit the model (default disabled to save memory, but option may be useful for certain\n",
        "        projects).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        samples_summary\n",
        "            The summary of the samples of the non-linear search, which include the maximum log likelihood instance and\n",
        "            median PDF model.\n",
        "        paths\n",
        "            An object describing the paths for saving data (e.g. hard-disk directories or entries in sqlite database).\n",
        "        samples\n",
        "            The samples of the non-linear search, for example the chains of an MCMC run.\n",
        "        search_internal\n",
        "            The internal representation of the non-linear search used to perform the model-fit.\n",
        "        analysis\n",
        "            The analysis used to fit the model.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Result\n",
        "            The result of the non-linear search, which is defined as a class variable in the `Analysis` class.\n",
        "        \"\"\"\n",
        "        return self.Result(\n",
        "            samples_summary=samples_summary,\n",
        "            paths=paths,\n",
        "            samples=samples,\n",
        "            search_internal=search_internal,\n",
        "            analysis=self,\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By repeating the model-fit above, the `Result` object returned by the search will be an instance of the `ResultExample`\n",
        "class, which includes the property `max_log_likelihood_model_data_1d`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "model = af.Model(af.Gaussian)\n",
        "\n",
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"output\", \"result_folder\"),\n",
        "    name=\"overview_2_scientific_workflow\",\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(result.max_log_likelihood_model_data_1d)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Result customization has full support for **latent variables**, which are parameters that are not sampled by the non-linear\n",
        "search but are computed from the sampled parameters.\n",
        "\n",
        "They are often integral to assessing and interpreting the results of a model-fit, as they present information\n",
        "on the model in a different way to the sampled parameters.\n",
        "\n",
        "The `result cookbook <https://pyautofit.readthedocs.io/en/latest/cookbooks/result.html>`_ gives a full run-through of\n",
        "all the different ways the result can be customized.\n",
        "\n",
        "__Model Composition__\n",
        "\n",
        "In many scientific workflows, there's often a need to construct and fit a variety of different models. This\n",
        "could range from making minor adjustments to a model's parameters to handling complex models with thousands of parameters and multiple components.\n",
        "\n",
        "For simpler scenarios, adjustments might include:\n",
        "\n",
        "- **Parameter Assignment**: Setting specific values for certain parameters or linking parameters together so they share the same value.\n",
        "- **Parameter Assertions**: Imposing constraints on model parameters, such as requiring one parameter to be greater than another.\n",
        "- **Model Arithmetic**: Defining relationships between parameters using arithmetic operations, such as defining a \n",
        "  linear relationship like `y = mx + c`, where `m` and `c` are model parameters.\n",
        "\n",
        "In more intricate cases, models might involve numerous parameters and complex compositions of multiple model components.\n",
        "\n",
        "**PyAutoFit** offers a sophisticated model composition API designed to handle these complexities. It provides\n",
        "tools for constructing elaborate models using lists of Python classes and hierarchical structures of Python classes.\n",
        "\n",
        "For a detailed exploration of these capabilities, you can refer to\n",
        "the `model cookbook <https://pyautofit.readthedocs.io/en/latest/cookbooks/model.html>`_, which provides comprehensive\n",
        "guidance on using the model composition API. This resource covers everything from basic parameter assignments to\n",
        "constructing complex models with hierarchical structures.\n",
        "\n",
        "__Searches__\n",
        "\n",
        "Different model-fitting problems often require different approaches to fitting the model effectively.\n",
        "\n",
        "The choice of the most suitable search method depends on several factors:\n",
        "\n",
        "- **Model Dimensions**: How many parameters constitute the model and its non-linear parameter space?\n",
        "- **Model Complexity**: Different models exhibit varying degrees of parameter degeneracy, which necessitates different \n",
        "  non-linear search techniques.\n",
        "- **Run Times**: How efficiently can the likelihood function be evaluated and the model-fit performed?\n",
        "- **Gradients**: If your likelihood function is differentiable, leveraging JAX and using a search that exploits \n",
        "  gradient information can be advantageous.\n",
        "\n",
        "**PyAutoFit** provides support for a wide range of non-linear searches, ensuring that users can select the method\n",
        "best suited to their specific problem.\n",
        "\n",
        "During the initial stages of setting up your scientific workflow, it's beneficial to experiment with different\n",
        "searches. This process helps identify which methods reliably infer maximum likelihood fits to the data and assess\n",
        "their efficiency in terms of computational time.\n",
        "\n",
        "For a comprehensive exploration of available search methods and customization options, refer to\n",
        "the `search cookbook <https://pyautofit.readthedocs.io/en/latest/cookbooks/search.html>`_. This resource covers\n",
        "detailed guides on all non-linear searches supported by PyAutoFit and provides insights into how to tailor them to your \n",
        "needs.\n",
        "\n",
        "There are currently no documentation guiding reads on what search might be appropriate for their problem and how to\n",
        "profile and experiment with different methods. Writing such documentation is on the to do list and will appear\n",
        "in the future. However, you can make progress now simply using visuals output by PyAutoFit and the ``search.summary` file.\n",
        "\n",
        "__Configs__\n",
        "\n",
        "As you refine your scientific workflow, you'll often find yourself repeatedly setting up models with identical priors\n",
        "and using the same non-linear search configurations. This repetition can result in lengthy Python scripts with\n",
        "redundant inputs.\n",
        "\n",
        "To streamline this process, configuration files can be utilized to define default values. This approach eliminates\n",
        "the need to specify identical prior inputs and search settings in every script, leading to more concise and\n",
        "readable Python code. Moreover, it reduces the cognitive load associated with performing model-fitting tasks.\n",
        "\n",
        "For a comprehensive guide on setting up and utilizing configuration files effectively, refer\n",
        "to the `configs cookbook <https://pyautofit.readthedocs.io/en/latest/cookbooks/configs.html>`_. This resource provides\n",
        "detailed instructions on configuring and optimizing your PyAutoFit workflow through the use of configuration files.\n",
        "\n",
        "__Database__\n",
        "\n",
        "By default, model-fitting results are written to folders on hard-disk, which is straightforward for navigating and\n",
        "manual inspection. However, this approach becomes impractical for large datasets or extensive scientific workflows,\n",
        "where manually checking each result can be time-consuming.\n",
        "\n",
        "To address this challenge, all results can be stored in an sqlite3 relational database. This enables loading results\n",
        "directly into Jupyter notebooks or Python scripts for inspection, analysis, and interpretation. The database\n",
        "supports advanced querying capabilities, allowing users to retrieve specific model-fits based on criteria such\n",
        "as the fitted model or dataset.\n",
        "\n",
        "For a comprehensive guide on using the database functionality within PyAutoFit, refer to\n",
        "the `database cookbook <https://pyautofit.readthedocs.io/en/latest/cookbooks/multiple_datasets.html>`. This resource\n",
        "provides detailed instructions on leveraging the database to manage and analyze model-fitting results efficiently.\n",
        "\n",
        "__Scaling Up__\n",
        "\n",
        "Regardless of your final scientific objective, it's crucial to consider scalability in your scientific workflow and\n",
        "ensure it remains flexible to accommodate varying scales of complexity.\n",
        "\n",
        "Initially, scientific studies often begin with a small number of datasets (e.g., tens of datasets). During this phase,\n",
        "researchers iteratively refine their models and gain insights through trial and error. This involves fitting numerous\n",
        "models to datasets and manually inspecting results to evaluate model performance. A flexible workflow is essential\n",
        "here, allowing rapid iteration and outputting results in a format that facilitates quick inspection and interpretation.\n",
        "\n",
        "As the study progresses, researchers may scale up to larger datasets (e.g., thousands of datasets). Manual inspection\n",
        "of individual results becomes impractical, necessitating a more automated approach to model fitting and interpretation.\n",
        "Additionally, analyses may transition to high-performance computing environments, requiring output formats suitable for \n",
        "these setups.\n",
        "\n",
        "**PyAutoFit** is designed to enable the development of effective scientific workflows for both small and large datasets.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "This overview has provided a comprehensive guide to the key features of **PyAutoFit** that support the development of\n",
        "effective scientific workflows. By leveraging these tools, researchers can tailor their workflows to specific problems,\n",
        "streamline model fitting, and gain valuable insights into their scientific studies.\n",
        "\n",
        "The final aspect of core functionality, described in the next overview, is the wide variety of statistical\n",
        "inference methods available in **PyAutoFit**. These methods include graphical models, hierarchical models,\n",
        "Bayesian model comparison and many more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}