{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example: Fit__\n",
        "\n",
        "In this example, we'll fit 1D data of a `Gaussian` profile with a 1D `Gaussian` model using the non-linear searches\n",
        "emcee, Dynesty and PySwarms.\n",
        "\n",
        "If you haven't already, you should checkout the files `example/model.py` and `example/analysis.py` to see how we have\n",
        "provided PyAutoFit with the necessary information on our model, data and log likelihood function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from os import path"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Load data of a 1D Gaussian from a .json file in the directory \n",
        "`autofit_workspace/dataset//gaussian_x1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets plot the data. We'll use its shape to determine the xvalues of the\n",
        "data for the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = range(data.shape[0])\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We now need to create our model of a 1D Gaussian, which is the model we will fit to the dataset above.\n",
        "\n",
        "A model component is written as a Python class in *PyAutoFit** using the following format:\n",
        "\n",
        " - The name of the class is the name of the model component, in this case, \u201cGaussian\u201d.\n",
        "\n",
        " - The input arguments of the constructor are the parameters of the mode (here centre, normalization and sigma).\n",
        "\n",
        " - The default values of the input arguments tell PyAutoFit whether a parameter is a single-valued float or a\n",
        " multi-valued tuple.\n",
        "\n",
        "Below, we define a 1D Gaussian model component, which is used throughout the **PyAutoFit** workspace to perform\n",
        "example model fits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=0.0,  # <- PyAutoFit recognises these constructor arguments\n",
        "        normalization=0.1,  # <- are the Gaussian`s model parameters.\n",
        "        sigma=0.01,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Represents a 1D `Gaussian` profile, which may be treated as a model-component of PyAutoFit the\n",
        "        parameters of which are fitted for by a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization of the `Gaussian` profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues):\n",
        "        \"\"\"\n",
        "        Calculate the 1D Gaussian profile on a line of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, using its centre.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues\n",
        "            The x coordinates in the original reference frame of the grid.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = xvalues - self.centre\n",
        "\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can convert a Python class written using the format above to a model-component via the `af.Model()` object.\n",
        "\n",
        "This inspects the `__init__` constructor defined above, to determine that this model-component has 3 free\n",
        "parameters (`centre`, `normalization`, `sigma`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can check it has 3 free parameters via the `prior_count` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Model Prior Count = {model.prior_count}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, the priors associated with every parameter of the model are loaded from a user-defined configuration file.\n",
        "\n",
        "For the `Gaussian` above, these priros are contained in the file `autofit_workspace/config/priors/model.json`, which\n",
        "you can checkout now to see the priors.\n",
        "\n",
        "We can print the priors via the model's `info` attribute.\n",
        "\n",
        "[The `info` below may not display optimally on your computer screen, for example the whitespace between parameter\n",
        "names on the left and parameter priors on the right may lead them to appear across multiple lines. This is a\n",
        "common issue in Jupyter notebooks.\n",
        "\n",
        "The`info_whitespace_length` parameter in the file `config/general.yaml` in the [output] section can be changed to \n",
        "increase or decrease the amount of whitespace (The Jupyter notebook kernel will need to be reset for this change to \n",
        "appear in a notebook).]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can overwrite priors before fitting the model to data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.normalization = af.LogUniformPrior(lower_limit=1e-2, upper_limit=1e2)\n",
        "model.sigma = af.GaussianPrior(\n",
        "    mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the priors on parameters have been overwritten by those specified above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We now set up our Analysis using an `Analysis` object, where:\n",
        "\n",
        " - The `__init__` constructor of the `Analysis` object contains the data, noise-map and any other quantities necessary\n",
        " to fit the model to the data.\n",
        "\n",
        " - The `log_likelihood_function` defines how given an instance of our model (a Gaussian) we fit the data and return a \n",
        " log likelihood value.\n",
        " \n",
        " - The `visualize` function outputs visuals to hard-disk during and at the end of the model-fit.\n",
        "   \n",
        "Read the comments and docstrings of the `Analysis` object below in detail for more insights into how this object\n",
        "works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "        \"\"\"\n",
        "        In this example the `Analysis` object only contains the data and noise-map. It can be easily extended,\n",
        "        for more complex data-sets and model fitting problems.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data\n",
        "            A 1D numpy array containing the data (e.g. a noisy 1D Gaussian) fitted in the workspace examples.\n",
        "        noise_map\n",
        "            A 1D numpy array containing the noise values of the data, used for computing the goodness of fit\n",
        "            metric.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    \"\"\"\n",
        "    In the log_likelihood_function function below, `instance` is an instance of our model, which in this example is\n",
        "    an instance of the `Gaussian` class in `model.py`. The parameters of the `Gaussian` are set via the non-linear\n",
        "    search. This gives us the instance of our model we need to fit our data!\n",
        "    \"\"\"\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        Determine the log likelihood of a fit of a `Gaussian` to the dataset, using a model instance of the Gaussian.\n",
        "\n",
        "        This function will be replaced with calculations specific to your model-fitting problem.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance\n",
        "        An instance of the `Gaussian` model, where the parameters have been set via the non-linear search of the\n",
        "        model-fit.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        fit\n",
        "            The log likelihood value indicating how well this model fit the dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\"\n",
        "        The `instance` that comes into this method is an instance of the `Gaussian` class. To convince yourself of \n",
        "        this, go ahead and uncomment the lines below and run the non-linear search.\n",
        "        \"\"\"\n",
        "        # print(\"Gaussian Instance:\")\n",
        "        # print(\"Centre = \", instance.centre)\n",
        "        # print(\"Normalization = \", instance.normalization)\n",
        "        # print(\"Sigma = \", instance.sigma)\n",
        "\n",
        "        \"\"\"\n",
        "        Get the range of x-values the data is defined on, to evaluate the model of the Gaussian.\n",
        "        \"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        \"\"\"\n",
        "        Use these xvalues to create model data of our Gaussian.\n",
        "        \"\"\"\n",
        "        model_data = instance.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "\n",
        "        \"\"\"\n",
        "        Fit the model gaussian line data to the observed data, computing the residuals and chi-squared.\n",
        "        \"\"\"\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        log_likelihood = -0.5 * sum(chi_squared_map)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def visualize(self, paths, instance, during_analysis):\n",
        "        \"\"\"\n",
        "        During a model-fit, the `visualize` method is called throughout the non-linear search and is used to output\n",
        "        images indicating the quality of the fit so far..\n",
        "\n",
        "        The `instance` passed into the visualize method is maximum log likelihood solution obtained by the model-fit\n",
        "        so far and it can be used to provide on-the-fly images showing how the model-fit is going.\n",
        "\n",
        "        For your model-fitting problem this function will be overwritten with plotting functions specific to your\n",
        "        problem.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        paths\n",
        "            The PyAutoFit paths object which manages all paths, e.g. where the non-linear search outputs are stored,\n",
        "            visualization, and the pickled objects used by the aggregator output by this function.\n",
        "        instance\n",
        "            An instance of the model that is being fitted to the data by this analysis (whose parameters have been set\n",
        "            via a non-linear search).\n",
        "        during_analysis\n",
        "            If True the visualization is being performed midway through the non-linear search before it is finished,\n",
        "            which may change which images are output.\n",
        "        \"\"\"\n",
        "\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "\n",
        "        plt.errorbar(\n",
        "            x=xvalues,\n",
        "            y=self.data,\n",
        "            yerr=self.noise_map,\n",
        "            color=\"k\",\n",
        "            ecolor=\"k\",\n",
        "            elinewidth=1,\n",
        "            capsize=2,\n",
        "        )\n",
        "        plt.plot(xvalues, model_data, color=\"r\")\n",
        "        plt.title(\"Model fit to 1D Gaussian dataset.\")\n",
        "        plt.xlabel(\"x values of profile\")\n",
        "        plt.ylabel(\"Profile normalization\")\n",
        "\n",
        "        os.makedirs(paths.image_path, exist_ok=True)\n",
        "        plt.savefig(path.join(paths.image_path, \"model_fit.png\"))\n",
        "        plt.clf()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have defined out `Analysis` object we create an instance of it to perform our model-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#############################\n",
        "###### NESTED SAMPLING ######\n",
        "#############################\n",
        "\n",
        "We now set up our non-linear search. We'll first fit the data with the nested sampling algorithm\n",
        "Dynesty. Below, we manually specify all of the Dynesty settings, however if we omitted them the default values\n",
        "found in the config file `config/non_linear/Dynesty.yaml` would be used.\n",
        "\n",
        "For a full description of Dynesty checkout its Github and documentation webpages:\n",
        "\n",
        "https://github.com/joshspeagle/dynesty\n",
        "https://dynesty.readthedocs.io/en/latest/index.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    nlive=100,\n",
        "    bound=\"multi\",\n",
        "    sample=\"auto\",\n",
        "    bootstrap=None,\n",
        "    enlarge=None,\n",
        "    update_interval=None,\n",
        "    walks=25,\n",
        "    facc=0.5,\n",
        "    slices=5,\n",
        "    fmove=0.9,\n",
        "    max_move=100,\n",
        "    number_of_cores=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform the fit with Dynesty, we pass it our model and analysis and we`re good to go!\n",
        "\n",
        "Checkout the folder `autofit_workspace/output/DynestyStatic`, where the `NonLinearSearch` results, visualization and\n",
        "information can be found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result object returned by the fit provides information on the results of the non-linear search. \n",
        "\n",
        "The `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can inspect the result's maximum likelihood instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.max_log_likelihood_instance)\n",
        "\n",
        "print(\"\\n Model-fit Max Log-likelihood Parameter Estimates: \\n\")\n",
        "print(\"Centre = \", result.max_log_likelihood_instance.centre)\n",
        "print(\"Normalization = \", result.max_log_likelihood_instance.normalization)\n",
        "print(\"Sigma = \", result.max_log_likelihood_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the result's maximum likelihood instance to compare the maximum log likelihood `Gaussian` to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = result.max_log_likelihood_instance.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"Dynesty model fit to 1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Probability Density Functions of the results can be plotted using Dynesty's in-built visualization tools, \n",
        "which are wrapped via the `DynestyPlotter` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_plotter = aplt.DynestyPlotter(samples=result.samples)\n",
        "search_plotter.cornerplot()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output to Hard Disk__\n",
        "\n",
        "The non-linear search `dynesty` above did not output results to hard-disk, which for quick model-fits and\n",
        "experimenting with different models is desirable.\n",
        "\n",
        "For many problems it is preferable for all results to be written to hard-disk. The benefits of doing this include:\n",
        "\n",
        "- Inspecting results in an ordered directory structure can be more efficient than using a Jupyter Notebook.\n",
        "- Results can be output on-the-fly, to check that a fit is progressing as expected mid way through.\n",
        "- An unfinished run can be resumed where it was terminated.\n",
        "- Additional information about a fit (e.g. visualization) can be output.\n",
        "- On high performance computers which use a batch system, this is the only way to transfer results.\n",
        "\n",
        "Any model-fit performed by **PyAutoFit** can be saved to hard-disk, by simply giving the non-linear search a \n",
        "`name`. A `path_prefix` can optionally be input to customize the output directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"overview\", \"simple\"),\n",
        "    name=\"DynestyStatic\",\n",
        "    nlive=100,\n",
        "    bound=\"multi\",\n",
        "    sample=\"auto\",\n",
        "    bootstrap=None,\n",
        "    enlarge=None,\n",
        "    update_interval=None,\n",
        "    walks=25,\n",
        "    facc=0.5,\n",
        "    slices=5,\n",
        "    fmove=0.9,\n",
        "    max_move=100,\n",
        "    number_of_cores=1,\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the `autofit_workspace/output` directory you should find a folder containing the results of the model-fit\n",
        "for inspection, including text files containing the `model.info`, `results.info` and other information.\n",
        "\n",
        "The results are in a folder which is a collection of random characters. This is the 'unique_identifier' of\n",
        "the model-fit. This identifier is generated based on the model fitted and search used, such that an identical\n",
        "combination of model and search generates the same identifier. \n",
        "\n",
        "This ensures that rerunning an identical fit will use the existing results to resume the model-fit. In contrast, if\n",
        "you change the model or search, a new unique identifier will be generated, ensuring that the model-fit results are\n",
        "output into a separate folder.\n",
        "\n",
        "We discuss in more detail how to use a results object in the files `autofit_workspace/example/simple/result.py`.\n",
        "\n",
        "##################\n",
        "###### MCMC ######\n",
        "##################\n",
        "\n",
        "To use a different non-linear we simply use call a different search from PyAutoFit, passing it the same the model\n",
        "and analysis as we did before to perform the fit. Below, we fit the same dataset using the MCMC sampler Emcee.\n",
        "Again, we manually specify all of the Emcee settings, however if they were omitted the values found in the config\n",
        "file `config/non_linear/Emcee.yaml` would be used instead.\n",
        "\n",
        "For a full description of Emcee, checkout its Github and readthedocs webpages:\n",
        "\n",
        "https://github.com/dfm/emcee\n",
        "https://emcee.readthedocs.io/en/stable/\n",
        "\n",
        "**PyAutoFit** extends **emcee** by providing an option to check the auto-correlation length of the samples\n",
        "during the run and terminating sampling early if these meet a specified threshold. See this page\n",
        "(https://emcee.readthedocs.io/en/stable/tutorials/autocorr/#autocorr) for a description of how this is implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Emcee(\n",
        "    path_prefix=path.join(\"overview\", \"simple\"),\n",
        "    name=\"Emcee\",\n",
        "    nwalkers=30,\n",
        "    nsteps=1000,\n",
        "    initializer=af.InitializerBall(lower_limit=0.49, upper_limit=0.51),\n",
        "    auto_correlations_settings=af.AutoCorrelationsSettings(\n",
        "        check_for_convergence=True,\n",
        "        check_size=100,\n",
        "        required_length=50,\n",
        "        change_threshold=0.01,\n",
        "    ),\n",
        "    number_of_cores=1,\n",
        ")\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result object returned by Emcee`s fit is similar in structure to the Dynesty result above.\n",
        " \n",
        "Printing its `info` shows that it does not estimate a Bayesian evidence, which `dynesty` above did, because it is an\n",
        "MCMC algoirithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It again provides us with the maximum log likelihood instance which we can use to visualize the fit to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = result.max_log_likelihood_instance.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"Emcee model fit to 1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Probability Density Functions (PDF's) of the results can be plotted using the Emcee's visualization \n",
        "tool `corner.py`, which is wrapped via the `EmceePlotter` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_plotter = aplt.EmceePlotter(samples=result.samples)\n",
        "search_plotter.corner()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "############################\n",
        "###### PARTICLE SWARM ######\n",
        "############################\n",
        "\n",
        "PyAutoFit also supports a number of searches, which seem to find the global (or local) maxima likelihood solution.\n",
        "Unlike nested samplers and MCMC algorithms, they do not extensive map out parameter space. This means they can find the\n",
        "best solution a lot faster than these algorithms, but they do not properly quantify the errors on each parameter.\n",
        "\n",
        "we'll use the Particle Swarm Optimization algorithm PySwarms. For a full description of PySwarms, checkout its Github \n",
        "and readthedocs webpages:\n",
        "\n",
        "https://github.com/ljvmiranda921/pyswarms\n",
        "https://pyswarms.readthedocs.io/en/latest/index.html\n",
        "\n",
        "**PyAutoFit** extends *PySwarms* by allowing runs to be terminated and resumed from the point of termination, as well\n",
        "as providing different options for the initial distribution of particles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.PySwarmsGlobal(\n",
        "    path_prefix=path.join(\"overview\", \"simple\"),\n",
        "    name=\"PySwarmsGlobal\",\n",
        "    n_particles=50,\n",
        "    iters=100,\n",
        "    cognitive=0.5,\n",
        "    social=0.3,\n",
        "    inertia=0.9,\n",
        "    ftol=-np.inf,\n",
        "    initializer=af.InitializerPrior(),\n",
        "    number_of_cores=1,\n",
        ")\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result object returned by PSO is again very similar in structure to previous results.\n",
        "\n",
        "The result info shows that the PSO does not estimate errors on the parameters, because it is a a maximum likelihood\n",
        "estimator (MLE)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It again provides us with the maximum log likelihood instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = result.max_log_likelihood_instance.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(xvalues, model_data, color=\"r\")\n",
        "plt.title(\"PySwarms model fit to 1D Gaussian dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results can be plotted using the PySwarm's in-built visualization tools which are wrapped via \n",
        "the `PySwarmsPlotter` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pyswarms_plotter = aplt.PySwarmsPlotter(samples=result.samples)\n",
        "pyswarms_plotter.cost_history()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Other Samplers__\n",
        "\n",
        "Checkout https://pyautofit.readthedocs.io/en/latest/api/api.html for the non-linear searches available in PyAutoFit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}