{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example: Fit__\n",
        "\n",
        "In this example, we'll fit 1D data of a `Gaussian` and Exponential profile with a 1D `Gaussian` + Exponential\n",
        "model using the non-linear searches Emcee and Dynesty.\n",
        "\n",
        "If you haven't already, you should checkout the files `overview/simple/fit.ipynb` for a basic introduction to\n",
        "the **PyAutoFit** tools used for model-fitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from os import path"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Load data of a 1D `Gaussian` + 1D Exponential, by loading it from a .json file in the directory \n",
        "`autofit_workspace/dataset//gaussian_x1__exponential_x1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1__exponential_x1\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets plot the data. We'll use its shape to determine the xvalues of the\n",
        "data for the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = range(data.shape[0])\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Next, we create our model, which in this case corresponds to a `Gaussian` + Exponential.\n",
        "\n",
        "We therefore need two classes, one for each model component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=0.0,  # <- PyAutoFit recognises these constructor arguments\n",
        "        normalization=0.1,  # <- are the Gaussian`s model parameters.\n",
        "        sigma=0.01,\n",
        "    ):\n",
        "        \"\"\"Represents a 1D `Gaussian` profile, which may be treated as a model-component of PyAutoFit the\n",
        "        parameters of which are fitted for by a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization normalisation of the `Gaussian` profile.\n",
        "        sigma\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues):\n",
        "        \"\"\"\n",
        "        Calculate the normalization of the profile on a line of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, using its centre.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        values\n",
        "            The x coordinates in the original reference frame of the grid.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.normalization, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n",
        "\n",
        "\n",
        "class Exponential:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=0.0,  # <- PyAutoFit recognises these constructor arguments are the model\n",
        "        normalization=0.1,  # <- parameters of the Exponential.\n",
        "        rate=0.01,\n",
        "    ):\n",
        "        \"\"\"Represents a 1D Exponential profile symmetric about a centre, which may be treated as a model-component\n",
        "        of PyAutoFit the parameters of which are fitted for by a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre\n",
        "            The x coordinate of the profile centre.\n",
        "        normalization\n",
        "            Overall normalization normalisation of the `Gaussian` profile.\n",
        "        ratw\n",
        "            The decay rate controlling has fast the Exponential declines.\n",
        "        \"\"\"\n",
        "\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.rate = rate\n",
        "\n",
        "    def model_data_1d_via_xvalues_from(self, xvalues):\n",
        "        \"\"\"\n",
        "        Calculate the normalization of the profile on a line of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Exponential, using its centre.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        values\n",
        "            The x coordinates in the original reference frame of the grid.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return self.normalization * np.multiply(\n",
        "            self.rate, np.exp(-1.0 * self.rate * abs(transformed_xvalues))\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use a `Model` to create and customize the `Gaussian` and `Exponential` models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian = af.Model(Gaussian)\n",
        "exponential = af.Model(Exponential)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkout `autofit_workspace/config/priors/model.json`, this config file defines the default priors of the `Gaussian` \n",
        "and `Exponential` model components. \n",
        "\n",
        "We can manually customize the priors of the model used by the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "gaussian.normalization = af.UniformPrior(lower_limit=0.0, upper_limit=1e2)\n",
        "gaussian.sigma = af.UniformPrior(lower_limit=0.0, upper_limit=30.0)\n",
        "exponential.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "exponential.normalization = af.UniformPrior(lower_limit=0.0, upper_limit=1e2)\n",
        "exponential.rate = af.UniformPrior(lower_limit=0.0, upper_limit=10.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now compose the overall model using a `Collection`, which takes the model components we defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=gaussian, exponential=exponential)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format, including the priors specified above.\n",
        "\n",
        "[The `info` below may not display optimally on your computer screen, for example the whitespace between parameter\n",
        "names on the left and parameter priors on the right may lead them to appear across multiple lines. This is a\n",
        "common issue in Jupyter notebooks.\n",
        "\n",
        "The`info_whitespace_length` parameter in the file `config/general.yaml` in the [output] section can be changed to \n",
        "increase or decrease the amount of whitespace (The Jupyter notebook kernel will need to be reset for this change to \n",
        "appear in a notebook).]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we named our model-components: we called the `Gaussian` component `gaussian` and Exponential component\n",
        "`exponential`. We could have chosen anything for these names, as shown by the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_custom_names = af.Collection(\n",
        "    custom_name=gaussian, another_custom_name=exponential\n",
        ")\n",
        "\n",
        "print(model_custom_names.custom_name)\n",
        "print(model_custom_names.another_custom_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The naming of model components is important, as these names will are adopted by the instance passed to the `Analysis`\n",
        "class and the results returned by the non-linear search.\n",
        "\n",
        "__Analysis__\n",
        "\n",
        "We now set up our Analysis, which describes how given an instance of our model (a `Gaussian` + Exponential) we fit the\n",
        "data and return a log likelihood value.\n",
        "\n",
        "This behaves analogous to the `Analysis` object used in the simple `fit.py` example, but has changes to deal\n",
        "with the fact that the input model is a `Collection`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "\n",
        "    \"\"\"\n",
        "    In this example the Analysis only contains the data and noise-map. It can be easily extended however, for more\n",
        "    complex data-sets and model fitting problems.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, noise_map):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    \"\"\"\n",
        "    In the log_likelihood_function function below, `instance` is an instance of our model, which in this example is\n",
        "    an instance of the `Gaussian` class and Exponential class in `model.py`. Their parameters are set via the\n",
        "    non-linear search. This gives us the instance of the model we need to fit our data!\n",
        "    \"\"\"\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        Determine the log likelihood of a fit of multiple profiles to the dataset.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance : af.Collection\n",
        "            The model instances of the profiles.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        fit\n",
        "            The log likelihood value indicating how well this model fit the dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        The `instance` that comes into this method is a Collection. It contains instances of every class\n",
        "        we instantiated it with, where each instance is named following the names given to the Collection,\n",
        "        which in this example is a `Gaussian` (with name `gaussian) and Exponential (with name `exponential`):\n",
        "        \"\"\"\n",
        "\n",
        "        # print(\"Gaussian Instance:\")\n",
        "        # print(\"Centre = \", instance.gaussian.centre)\n",
        "        # print(\"Normalization = \", instance.gaussian.normalization)\n",
        "        # print(\"Sigma = \", instance.gaussian.sigma)\n",
        "\n",
        "        # print(\"Exponential Instance:\")\n",
        "        # print(\"Centre = \", instance.exponential.centre)\n",
        "        # print(\"Normalization = \", instance.exponential.normalization)\n",
        "        # print(\"Rate = \", instance.exponential.rate)\n",
        "\n",
        "        \"\"\"Get the range of x-values the data is defined on, to evaluate the model of the profiles.\"\"\"\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        \"\"\"\n",
        "        The simplest way to create the summed profile is to add the profile of each model component. If we\n",
        "        know we are going to fit a `Gaussian` + Exponential we can do the following:\n",
        "\n",
        "            model_data_gaussian = instance.gaussian.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "            model_data_exponential = instance.exponential.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "            model_data = model_data_gaussian + model_data_exponential\n",
        "\n",
        "        However, this does not work if we change our model components. However, the *instance* variable is a list of\n",
        "        our model components. We can iterate over this list, calling their model_data_1d_via_xvalues_from and summing the result\n",
        "        to compute the summed profile of any model.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"Use these xvalues to create model data of our profiles.\"\"\"\n",
        "        model_data = sum(\n",
        "            [line.model_data_1d_via_xvalues_from(xvalues=xvalues) for line in instance]\n",
        "        )\n",
        "\n",
        "        \"\"\"Fit the model profile data to the observed data, computing the residuals and chi-squareds.\"\"\"\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        log_likelihood = -0.5 * sum(chi_squared_map)\n",
        "\n",
        "        return log_likelihood\n",
        "\n",
        "    def visualize(self, paths, instance, during_analysis):\n",
        "        \"\"\"\n",
        "        During a model-fit, the `visualize` method is called throughout the non-linear search. The `instance` passed\n",
        "        into the visualize method is maximum log likelihood solution obtained by the model-fit so far and it can be\n",
        "        used to provide on-the-fly images showing how the model-fit is going.\n",
        "        \"\"\"\n",
        "\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_datas = [\n",
        "            line.model_data_1d_via_xvalues_from(xvalues=xvalues) for line in instance\n",
        "        ]\n",
        "        model_data = sum(model_datas)\n",
        "\n",
        "        plt.errorbar(\n",
        "            x=xvalues,\n",
        "            y=self.data,\n",
        "            yerr=self.noise_map,\n",
        "            color=\"k\",\n",
        "            ecolor=\"k\",\n",
        "            elinewidth=1,\n",
        "            capsize=2,\n",
        "        )\n",
        "        plt.plot(range(self.data.shape[0]), model_data, color=\"r\")\n",
        "        for model_data_individual in model_datas:\n",
        "            plt.plot(range(self.data.shape[0]), model_data_individual, \"--\")\n",
        "        plt.title(\"Dynesty model fit to 1D Gaussian + Exponential dataset.\")\n",
        "        plt.xlabel(\"x values of profile\")\n",
        "        plt.ylabel(\"Profile normalization\")\n",
        "\n",
        "        os.makedirs(paths.image_path, exist_ok=True)\n",
        "        plt.savefig(path.join(paths.image_path, \"model_fit.png\"))\n",
        "        plt.clf()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the analysis as an instance for the model-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=data, noise_map=noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Paths__\n",
        "\n",
        "We specify a `path_prefix` which is passed to the non-linear search below, so that our results go to the \n",
        "folder `autofit_workspace/output/overview/complex`. The search is also given a `name`, which defines the folder\n",
        "results are output too.\n",
        "\n",
        "Results are output to a folder which is a collection of random characters, which is the 'unique_identifier' of\n",
        "the model-fit. This identifier is generated based on the model fitted and search used, such that an identical\n",
        "combination of model and search generates the same identifier.\n",
        "\n",
        "This ensures that rerunning an identical fit will use the existing results to resume the model-fit. In contrast, if\n",
        "you change the model or search, a new unique identifier will be generated, ensuring that the model-fit results are\n",
        "output into a separate folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path_prefix = path.join(\"overview\", \"complex\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#####################\n",
        "###### DYNESTY ######\n",
        "#####################\n",
        "\n",
        "We finally choose and set up our non-linear search. we'll first fit the data with the nested sampling algorithm\n",
        "Dynesty. Below, we manually specify all of the Dynesty settings, however if we omitted them the default values\n",
        "found in the config file `config/non_linear/Dynesty.yaml` would be used.\n",
        "\n",
        "For a full description of Dynesty checkout its Github and documentation webpages:\n",
        "\n",
        "https://github.com/joshspeagle/dynesty\n",
        "https://dynesty.readthedocs.io/en/latest/index.html\n",
        "\n",
        "NOTE: In `autofit_workspace/*/overview/simple/fit.py` we describe how inputting a `name` and `path_prefix` to the\n",
        "non-linear search outputs the results to hard-disk. \n",
        "\n",
        "We do the same below, but checkout that tutorial for an explanation of the benefits of doing this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=path_prefix,\n",
        "    name=\"DynestyStatic\",\n",
        "    nlive=60,\n",
        "    bound=\"multi\",\n",
        "    sample=\"rwalk\",\n",
        "    bootstrap=None,\n",
        "    enlarge=None,\n",
        "    update_interval=None,\n",
        "    walks=5,\n",
        "    facc=0.5,\n",
        "    slices=5,\n",
        "    fmove=0.9,\n",
        "    max_move=100,\n",
        "    number_of_cores=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform the fit with Dynesty, we pass it our model and analysis and we`re good to go!\n",
        "\n",
        "Checkout the folder `autofit_workspace/output/dynestystatic`, where the `NonLinearSearch` results, visualization and\n",
        "information can be found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result object returned by the fit provides information on the results of the non-linear search. \n",
        "\n",
        "The `info` attribute shows the model in a readable format, including the priors specified above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets use it to compare the maximum log likelihood `Gaussian` + `Exponential` to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = result.max_log_likelihood_instance\n",
        "\n",
        "model_gaussian = instance.gaussian.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "model_exponential = instance.exponential.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "model_data = model_gaussian + model_exponential\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
        "plt.plot(range(data.shape[0]), model_gaussian, \"--\")\n",
        "plt.plot(range(data.shape[0]), model_exponential, \"--\")\n",
        "plt.title(\"Dynesty model fit to 1D Gaussian + Exponential dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Probability Density Functions of the results can be plotted using Dynesty's in-built visualization tools, \n",
        "which are wrapped via the `DynestyPlotter` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_plotter = aplt.DynestyPlotter(samples=result.samples)\n",
        "search_plotter.cornerplot()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We discuss in more detail how to use a results object in the files `autofit_workspace/example/results`.\n",
        "\n",
        "#################\n",
        "##### Emcee #####\n",
        "#################\n",
        "\n",
        "To use a different non-linear we simply use call a different search from PyAutoFit, passing it the same the model\n",
        "and analysis as we did before to perform the fit. Below, we fit the same dataset using the MCMC sampler Emcee.\n",
        "Again, we manually specify all of the Emcee settings, however if they were omitted the values found in the config\n",
        "file `config/non_linear/Emcee.yaml` would be used instead.\n",
        "\n",
        "For a full description of Emcee, checkout its Github and readthedocs webpages:\n",
        "\n",
        "https://github.com/dfm/emcee\n",
        "https://emcee.readthedocs.io/en/stable/\n",
        "\n",
        "**PyAutoFit** extends **emcee** by providing an option to check the auto-correlation length of the samples\n",
        "during the run and terminating sampling early if these meet a specified threshold. See this page\n",
        "(https://emcee.readthedocs.io/en/stable/tutorials/autocorr/#autocorr) for a description of how this is implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Emcee(\n",
        "    path_prefix=path_prefix,\n",
        "    name=\"Emcee\",\n",
        "    nwalkers=30,\n",
        "    nsteps=2000,\n",
        "    initializer=af.InitializerBall(lower_limit=0.49, upper_limit=0.51),\n",
        "    auto_correlations_settings=af.AutoCorrelationsSettings(\n",
        "        check_for_convergence=True,\n",
        "        check_size=100,\n",
        "        required_length=50,\n",
        "        change_threshold=0.01,\n",
        "    ),\n",
        "    number_of_cores=1,\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result object returned by Emcee`s fit is similar in structure to the Dynesty result above.\n",
        " \n",
        "Printing its `info` shows that it does not estimate a Bayesian evidence, which `dynesty` above did, because it is an\n",
        "MCMC algoirithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It again provides us with the maximum log likelihood instance which we can use to visualize the fit to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = result.max_log_likelihood_instance\n",
        "\n",
        "model_gaussian = instance.gaussian.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "model_exponential = instance.exponential.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "model_data = model_gaussian + model_exponential\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
        "plt.plot(range(data.shape[0]), model_gaussian, \"--\")\n",
        "plt.plot(range(data.shape[0]), model_exponential, \"--\")\n",
        "plt.title(\"Emcee model fit to 1D Gaussian + Exponential dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Probability Density Functions (PDF's) of the results can be plotted using the Emcee's visualization \n",
        "tool `corner.py`, which is wrapped via the `EmceePlotter` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_plotter = aplt.EmceePlotter(samples=result.samples)\n",
        "search_plotter.corner()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "############################\n",
        "###### PARTICLE SWARM ######\n",
        "############################\n",
        "\n",
        "PyAutoFit also supports a number of searches, which seem to find the global (or local) maxima likelihood solution.\n",
        "Unlike nested samplers and MCMC algorithms, they do not extensive map out parameter space. This means they can find the\n",
        "best solution a lot faster than these algorithms, but they do not properly quantify the errors on each parameter.\n",
        "\n",
        "we'll use the Particle Swarm Optimization algorithm PySwarms. For a full description of PySwarms, checkout its Github \n",
        "and readthedocs webpages:\n",
        "\n",
        "https://github.com/ljvmiranda921/pyswarms\n",
        "https://pyswarms.readthedocs.io/en/latest/index.html\n",
        "\n",
        "**PyAutoFit** extends *PySwarms* by allowing runs to be terminated and resumed from the point of termination, as well\n",
        "as providing different options for the initial distribution of particles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.PySwarmsLocal(\n",
        "    path_prefix=path_prefix,\n",
        "    name=\"PySwarmsLocal\",\n",
        "    n_particles=100,\n",
        "    iters=1000,\n",
        "    cognitive=0.5,\n",
        "    social=0.3,\n",
        "    inertia=0.9,\n",
        "    ftol=-np.inf,\n",
        "    initializer=af.InitializerPrior(),\n",
        "    number_of_cores=1,\n",
        ")\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result object returned by PSO is again very similar in structure to previous results.\n",
        "\n",
        "The result info shows that the PSO does not estimate errors on the parameters, because it is a a maximum likelihood\n",
        "estimator (MLE)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It again provides us with the maximum log likelihood instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = result.max_log_likelihood_instance\n",
        "\n",
        "model_gaussian = instance.gaussian.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "model_exponential = instance.exponential.model_data_1d_via_xvalues_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "model_data = model_gaussian + model_exponential\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
        "plt.plot(range(data.shape[0]), model_gaussian, \"--\")\n",
        "plt.plot(range(data.shape[0]), model_exponential, \"--\")\n",
        "plt.title(\"PySwarms model fit to 1D Gaussian + Exponential dataset.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results can be plotted using the PySwarm's in-built visualization tools which are wrapped via \n",
        "the `PySwarmsPlotter` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pyswarms_plotter = aplt.PySwarmsPlotter(samples=result.samples)\n",
        "pyswarms_plotter.cost_history()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Other Samplers__\n",
        "\n",
        "Checkout https://pyautofit.readthedocs.io/en/latest/api/api.html for the non-linear searches available in PyAutoFit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}