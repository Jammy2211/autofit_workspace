{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Database__\n",
        "\n",
        "Before reading this example, you should have read the examples:\n",
        "\n",
        " `overview/simple/fit.py`\n",
        " `overview/simple/result.py`\n",
        "\n",
        "In the example `result.py`, we discussed the `Result`'s object, which contains information on the `NonLinearSearch`\n",
        "samples, the best-fit model and parameter estimates and errors. If you are fitting a model to only one dataset, this\n",
        "object suffices, but what if you are fitting the model to many datasets? How do you analyse, interpret and combine the\n",
        "results?\n",
        "\n",
        "Lets extend our example of fitting a 1D `Gaussian` profile and fit 3 independent datasets containing 1D Gaussians,\n",
        "such that the results of every `NonLinearSearch` are in an ordered path structure on our hard-disk. we'll then use\n",
        "the `Aggregator` to load the results of all 3 *non-linear searches*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "\n",
        "from os import path\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We create the same model used in the example `autofit_workspace/notebooks/overview/simple/fit.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=0.0,  # <- PyAutoFit recognises these constructor arguments\n",
        "        intensity=0.1,  # <- are the Gaussian`s model parameters.\n",
        "        sigma=0.01,\n",
        "    ):\n",
        "        \"\"\"Represents a 1D `Gaussian` profile, which may be treated as a model-component of PyAutoFit the\n",
        "        parameters of which are fitted for by a non-linear search.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        centre : float\n",
        "            The x coordinate of the profile centre.\n",
        "        intensity : float\n",
        "            Overall intensity normalisation of the `Gaussian` profile.\n",
        "        sigma : float\n",
        "            The sigma value controlling the size of the Gaussian.\n",
        "        \"\"\"\n",
        "\n",
        "        self.centre = centre\n",
        "        self.intensity = intensity\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def profile_from_xvalues(self, xvalues):\n",
        "        \"\"\"\n",
        "        Calculate the intensity of the profile on a line of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, using its centre.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues : np.ndarray\n",
        "            The x coordinates in the original reference frame of the grid.\n",
        "        \"\"\"\n",
        "\n",
        "        transformed_xvalues = xvalues - self.centre\n",
        "\n",
        "        return np.multiply(\n",
        "            np.divide(self.intensity, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We also create the same `Analysis` used in the example `autofit_workspace/notebooks/overview/simple/fit.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "        \"\"\"\n",
        "        Determine the log likelihood of a fit of a `Gaussian` to the dataset, using a model instance of the Gaussian.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance : model.Gaussian\n",
        "            The `Gaussian` model instance.\n",
        "\n",
        "        Returnsn\n",
        "        -------\n",
        "        fit : Fit.log_likelihood\n",
        "            The log likelihood value indicating how well this model fit the dataset.\n",
        "        \"\"\"\n",
        "\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "        model_data = instance.profile_from_xvalues(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        log_likelihood = -0.5 * sum(chi_squared_map)\n",
        "\n",
        "        return log_likelihood\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each dataset we are going to set up its output path, create its mask and fit it with the phase above. \n",
        "\n",
        "The 3 datasets will come from the `autofit_workspace/dataset/example_1d` folder.\n",
        "\n",
        "We want our results to be in a folder specific to the dataset. we'll use the `Dataset`'s name string to do this. Lets\n",
        "create a list of all 3 of our dataset names.\n",
        "\n",
        "We'll also pass these names to the dataset when we create it, the name will be accessible to the aggregator, and we \n",
        "will use it to label  figures we make via the aggregator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_names = [\"gaussian_x1_0\", \"gaussian_x1_1\", \"gaussian_x1_2\"]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also attach information to the model-fit, by setting up an info dictionary. \n",
        "\n",
        "Information about our model-fit (e.g. the dataset) that isn't part of the model-fit is made accessible to the \n",
        "aggregator. For example, below we write info on the dataset`s data of observation and exposure time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "info = {\"date_of_observation\": \"01-02-18\", \"exposure_time\": 1000.0}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Next, we create our model, which again corresponds to a single `Gaussian` and we'll manually specify its priors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.PriorModel(Gaussian)\n",
        "\n",
        "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.intensity = af.LogUniformPrior(lower_limit=1e-2, upper_limit=1e2)\n",
        "model.sigma = af.GaussianPrior(\n",
        "    mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This for loop runs over every dataset, checkout the comments below for how we set up the path structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name in dataset_names:\n",
        "\n",
        "    \"\"\"\n",
        "    The code below sets up the Analysis and creates the mask.\n",
        "    \"\"\"\n",
        "    dataset_path = path.join(\"dataset\", \"example_1d\", dataset_name)\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    \"\"\"\n",
        "    In all examples so far, our results have gone to the default path, which was the `/output/` folder and a folder\n",
        "    named after the non linear search. \n",
        "    \n",
        "    Below, using the `Paths` class and the input parameter `path_pefix`, we define the names of folders that the phase \n",
        "    goes in between the output path and phase paths as:\n",
        "\n",
        "    `/path/to/autofit_workspace/output/aggregator_example/gaussian_x1_0`\n",
        "    \n",
        "    Note that this therefore means the fit to each dataset will go into a unique folder.\n",
        "    \"\"\"\n",
        "    emcee = af.Emcee(\n",
        "        paths=af.Paths(path_prefix=path.join(\"overview\", \"database\", dataset_name)),\n",
        "        nwalkers=30,\n",
        "        nsteps=1000,\n",
        "        initializer=af.InitializerBall(lower_limit=0.49, upper_limit=0.51),\n",
        "        auto_correlation_check_for_convergence=True,\n",
        "        auto_correlation_check_size=100,\n",
        "        auto_correlation_required_length=50,\n",
        "        auto_correlation_change_threshold=0.01,\n",
        "        number_of_cores=1,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Emcee has begun running - checkout the autofit_workspace/output/{dataset_name} folder for live \"\n",
        "        f\"output of the results. This Jupyter notebook cell with progress once Emcee has completed - this could take a \"\n",
        "        f\"few minutes!\"\n",
        "    )\n",
        "\n",
        "    result = emcee.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")\n",
        "\n",
        "from autofit.database.aggregator import Aggregator\n",
        "\n",
        "aggregator = Aggregator.from_database(\n",
        "    \"database.sqlite\"\n",
        ")\n",
        "\n",
        "aggregator.add_directory(path.join(\"output\"))\n",
        "\n",
        "stop"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkout the output folder, you should see three new sets of results corresponding to the 3 `Gaussian` datasets.\n",
        "Unlike previous tutorials, these folders in the output folder are named after the dataset.\n",
        "\n",
        "To load these results with the aggregator, we simply point it to the path of the results we want it to inspect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg = af.Aggregator(directory=path.join(\"output\", \"overview\", \"database\"))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin, let me quickly explain what a generator is in Python, for those unaware. A generator is an object that \n",
        "iterates over a function when it is called. The aggregator creates all objects as generators, rather than lists, or \n",
        "dictionaries, or whatever.\n",
        "\n",
        "Why? Because lists and dictionaries store every entry in memory simultaneously. If you fit many datasets, you`ll have \n",
        "lots of results and therefore use a lot of memory. This will crash your laptop! On the other hand, a generator only \n",
        "stores the object in memory when it runs the function; it is free to overwrite it afterwards. Thus, your laptop won't \n",
        "crash!\n",
        "\n",
        "There are two things to bare in mind with generators:\n",
        "\n",
        "1) A generator has no length, thus to determine how many entries of data it corresponds to you first must turn it to a \n",
        "list.\n",
        "\n",
        "2) Once we use a generator, we cannot use it again - we'll need to remake it. For this reason, we typically avoid \n",
        "   storing the generator as a variable and instead use the aggregator to create them on use.\n",
        "\n",
        "We can now create a `samples` generator of every fit. An instance of the `Samples` class acts as an \n",
        "interface between the results of the non-linear fit on your hard-disk and Python and was discussed in the `results.py`\n",
        "example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_gen = agg.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we print this list of outputs you should see 3 different MCMCSamples instances, corresponding to the 3 model-fits\n",
        "performed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Emcee Samples:\\n\")\n",
        "print(samples_gen)\n",
        "print(\"Total Samples Objects = \", len(list(samples_gen)), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Samples` class is described in the `result.py` example script. Using the `Aggregator` we can access all of the \n",
        "attributes described in that example, for example the value of every parameter.\n",
        "\n",
        "Refer to `result.py` for all the properties that are accessible via the `Aggregator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(\"All parameters of the very first sample\")\n",
        "    print(samples.parameters[0])\n",
        "    print(\"The tenth sample`s third parameter\")\n",
        "    print(samples.parameters[9][2], \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the `Aggregator`'s filter tool, we can remove certain fits and load only the fits we are interested in. We filter\n",
        "for results by inputting a string (or strings) that the output path of the results we are loading must contain.\n",
        "\n",
        "For example, by inputting the string `gaussian_x1_1` the output path must contain this string, \n",
        "meaning we only load the results of the *model-fit* to the second `Gaussian` in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_filter = agg.filter(agg.directory.contains(\"gaussian_x1_1\"))\n",
        "samples_gen = agg_filter.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, this list now has only 1 MCMCSamples corresponding to the second dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Filtered Emcee Samples:\\n\")\n",
        "print(samples_gen)\n",
        "print(\"Total Samples Objects = \", len(list(samples_gen)), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets next try something more ambitious and create a plot of the inferred sigma values vs intensity of each\n",
        "Gaussian profile, including error bars at 3 sigma confidence.\n",
        "\n",
        "This will use many of the methods described in the `result.py` example scripts, so if anything below appears new or\n",
        "unclear checkout that script for a explanation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mp_instances = [samps.median_pdf_instance for samps in agg.values(\"samples\")]\n",
        "ue3_instances = [\n",
        "    out.error_instance_at_upper_sigma(sigma=3.0) for out in agg.values(\"samples\")\n",
        "]\n",
        "le3_instances = [\n",
        "    out.error_instance_at_lower_sigma(sigma=3.0) for out in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "print(mp_instances)\n",
        "\n",
        "mp_sigmas = [instance.sigma for instance in mp_instances]\n",
        "ue3_sigmas = [instance.sigma for instance in ue3_instances]\n",
        "le3_sigmas = [instance.sigma for instance in le3_instances]\n",
        "mp_intensitys = [instance.sigma for instance in mp_instances]\n",
        "ue3_intensitys = [instance.sigma for instance in ue3_instances]\n",
        "le3_intensitys = [instance.intensity for instance in le3_instances]\n",
        "\n",
        "plt.errorbar(\n",
        "    x=mp_sigmas,\n",
        "    y=mp_intensitys,\n",
        "    marker=\".\",\n",
        "    linestyle=\"\",\n",
        "    xerr=[le3_sigmas, ue3_sigmas],\n",
        "    yerr=[le3_intensitys, ue3_intensitys],\n",
        ")\n",
        "plt.title(\"Intensity vs Sigma for 3 model-fits to 1D Gaussians.\")\n",
        "plt.ylabel(\"Intensity\")\n",
        "plt.xlabel(\"Sigma\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Probability Density Functions (PDF's) of the results can be plotted using the library:\n",
        "\n",
        " corner.py: https://corner.readthedocs.io/en/latest/\n",
        "\n",
        "(In built visualization for PDF's and non-linear searches is a future feature of PyAutoFit, but for now you`ll have to \n",
        "use the libraries yourself!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import corner\n",
        "\n",
        "samples = list(agg.values(\"samples\"))[0]\n",
        "\n",
        "corner.corner(\n",
        "    xs=samples.parameters,\n",
        "    weights=samples.weights,\n",
        "    labels=samples.model.parameter_labels,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}