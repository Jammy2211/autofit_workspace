{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature: Multiple Datasets\n",
        "==========================\n",
        "\n",
        "It is common to have multiple observations of the same signal. For the 1D Gaussian example, this would be multiple\n",
        "1D datasets of the same underlying Gaussian, with different noise-map realizations. In this situation, fitting the\n",
        "same model to all datasets simultaneously is desired, and would provide better constraints on the model.\n",
        "\n",
        "On other occations, the signal may vary across the datasets in a way that requires that the model is updated\n",
        "accordingly. For example, a scenario where the centre of each Gaussian is the same across the datasets, but\n",
        "their `sigma` values are different in each dataset. A model where all Gaussians share the same `centre` is now required.\n",
        "\n",
        "This examples illustrates how to perform model-fits to multiple datasets simultaneously, including tools to customize\n",
        "the model composition such that specific parameters of the model vary across the datasets.\n",
        "\n",
        "This uses the summing of `Analysis` object, which each have their own unique dataset and `log_likelihood_function`.\n",
        "Unique `Analysis` objects can be written for each dataset, meaning that we can perform model-fits to diverse datasets\n",
        "with different formats and structures.\n",
        "\n",
        "It is also common for each individual dataset to only constrain specific aspects of a model. The high level of model\n",
        "customizaiton ensures that composing a model that is appropriate for fitting to such large datasets is straight\n",
        "forward.\n",
        "\n",
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and \n",
        " `visualize` functions.\n",
        " \n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        "These are functionally identical to the `Analysis` and `Gaussian` objects you have seen elsewhere in the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af\n",
        "import autofit.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "First, lets load 3 datasets of a 1D Gaussian, by loading them from .json files in the directory \n",
        "`autofit_workspace/dataset/`.\n",
        "\n",
        "All three datasets contain an identical signal, meaning that it is appropriate to fit the same model to all three \n",
        "datasets simultaneously.\n",
        "\n",
        "Each dataset has a different noise realization, meaning that performing a simultaneously fit will offer improved \n",
        "constraints over individual fits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_size = 3\n",
        "\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "\n",
        "for dataset_index in range(dataset_size):\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", f\"gaussian_x1_identical_{dataset_index}\"\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    data_list.append(data)\n",
        "\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "    noise_map_list.append(noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets plot all 3 datasets, including their error bars. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    xvalues = range(data.shape[0])\n",
        "\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=data,\n",
        "        yerr=noise_map,\n",
        "        color=\"k\",\n",
        "        ecolor=\"k\",\n",
        "        linestyle=\" \",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Next, we create our model, which corresponds to a single 1D Gaussian, that is fitted to all 3 datasets simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(af.ex.Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkout `autofit_workspace/config/priors/model.yaml`, this config file defines the default priors of the `Gaussian` \n",
        "model component. \n",
        "\n",
        "We overwrite the priors below to make them explicit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.normalization = af.LogUniformPrior(lower_limit=1e-2, upper_limit=1e2)\n",
        "model.sigma = af.GaussianPrior(\n",
        "    mean=10.0, sigma=5.0, lower_limit=0.0, upper_limit=np.inf\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We set up our three instances of the `Analysis` class, using the class described in `analysis.py`.\n",
        " \n",
        "We set up an `Analysis` for each dataset one-by-one, using a for loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Summing__\n",
        "\n",
        "We now sum together every analysis in the list, to produce an overall analysis class which we fit with the non-linear\n",
        "search.\n",
        "\n",
        "By summing analysis objects the following happen:\n",
        "\n",
        " - The log likelihood values computed by the `log_likelihood_function` of each individual analysis class are summed to\n",
        "   give an overall log likelihood value that the non-linear search uses for model-fitting.\n",
        "\n",
        " - The output path structure of the results goes to a single folder, which includes sub-folders for the visualization\n",
        "   of every individual analysis object based on the `Analysis` object's `visualize` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = analysis_list[0] + analysis_list[1] + analysis_list[2]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can alternatively sum a list of analysis objects as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = sum(analysis_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `log_likelihood_function`'s can be called in parallel over multiple cores by changing the `n_cores` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis.n_cores = 1"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "To fit multiple datasets via a non-linear search we use this summed analysis object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(path_prefix=\"features\", name=\"multiple_datasets_simple\")\n",
        "\n",
        "result_list = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result object returned by the fit is a list of the `Result` objects you have used in other examples.\n",
        "\n",
        "In this example, the same model is fitted across all analyses, thus every `Result` in the `result_list` contains\n",
        "the same information on the samples and thus gives the same output from methods such \n",
        "as `max_log_likelihood_instance`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_list[0].max_log_likelihood_instance)\n",
        "print(result_list[1].max_log_likelihood_instance)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the model-fit to each dataset by iterating over the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for result in result_list:\n",
        "    instance = result.max_log_likelihood_instance\n",
        "\n",
        "    model_data = instance.model_data_1d_via_xvalues_from(\n",
        "        xvalues=np.arange(data.shape[0])\n",
        "    )\n",
        "\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=data,\n",
        "        yerr=noise_map,\n",
        "        color=\"k\",\n",
        "        ecolor=\"k\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.plot(xvalues, model_data, color=\"r\")\n",
        "    plt.title(\"Dynesty model fit to 1D Gaussian dataset.\")\n",
        "    plt.xlabel(\"x values of profile\")\n",
        "    plt.ylabel(\"Profile normalization\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Variable Model Across Datasets__\n",
        "\n",
        "Above, the same model was fitted to every dataset simultaneously, which was possible because all 3 datasets contained \n",
        "an identical signal with only the noise varying across the datasets.\n",
        "\n",
        "It is common for the signal in each dataset to be different and for it to constrain only certain aspects of the model.\n",
        "The model parameterization therefore needs to change in order to account for this.\n",
        "\n",
        "Lets look at an example of a dataset of 3 1D Gaussians where the signal varies across the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1_variable\")\n",
        "\n",
        "dataset_name_list = [\"sigma_0\", \"sigma_1\", \"sigma_2\"]\n",
        "\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "\n",
        "for dataset_name in dataset_name_list:\n",
        "    dataset_time_path = path.join(dataset_path, dataset_name)\n",
        "\n",
        "    data = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_time_path, \"data.json\")\n",
        "    )\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_time_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    data_list.append(data)\n",
        "    noise_map_list.append(noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we plot these datasets, we see that the `sigma` of each Gaussian decreases.\n",
        "\n",
        "We will illustrate models which vary over the data based on this `sigma` value. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    xvalues = range(data.shape[0])\n",
        "\n",
        "    af.ex.plot_profile_1d(xvalues=xvalues, profile_1d=data)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the `centre` and `normalization` of all three 1D Gaussians are the same in each dataset,\n",
        "but their `sigma` values are decreasing.\n",
        "\n",
        "We therefore wish to compose and to fit a model to all three datasets simultaneously, where the `centre` \n",
        "and `normalization` are the same across all three datasets but the `sigma` value is unique for each dataset.\n",
        "\n",
        "To do that, we interface a model with a summed list of analysis objects, which we create below for this new\n",
        "dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "    analysis_list.append(analysis)\n",
        "\n",
        "analysis = sum(analysis_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We next compose a model of a 1D Gaussian, as performed frequently throughout the **PyAutoFit** examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.Model(af.ex.Gaussian))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now update the model using the summed `Analysis `objects to compose a model where: \n",
        "\n",
        " - The `centre` and `normalization` values of the Gaussian fitted to every dataset in every `Analysis` object are\n",
        " identical. \n",
        " \n",
        " - The `sigma` value of the every Gaussian fitted to every dataset in every `Analysis` object are different.\n",
        "\n",
        "This means that the model has 5 free parameters in total, the shared `centre` and `normalization` and a unique\n",
        "`sigma` value for every dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = analysis.with_free_parameters(model.gaussian.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This means that the model has 5 free parameters in total, the shared `centre` and `normalization` and a unique\n",
        "`sigma` value for every dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.prior_count)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now fit this model to the data using the usual **PyAutoFit** tools:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(path_prefix=\"features\", name=\"multiple_datasets_free_sigma\")\n",
        "\n",
        "result_list = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Individual Sequential Searches__\n",
        "\n",
        "The API above is used to create a model with free parameters across ``Analysis`` objects, which are all fit\n",
        "simultaneously using a summed ``log_likelihood_function`` and single non-linear search.\n",
        "\n",
        "Each ``Analysis`` can be fitted one-by-one, using a series of multiple non-linear searches, using\n",
        "the ``fit_sequential`` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=\"features\", name=\"multiple_datasets_free_sigma__sequential\"\n",
        ")\n",
        "\n",
        "result_list = search.fit_sequential(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The benefit of this method is for complex high dimensionality models (e.g. when many parameters are passed\n",
        "to `` analysis.with_free_parameters``, it breaks the fit down into a series of lower dimensionality non-linear\n",
        "searches that may convergence on a solution more reliably.\n",
        "\n",
        "__Variable Parameters As Relationship__\n",
        "\n",
        "In the model above, an extra free parameter `sigma` was added for every dataset. \n",
        "\n",
        "This was ok for the simple model fitted here to just 3 datasets, but for more complex models and problems with 10+\n",
        "datasets one will quick find that the model complexity increases dramatically.\n",
        "\n",
        "In these circumstances, one can instead compose a model where the parameters vary smoothly across the datasets\n",
        "via a user defined relation.\n",
        "\n",
        "Below, we compose a model where the `sigma` value fitted to each dataset is computed according to:\n",
        "\n",
        " `y = m * x + c` : `sigma` = sigma_m * x + sigma_c`\n",
        " \n",
        "Where x is an integer number specifying the index of the dataset (e.g. 1, 2 and 3).\n",
        " \n",
        "By defining a relation of this form, `sigma_m` and `sigma_c` are the only free parameters of the model which vary\n",
        "across the datasets. \n",
        "\n",
        "Therefore, if more datasets are added the number of model parameter does not increase, like we saw above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sigma_m = af.UniformPrior(lower_limit=-10.0, upper_limit=10.0)\n",
        "sigma_c = af.UniformPrior(lower_limit=-10.0, upper_limit=10.0)\n",
        "\n",
        "x_list = [1.0, 2.0, 3.0]\n",
        "\n",
        "analysis_with_relation_list = []\n",
        "\n",
        "for x, analysis in zip(x_list, analysis_list):\n",
        "    sigma_relation = (sigma_m * x) + sigma_c\n",
        "\n",
        "    analysis_with_relation = analysis.with_model(\n",
        "        model.replacing({model.gaussian.sigma: sigma_relation})\n",
        "    )\n",
        "\n",
        "    analysis_with_relation_list.append(analysis_with_relation)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can fit this model as per usual, you may wish to checkout the `model.info` file to see how a schematic of this\n",
        "model's composition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_with_relation = sum(analysis_with_relation_list)\n",
        "\n",
        "search = af.DynestyStatic(path_prefix=\"features\", name=\"multiple_datasets_relation\")\n",
        "\n",
        "result_list = search.fit(model=model, analysis=analysis_with_relation)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Temporally Varying Models__\n",
        "\n",
        "An obvious example of fitting models which vary across datasets are time-varying models, where the datasets are\n",
        "observations of a signal which varies across time.\n",
        "\n",
        "In such circumstances, it is common for certain model parameters to be known to not vary as a function of time (and \n",
        "therefore be fixed across the datasets) whereas other parameters are known to vary as a function of time (and therefore\n",
        "should be parameterized accordingly using the API illustrated here).\n",
        "\n",
        "__Different Analysis Objects__\n",
        "\n",
        "For simplicity, this example summed together only a single `Analysis` class. \n",
        "\n",
        "For many problems one may have multiple datasets which are quite different in their format and structure (perhaps \n",
        "one is a  1D signal whereas another dataset is an image). In this situation, one can simply define unique `Analysis`\n",
        "objects for each type of dataset, which will contain a unique `log_likelihood_function` and methods for visualization.\n",
        "\n",
        "Nevertheless, the analysis summing API illustrated here will still work, meaning that **PyAutoFit** makes it simple to \n",
        "fit highly customized models to multiple datasets that are different in their format and structure. \n",
        "\n",
        "__Graphical Models__\n",
        "\n",
        "A common class of models used for fitting complex models to large datasets are graphical models. \n",
        "\n",
        "Graphical models can include addition parameters not specific to individual datasets describing the overall \n",
        "relationship between different model components, thus allowing one to infer the global trends contained within a \n",
        "dataset.\n",
        "\n",
        "**PyAutoFit** has a dedicated feature set for fitting graphical models and interested readers should\n",
        "checkout the graphical modeling chapter of **HowToFit** (https://pyautofit.readthedocs.io/en/latest/howtofit/chapter_graphical_models.html)\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "We have shown how **PyAutoFit** can fit large datasets simultaneously, using custom models that vary specific\n",
        "parameters across the dataset.\n",
        "\n",
        "The `autofit_workspace/*/model/cookbook_3_multiple_datasets` cookbook gives a concise API reference for model \n",
        "composition when fitting multiple datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}