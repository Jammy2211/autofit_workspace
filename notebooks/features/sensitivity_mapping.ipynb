{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature: Sensitivity Mapping\n",
        "============================\n",
        "\n",
        "Bayesian model comparison allows us to take a dataset, fit it with multiple models and use the Bayesian evidence to\n",
        "quantify which model objectively gives the best-fit following the principles of Occam's Razor.\n",
        "\n",
        "However, a complex model may not be favoured by model comparison not because it is the 'wrong' model, but simply\n",
        "because the dataset being fitted is not of a sufficient quality for the more complex model to be favoured. Sensitivity\n",
        "mapping allows us to address what quality of data would be needed for the more complex model to be favoured or\n",
        "alternatively for what sets of model parameter values it would be favoured for data of a given quality.\n",
        "\n",
        "In order to do this, sensitivity mapping involves us writing a function that uses the model(s) to simulate a dataset.\n",
        "We then use this function to simulate many datasets, for many different models, and fit each dataset using the same\n",
        "model-fitting procedure we used to perform Bayesian model comparison. This allows us to infer how much of a Bayesian\n",
        "evidence increase we should expect for datasets of varying quality and / or models with different parameters.\n",
        "\n",
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and\n",
        " `visualize` functions.\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        "These are functionally identical to the `Analysis` and `Gaussian` objects you have seen elsewhere in the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Load data of a 1D Gaussian from a .json file in the directory \n",
        "`autofit_workspace/dataset/gaussian_x1_with_feature`.\n",
        "\n",
        "This 1D data includes a small feature to the right of the central `Gaussian`. This feature is a second `Gaussian` \n",
        "centred on pixel 70. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1_with_feature\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets plot the data. \n",
        "\n",
        "The feature on pixel 70 is clearly visible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = range(data.shape[0])\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.title(\"1D Gaussian Data With Feature at pixel 70.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "Create the analysis which fits the model to the data.\n",
        "\n",
        "It fits the data as the sum of the two `Gaussian`'s in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = af.ex.Analysis(data=data, noise_map=noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Comparison__\n",
        "\n",
        "Before performing sensitivity mapping, we will quickly perform Bayesian model comparison on this data to get a sense \n",
        "for whether the `Gaussian` feature is detectable and how much the Bayesian evidence increases when it is included in\n",
        "the model.\n",
        "\n",
        "We therefore fit the data using two models, one where the model is a single `Gaussian` and one where it is \n",
        "two `Gaussians`. \n",
        "\n",
        "To avoid slow model-fitting and more clearly prounce the results of model comparison, we restrict the centre of \n",
        "the`gaussian_feature` to its true centre of 70 and sigma value of 0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian_main=af.ex.Gaussian)\n",
        "\n",
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"features\", \"sensitivity_mapping\", \"single_gaussian\"),\n",
        "    nlive=100,\n",
        ")\n",
        "\n",
        "result_single = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "model = af.Collection(gaussian_main=af.ex.Gaussian, gaussian_feature=af.ex.Gaussian)\n",
        "model.gaussian_feature.centre = 70.0\n",
        "model.gaussian_feature.sigma = 0.5\n",
        "\n",
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"features\", \"sensitivity_mapping\", \"two_gaussians\"), nlive=100\n",
        ")\n",
        "\n",
        "result_multiple = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now print the `log_evidence` of each fit and confirm the model with two `Gaussians` was preferred to the model\n",
        "with just one `Gaussian`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_single.samples.log_evidence)\n",
        "print(result_multiple.samples.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Sensitivity Mapping__\n",
        "\n",
        "The model comparison above shows that in this dataset, the `Gaussian` feature was detectable and that it increased the \n",
        "Bayesian evidence by ~25. Furthermore, the normalization of this `Gaussian` was ~0.3. \n",
        "\n",
        "A lower value of normalization makes the `Gaussian` fainter and harder to detect. We will demonstrate sensitivity mapping \n",
        "by answering the following question, at what value of normalization does the `Gaussian` feature become undetectable and\n",
        "not provide us with a noticeable increase in Bayesian evidence?\n",
        "\n",
        "To begin, we define the `base_model` that we use to perform sensitivity mapping. This model is used to simulate every \n",
        "dataset. It is also fitted to every simulated dataset without the extra model component below, to give us the Bayesian\n",
        "evidence of the every simpler model to compare to the more complex model. \n",
        "\n",
        "The `base_model` corresponds to the `gaussian_main` above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "base_model = af.Collection(gaussian_main=af.ex.Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define the `perturbation_model`, which is the model component whose parameters we iterate over to perform \n",
        "sensitivity mapping. Many instances of the `perturbation_model` are created and used to simulate the many datasets \n",
        "that we fit. However, it is only included in half of the model-fits; corresponding to the more complex models whose\n",
        "Bayesian evidence we compare to the simpler model-fits consisting of just the `base_model`.\n",
        "\n",
        "The `perturbation_model` is therefore another `Gaussian` but now corresponds to the `gaussian_feature` above.\n",
        "\n",
        "By fitting both of these models to every simulated dataset, we will therefore infer the Bayesian evidence of every\n",
        "model to every dataset. Sensitivity mapping therefore maps out for what values of `normalization` in the `gaussian_feature`\n",
        " does the more complex model-fit provide higher values of Bayesian evidence than the simpler model-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "perturbation_model = af.Model(af.ex.Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sensitivity mapping is typically performed over a large range of parameters. However, to make this demonstration quick\n",
        "and clear we are going to fix the `centre` and `sigma` values to the true values of the `gaussian_feature`. We will \n",
        "also iterate over just two `normalization` values corresponding to 0.01 and 100.0, which will clearly exhaggerate the\n",
        "sensitivity between the models at these two values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "perturbation_model.centre = 70.0\n",
        "perturbation_model.sigma = 0.5\n",
        "perturbation_model.normalization = af.UniformPrior(lower_limit=0.01, upper_limit=100.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are performing sensitivity mapping to determine how bright the `gaussian_feature` needs to be in order to be \n",
        "detectable. However, every simulated dataset must include the `main_gaussian`, as its presence in the data will effect\n",
        "the detectability of the `gaussian_feature`.\n",
        "\n",
        "We can pass the `main_gaussian` into the sensitivity mapping as the `simulation_instance`, meaning that it will be used \n",
        "in the simulation of every dataset. For this example we use the inferred `main_gaussian` from one of the model-fits\n",
        "performed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "simulation_instance = result_single.instance"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are about to write a `simulate_function` that simulates examples of 1D `Gaussian` datasets that are fitted to\n",
        "perform sensitivity mapping.\n",
        "\n",
        "To pass each simulated data through **PyAutoFit**'s sensitivity mapping tools, the function must return a single \n",
        "Python object. We therefore define a `Dataset` class that combines the `data` and `noise_map` that are to be \n",
        "output by this `simulate_function`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self, data, noise_map):\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now write the `simulate_function`, which takes the `simulation_instance` of our model (defined above) and uses it to \n",
        "simulate a dataset which is subsequently fitted.\n",
        "\n",
        "Note that when this dataset is simulated, the quantity `instance.perturbation` is used in the `simulate_function`.\n",
        "This is an instance of the `gaussian_feature`, and it is different every time the `simulate_function` is called. \n",
        "\n",
        "In this example, this `instance.perturbation` corresponds to two different `gaussian_feature` with values of\n",
        "`normalization` of 0.01 and 100.0, such that our simulated datasets correspond to a very faint and very bright gaussian \n",
        "features ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def simulate_function(instance, simulate_path):\n",
        "    \"\"\"\n",
        "    The `simulate_path` specifies the folder the results of sensitivity mapping of this simulated dataset is output to.\n",
        "    It can be used to output information specific to this simulation, such as visualization of the dataset, etc.\n",
        "    \"\"\"\n",
        "    print(simulate_path)\n",
        "\n",
        "    \"\"\"\n",
        "    Specify the number of pixels used to create the xvalues on which the 1D line of the profile is generated using and\n",
        "    thus defining the number of data-points in our data.\n",
        "    \"\"\"\n",
        "    pixels = 100\n",
        "    xvalues = np.arange(pixels)\n",
        "\n",
        "    \"\"\"\n",
        "    Evaluate the `Gaussian` and Exponential model instances at every xvalues to create their model profile and sum\n",
        "    them together to create the overall model profile.\n",
        "    \n",
        "    This print statement will show that, when you run `Sensitivity` below the values of the perturbation use fixed \n",
        "    values of `centre=70` and `sigma=0.5`, whereas the normalization varies over the `number_of_steps` based on its prior.\n",
        "    \"\"\"\n",
        "\n",
        "    print(instance.perturbation.centre)\n",
        "    print(instance.perturbation.normalization)\n",
        "    print(instance.perturbation.sigma)\n",
        "\n",
        "    model_line = instance.gaussian_main.model_data_1d_via_xvalues_from(\n",
        "        xvalues=xvalues\n",
        "    ) + instance.perturbation.model_data_1d_via_xvalues_from(xvalues=xvalues)\n",
        "\n",
        "    \"\"\"\n",
        "    Determine the noise (at a specified signal to noise level) in every pixel of our model profile.\n",
        "    \"\"\"\n",
        "    signal_to_noise_ratio = 25.0\n",
        "    noise = np.random.normal(0.0, 1.0 / signal_to_noise_ratio, pixels)\n",
        "\n",
        "    \"\"\"\n",
        "    Add this noise to the model line to create the line data that is fitted, using the signal-to-noise ratio to compute\n",
        "    noise-map of our data which is required when evaluating the chi-squared value of the likelihood.\n",
        "    \"\"\"\n",
        "    data = model_line + noise\n",
        "    noise_map = (1.0 / signal_to_noise_ratio) * np.ones(pixels)\n",
        "\n",
        "    return Dataset(data=data, noise_map=noise_map)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each model-fit performed by sensitivity mapping creates a new instance of an `Analysis` class, which contains the\n",
        "data simulated by the `simulate_function` for that model.\n",
        "\n",
        "This requires us to write a wrapper around the `Analysis` class that we used to fit the model above, so that is uses\n",
        "the `Dataset` object above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.ex.Analysis):\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__(data=dataset.data, noise_map=dataset.noise_map)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We next specify the search used to perform each model fit by the sensitivity mapper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"features\", \"sensitivity_mapping\", \"sensitivity_map\"),\n",
        "    nlive=100,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now combine all of the objects created above and perform sensitivity mapping. The inputs to the `Sensitivity`\n",
        "object below are:\n",
        "\n",
        "- `simulation_instance`: This is an instance of the model used to simulate every dataset that is fitted. In this example it \n",
        "contains an instance of the `gaussian_main` model component.\n",
        "\n",
        "- `base_model`: This is the simpler model that is fitted to every simulated dataset, which in this example is composed of \n",
        "a single `Gaussian` called the `gaussian_main`.\n",
        "\n",
        "- `perturbation_model`: This is the extra model component that alongside the `base_model` is fitted to every simulated \n",
        "dataset, which in this example  is composed of two `Gaussians` called the `gaussian_main` and `gaussian_feature`.\n",
        "\n",
        "- `simulate_function`: This is the function that uses the `simulation_instance` and many instances of the `perturbation_model` \n",
        "to simulate many datasets that are fitted with the `base_model` and `base_model` + `perturbation_model`.\n",
        "\n",
        "- `analysis_class`: The wrapper `Analysis` class that passes each simulated dataset to the `Analysis` class that fits \n",
        "the data.\n",
        "\n",
        "- `number_of_steps`: The number of steps over which the parameters in the `perturbation_model` are iterated. In this \n",
        "example, normalization has a `LogUniformPrior` with lower limit 1e-4 and upper limit 1e2, therefore the `number_of_steps` \n",
        "of 2 wills imulate and fit just 2 datasets where the intensities between 1e-4 and 1e2.\n",
        "\n",
        "- `number_of_cores`: The number of cores over which the sensitivity mapping is performed, enabling parallel processing\n",
        "if set above 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.non_linear.grid import sensitivity as s\n",
        "\n",
        "sensitivity = s.Sensitivity(\n",
        "    search=search,\n",
        "    simulation_instance=simulation_instance,\n",
        "    base_model=base_model,\n",
        "    perturbation_model=perturbation_model,\n",
        "    simulate_function=simulate_function,\n",
        "    analysis_class=Analysis,\n",
        "    number_of_steps=2,\n",
        "    number_of_cores=2,\n",
        ")\n",
        "\n",
        "sensitivity_result = sensitivity.run()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should now look at the results of the sensitivity mapping in the folder `output/features/sensitivity_mapping`. \n",
        "\n",
        "You will note the following 4 model-fits have been performed:\n",
        "\n",
        " - The `base_model` is fitted to a simulated dataset where the `simulation_instance` and \n",
        " a `perturbation` with `normalization=0.01` are used.\n",
        "\n",
        " - The `base_model` + `perturbation_model`  is fitted to a simulated dataset where the `simulation_instance` and \n",
        " a `perturbation` with `normalization=0.01` are used.\n",
        "\n",
        " - The `base_model` is fitted to a simulated dataset where the `simulation_instance` and \n",
        " a `perturbation` with `normalization=100.0` are used.\n",
        "\n",
        " - The `base_model` + `perturbation_model`  is fitted to a simulated dataset where the `simulation_instance` and \n",
        " a `perturbation` with `normalization=100.0` are used.\n",
        "\n",
        "The fit produced a `sensitivity_result`. \n",
        "\n",
        "We are still developing the `SensitivityResult` class to provide a data structure that better streamlines the analysis\n",
        "of results. If you intend to use sensitivity mapping, the best way to interpret the resutls is currently via\n",
        "**PyAutoFit**'s database and `Aggregator` tools. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(sensitivity_result.results[0].result.samples.log_evidence)\n",
        "print(sensitivity_result.results[1].result.samples.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}