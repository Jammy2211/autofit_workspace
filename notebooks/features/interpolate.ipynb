{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature: Interpolate\n",
        "====================\n",
        "\n",
        "It is common to fit a model to many similar datasets, where it is anticipated that one or more model parameters vary\n",
        "smoothly across the datasets.\n",
        "\n",
        "For example, the datasets may be taken at different times, where the signal in the data and therefore model parameters\n",
        "vary smoothly as a function of time. Alternaitvely, the datasets may be taken at different wavelengths, with the signal\n",
        "varying smoothly as a function of wavelength.\n",
        "\n",
        "In any of these cases, it may be desireable to fit the datasets one-by-one and then interpolate the results in order\n",
        "to determine the most likely model parameters at any point in time (or at any wavelength).\n",
        "\n",
        "This example illustrates model interpolation functionality in **PyAutoFit** using the example of fitting 3 noisy\n",
        "1D Gaussians, where these data are assumed to have been taken at 3 different times. The `centre` of each `Gaussian`\n",
        "varies smoothly over time. The interpolation is therefore used to estimate the `centre` of each `Gaussian` at any time\n",
        "outside of the times the data were observed.\n",
        "\n",
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and\n",
        " `visualize` functions.\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        "These are functionally identical to the `Analysis` and `Gaussian` objects you have seen elsewhere in the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from os import path\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "We illustrate model interpolation using 3 noisy 1D Gaussian datasets taken at 3 different times, where the `centre` of \n",
        "each `Gaussian` varies smoothly over time.\n",
        "\n",
        "The datasets are taken at 3 times, t=0, t=1 and t=2, which defines the name of the folder we load the data from.\n",
        "\n",
        "We load each data and noise map and store them in lists, so we can plot them next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_datasets = 3\n",
        "\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "time_list = []\n",
        "\n",
        "for time in range(3):\n",
        "    dataset_name = f\"time_{time}\"\n",
        "\n",
        "    dataset_prefix_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1_variable\")\n",
        "\n",
        "    dataset_path = path.join(dataset_prefix_path, dataset_name)\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    data_list.append(data)\n",
        "    noise_map_list.append(noise_map)\n",
        "    time_list.append(time)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visual comparison of the datasets shows that the `centre` of each `Gaussian` varies smoothly over time, with it moving\n",
        "from pixel 40 at t=0 to pixel 60 at t=2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for time in range(3):\n",
        "    xvalues = range(data_list[time].shape[0])\n",
        "\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=data_list[time],\n",
        "        yerr=noise_map_list[time],\n",
        "        color=\"k\",\n",
        "        ecolor=\"k\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.title(\"1D Gaussian Data #1.\")\n",
        "    plt.xlabel(\"x values of profile\")\n",
        "    plt.ylabel(\"Profile normalization\")\n",
        "    plt.show()\n",
        "    plt.close()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Fit__\n",
        "\n",
        "We now fit each of the 3 datasets.\n",
        "\n",
        "The fits are performed in a for loop, with the docstrings inside the loop explaining the code.\n",
        "\n",
        "The interpolate at the end of the fits uses the maximum log likelihood model of each fit, which we store in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_instances_list = []\n",
        "\n",
        "for data, noise_map, time in zip(data_list, noise_map_list, time_list):\n",
        "    \"\"\"\n",
        "    __Analysis__\n",
        "\n",
        "    For each dataset we create an `Analysis` class, which includes the `log_likelihood_function` we fit the data with.\n",
        "    \"\"\"\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    \"\"\"\n",
        "    __Time__\n",
        "    \n",
        "    The model composed below has an input not seen in other examples, the parameter `time`.\n",
        "    \n",
        "    This is the time that the simulated data was acquired and is not a free parameter in the fit. \n",
        "    \n",
        "    For interpolation it plays a crucial role, as the model is interpolated to the time of every dataset input\n",
        "    into the model below. If the `time` input were missing, interpolation could not be performed.\n",
        "    \n",
        "    Over the iterations of the for loop, the `time` input will therefore be the values 0.0, 1.0 and 2.0.\n",
        "\n",
        "    __Model__\n",
        "    \n",
        "    We now compose our model, which is a single `Gaussian`.\n",
        "    \n",
        "    The `centre` of the `Gaussian` is a free parameter with a `UniformPrior` that ranges between 0.0 and 100.0. \n",
        "    \n",
        "    We expect the inferred `centre` inferred from the fit to each dataset to vary smoothly as a function of time.\n",
        "    \"\"\"\n",
        "    model = af.Collection(gaussian=af.Model(af.ex.Gaussian), time=time)\n",
        "\n",
        "    \"\"\"\n",
        "    __Search__\n",
        "    \n",
        "    The model is fitted to the data using the nested sampling algorithm \n",
        "    Dynesty (https://johannesbuchner.github.io/UltraNest/readme.html).\n",
        "    \"\"\"\n",
        "    search = af.DynestyStatic(\n",
        "        path_prefix=path.join(\"interpolate\"),\n",
        "        name=f\"time_{time}\",\n",
        "        nlive=100,\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    __Model-Fit__\n",
        "    \n",
        "    We can now begin the model-fit by passing the model and analysis object to the search, which performs a non-linear\n",
        "    search to find which models fit the data with the highest likelihood.\n",
        "    \"\"\"\n",
        "    result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "    \"\"\"\n",
        "    __Instances__\n",
        "    \n",
        "    Interpolation uses the maximum log likelihood model of each fit to build an interpolation model of the model as a\n",
        "    function of time. \n",
        "    \n",
        "    We therefore store the maximum log likelihood model of every fit in a list, which is used below.\n",
        "    \"\"\"\n",
        "    ml_instances_list.append(result.instance)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Interpolation__\n",
        "\n",
        "Now all fits are complete, we use the `ml_instances_list` to build an interpolation model of the model as a function \n",
        "of time.\n",
        "\n",
        "This is performed using the `LinearInterpolator` object, which interpolates the model parameters as a function of\n",
        "time linearly between the values computed by the model-fits above.\n",
        "\n",
        "More advanced interpolation schemes are available and described in the `interpolation.py` example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "interpolator = af.LinearInterpolator(instances=ml_instances_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model can be interpolated to any time, for example time=1.5.\n",
        "\n",
        "This returns a new `instance` of the model, as an instance of the `Gaussian` object, where the parameters are computed \n",
        "by interpolating between the values computed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = interpolator[interpolator.time == 1.5]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `centre` of the `Gaussian` at time 1.5 is between the value inferred for the first and second fits taken\n",
        "at times 1.0 and 2.0.\n",
        "\n",
        "This is a `centre` close to a value of 55.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Gaussian centre of fit 1 (t = 1): {ml_instances_list[0].gaussian.centre}\")\n",
        "print(f\"Gaussian centre of fit 2 (t = 2): {ml_instances_list[1].gaussian.centre}\")\n",
        "\n",
        "print(f\"Gaussian centre interpolated at t = 1.5 {instance.gaussian.centre}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Serialization__\n",
        "\n",
        "The interpolator and model can be serialized to a .json file using **PyAutoConf**'s dedicated serialization methods. \n",
        "\n",
        "This means an interpolator can easily be loaded into other scripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autoconf.dictable import output_to_json, from_json\n",
        "\n",
        "json_file = path.join(dataset_prefix_path, \"interpolator.json\")\n",
        "\n",
        "output_to_json(obj=interpolator, file_path=json_file)\n",
        "\n",
        "interpolator = from_json(file_path=json_file)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Database__\n",
        "\n",
        "It may be inconvenient to fit all the models in a single Python script (e.g. the model-fits take a long time and you \n",
        "are fitting many datasets).\n",
        "\n",
        "PyAutoFit's allows you to store the results of model-fits from hard-disk. \n",
        "\n",
        "Database functionality then allows you to load the results of the fit above, set up the interpolator and perform the \n",
        "interpolation.\n",
        "\n",
        "If you are not familiar with the database API, you should checkout the `cookbook/database.ipynb` example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.aggregator.aggregator import Aggregator\n",
        "\n",
        "agg = Aggregator.from_directory(\n",
        "    directory=path.join(\"output\", \"interpolate\"), completed_only=False\n",
        ")\n",
        "\n",
        "ml_instances_list = [samps.max_log_likelihood() for samps in agg.values(\"samples\")]\n",
        "\n",
        "interpolator = af.LinearInterpolator(instances=ml_instances_list)\n",
        "\n",
        "instance = interpolator[interpolator.time == 1.5]\n",
        "\n",
        "print(f\"Gaussian centre of fit 1 (t = 1): {ml_instances_list[0].gaussian.centre}\")\n",
        "print(f\"Gaussian centre of fit 2 (t = 2): {ml_instances_list[1].gaussian.centre}\")\n",
        "\n",
        "print(f\"Gaussian centre interpolated at t = 1.5 {instance.gaussian.centre}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}