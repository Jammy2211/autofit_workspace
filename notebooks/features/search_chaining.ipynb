{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature: Search Chaining\n",
        "========================\n",
        "\n",
        "To perform a model-fit, we typically compose one model and fit it to our data using one non-linear search.\n",
        "\n",
        "Search chaining fits many different models to a dataset using a chained sequence of non-linear searches. Initial\n",
        "fits are performed using simplified model parameterizations and faster non-linear fitting techniques. The results of\n",
        "these simplified fits can then be used to initialize fits using a higher dimensionality model with more detailed\n",
        "non-linear search.\n",
        "\n",
        "To fit highly complex models our aim is therefore to granularize the fitting procedure into a series of **bite-sized**\n",
        "searches which are faster and more reliable than fitting the more complex model straight away.\n",
        "\n",
        "Our ability to construct chained non-linear searches that perform model fitting more accurately and efficiently relies\n",
        "on our **domain specific knowledge** of the model fitting task. For example, we may know that our dataset contains\n",
        "multiple features that can be accurately fitted separately before performing a joint fit, or that certain parameter\n",
        "share minimal covariance such that certain parameters can be fixed before fitting both with a more complex model\n",
        "parameterization.\n",
        "\n",
        "We may also know tricks that can speed up the fitting of the initial model, for example reducing the size of the data\n",
        "or changing the likelihood evaluations in a way that makes them quicker (likely at the expense of the overall\n",
        "quality of the fit itself). By using chained searches these speed-ups can be relaxed towards the end of the\n",
        "model-fitting sequence when we want the most precise, most accurate model that best fits the dataset available.\n",
        "\n",
        "In this example we demonstrate search chaining using the example data where there are two `Gaussians` that are visibly\n",
        "split. Instead of fitting them simultaneously using a single non-linear search consisting of N=6 parameters, we break\n",
        "this model-fit into a chained of three searches where:\n",
        "\n",
        " 1) The first model fits just the left `Gaussian` where N=3.\n",
        " 2) The first model fits just the right `Gaussian` where again N=3.\n",
        " 3) The final model is fitted with both `Gaussians` where N=6. Crucially, the results of the first two searches\n",
        " are used to initialize the search and tell it the highest likelihood regions of parameter space.\n",
        "\n",
        "By initially fitting parameter spaces of reduced complexity we can achieve a more efficient and reliable model-fitting\n",
        "procedure.\n",
        "\n",
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and\n",
        " `visualize` functions.\n",
        "\n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        "These are functionally identical to the `Analysis` and `Gaussian` objects you have seen elsewhere in the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data__\n",
        "\n",
        "Load data of two 1D Gaussians, by loading it from a .json file in the directory \n",
        "`autofit_workspace/dataset/gaussian_x1_with_feature`.\n",
        "\n",
        "This 1D data includes two `Gaussians` that are split from one another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x2_split\")\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets plot the data. \n",
        "\n",
        "Two separate `Gaussians` are clearly visible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = range(data.shape[0])\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.title(\"1D Gaussian Data With two Gaussians split apart.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "Create the analysis which fits the model to the data.\n",
        "\n",
        "It fits the data as the sum of as many `Gaussian`'s as are in the model.\n",
        "\n",
        "To better fit the left gaussian, we remove all data points in the right-half of the data. Note that for more \n",
        "computationally demanding model-fitting problems this would give a significant speed-up in log likelihood function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_1 = af.ex.Analysis(data=data[0:50], noise_map=noise_map[0:50])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We are now going to fit the left `Gaussian` in this split dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_1 = af.Collection(gaussian_left=af.ex.Gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model_1.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search 1__\n",
        "\n",
        "Fit the data with the `left_gaussian` using a single non-linear search. \n",
        "\n",
        "Given the simplicity of the model, we can use a low number of live points to achieve a fast model-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_1 = af.DynestyStatic(\n",
        "    name=\"search[1]__left_gaussian\",\n",
        "    path_prefix=path.join(\"features\", \"search_chaining\"),\n",
        "    nlive=30,\n",
        ")\n",
        "\n",
        "result_1 = search_1.fit(model=model_1, analysis=analysis_1)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result 1__\n",
        "\n",
        "The `info` attribute shows the result in a readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_1.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting the result we can see we have fitted the left `Gaussian` reasonably well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = result_1.max_log_likelihood_instance\n",
        "\n",
        "model_data = instance.gaussian_left.model_data_from(xvalues=np.arange(data.shape[0]))\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
        "plt.title(\"Search 1  fit to left Gaussian.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search 2 __\n",
        "\n",
        "We now repeat the above process for the right `Gaussian`.\n",
        "\n",
        "We could remove the data on the left like we did the `Gaussian` above. However, we are instead going to fit the full \n",
        "dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_2 = af.ex.Analysis(data=data, noise_map=noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Because the left Gaussian is now again in the data, we need to fit it. We can do this, without increasing the \n",
        "dimensionality of our parameter space to N=6, by using the maximum log likelihood model of the `gaussian_left` in\n",
        "search[1].\n",
        "\n",
        "For search chaining, **PyAutoFit** has many convenient methods for passing the results of a search to a subsequence \n",
        "search. Below, we achieve this by passing the result of the search above as an `instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_2 = af.Collection(\n",
        "    gaussian_left=result_1.instance.gaussian_left, gaussian_right=af.ex.Gaussian\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model, including how parameters and priors were passed from `result_1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model_2.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now run our second Dynesty search to fit the right `Gaussian`.\n",
        "\n",
        "Given the simplicity of the model, we can again use a low number of live points to achieve a fast model-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_2 = af.DynestyStatic(\n",
        "    name=\"search[2]__right_gaussian\",\n",
        "    path_prefix=path.join(\"features\", \"search_chaining\"),\n",
        "    nlive=30,\n",
        ")\n",
        "\n",
        "result_2 = search_2.fit(model=model_2, analysis=analysis_2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result 2__\n",
        "\n",
        "The `info` attribute shows the result in a readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_2.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting the result we can see we have fitted the right `Gaussian` reasonably well and that the model includes the\n",
        "`left_gaussian` from the first search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = result_2.max_log_likelihood_instance\n",
        "\n",
        "gaussian_left = instance.gaussian_left.model_data_from(xvalues=np.arange(data.shape[0]))\n",
        "gaussian_right = instance.gaussian_right.model_data_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "\n",
        "model_data = gaussian_left + gaussian_right\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
        "plt.plot(range(data.shape[0]), gaussian_left, \"--\")\n",
        "plt.plot(range(data.shape[0]), gaussian_right, \"--\")\n",
        "plt.title(\"Search 2 fit to right Gaussian.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search 3__\n",
        "\n",
        "We now fit both `Gaussians`'s simultaneously, using the results of the previous two searches to initialize where \n",
        "the non-linear searches parameter space.\n",
        "\n",
        "To pass the result in this way we use the command `result.model`, which in contrast to `result.instance` above passes\n",
        "the parameters not as the maximum log likelihood values but as `GaussianPrior`'s that are fitted for by the\n",
        "non-linear search. We discuss below how this `GaussianPrior` is passed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "model_3 = af.Collection(\n",
        "    gaussian_left=result_1.model.gaussian_left,\n",
        "    gaussian_right=result_2.model.gaussian_right,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model, including how parameters and priors were passed from `result_1` and `result_2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model_3.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now perform the search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_3 = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "search_3 = af.DynestyStatic(\n",
        "    name=\"search[3]__both_gaussians\",\n",
        "    path_prefix=path.join(\"features\", \"search_chaining\"),\n",
        "    nlive=100,\n",
        ")\n",
        "\n",
        "result_3 = search_3.fit(model=model_3, analysis=analysis_3)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting the result we can now see we have fitted both `Gaussian`'s accurately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = result_3.max_log_likelihood_instance\n",
        "\n",
        "gaussian_left = instance.gaussian_left.model_data_from(xvalues=np.arange(data.shape[0]))\n",
        "gaussian_right = instance.gaussian_right.model_data_from(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "\n",
        "model_data = gaussian_left + gaussian_right\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues, y=data, yerr=noise_map, color=\"k\", ecolor=\"k\", elinewidth=1, capsize=2\n",
        ")\n",
        "plt.plot(range(data.shape[0]), model_data, color=\"r\")\n",
        "plt.plot(range(data.shape[0]), gaussian_left, \"--\")\n",
        "plt.plot(range(data.shape[0]), gaussian_right, \"--\")\n",
        "plt.title(\"Search 3 fit to both Gaussian.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile normalization\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Prior Passing__\n",
        "\n",
        "Now search 3 is complete, you should checkout its `model.info` file. The parameters do not use the default priors of \n",
        "the `Gaussian` model component. Instead, they use GaussianPrior`s where:\n",
        "\n",
        " - The mean values are the median PDF results of every parameter inferred by the fits performed in searches 1 and 2.\n",
        " - They sigma values are the errors computed by these searches, or they are values higher than these errors.\n",
        "\n",
        "The sigma values uses the errors of searches 1 and 2 for an obvious reason, this is a reasonable estimate of where in\n",
        "parameter space the model-fit can be expected to provide a good fit to the data. However, we may want to specify \n",
        "even larger sigma values on certain parameters, if for example we anticipate that our earlier searches may under \n",
        "estimate the errors.\n",
        "\n",
        "The `width_modifier` term in the `Gaussian` section of the `config/priors/gaussian.yaml` is used instead of the errors \n",
        "of a search, when the errors estimated are smaller  than the `width_modifier` value. This ensure that the sigma \n",
        "values used in later searches do not assume extremely small values if earlier searches risk under estimating the errors.\n",
        "\n",
        "Thus, search 3 used the results of searches 1 and 2 to inform it where to search non-linear parameter space! \n",
        "\n",
        "The `prior_passer` settings in the `general.yaml` config customizes how priors are passed from a search as follows:\n",
        "\n",
        " - sigma: The sigma value of the errors passed to set the sigma values in the previous search are estimated at.\n",
        " - use_widths: If False, the \"width_modifier\" values in the json_prior configs are not used to override a passed\n",
        " error value.\n",
        " - use_errors: If False, errors are not passed from search 1 to set up the priors and only the `width_modifier`\n",
        "  entries in the configs are used.  \n",
        "\n",
        "There are two ways a value is specified using the priors/width file:\n",
        "\n",
        " 1) Absolute: In this case, the error assumed on the parameter is the value given in the config file. \n",
        "  For example, if for the width on `centre` the width modifier reads \"Absolute\" with a value 20.0, this means if the \n",
        "  error on the parameter `centre` was less than 20.0 in the previous search, the sigma of its `GaussianPrior` in \n",
        "  the next search will be 20.0.\n",
        "\n",
        " 2) Relative: In this case, the error assumed on the parameter is the % of the value of the estimate value given in \n",
        " the config file. For example, if the normalization estimated in the previous search was 2.0, and the relative error in \n",
        " the config file reads \"Relative\" with a value 0.5, then the sigma of the `GaussianPrior` \n",
        "  will be 50% of this value, i.e. sigma = 0.5 * 2.0 = 1.0.\n",
        "\n",
        "We use absolute and relative values for different parameters, depending on their properties. For example, using the \n",
        "relative value of a parameter like the `centre` makes no sense as the profile could be centred at 0.0, making \n",
        "the relative error tiny and poorly defined.\n",
        "\n",
        "However, there are parameters where using an absolute value does not make sense. Normalization is a good example of this. \n",
        "The normalization of an image depends on its units and S/N. There is no single absolute value that one can use to \n",
        "generically chain the normalization of any two proflies. Thus, it makes more sense to chain them using the relative value \n",
        "from a previous search.\n",
        "\n",
        "We can customize how priors are passed from the results of a search editing the `prior_passer` settings in \n",
        "the `general.yaml` config.\n",
        "\n",
        "__EXAMPLE__\n",
        "\n",
        "Lets go through an example using a real parameter. Lets say in search 1 we fit a `Gaussian` and we estimate that \n",
        "its normalization is equal to 4.0 +- 2.0 where the error value of 2.0 was computed at 3.0 sigma confidence. To pass this \n",
        "as a prior to search 2, we would write:\n",
        "\n",
        "    gaussian.normalization = search_1_result.model.gaussian.normalization\n",
        "\n",
        "The prior on the `Gaussian` `normalization` in search 2 would thus be a `GaussianPrior`, with mean=4.0 and \n",
        "sigma=2.0. If we had used a sigma value of 1.0 to compute the error, which reduced the estimate from 4.0 +- 2.0 to \n",
        "4.0 +- 1.0, the sigma of the `GaussianPrior` would instead be 1.0. \n",
        "\n",
        "If the error on the normalization in search 1 had been really small, lets say, 0.01, we would instead use the value of the \n",
        "normalization width in the priors config file to set sigma instead. In this case, the prior config file specifies \n",
        "that we use an \"Relative\" value of 0.5 to chain this prior. Thus, the GaussianPrior in search 2 would have a mean=4.0 \n",
        "and sigma=2.0.\n",
        "\n",
        "And with that, we`re done. Chaining searches is a bit of an art form, but for certain problems can be extremely \n",
        "powerful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# %%\n",
        "'''\n",
        "Cookbook 5: Model Linking\n",
        "=========================\n",
        "\n",
        "__Prerequisites__\n",
        "\n",
        "You should be familiar with the search chaining API detailed in the following scripts and docs:\n",
        "\n",
        "__Overview__\n",
        "\n",
        "Search chaining allows one to perform back-to-back non-linear searches to fit a dataset, where the model complexity\n",
        "increases after each fit.\n",
        "\n",
        "To perform search chaining, **PyAutoFit** has tools for passing the results of one model-fit from one fit to the next,\n",
        "and change its parameterization between each fit.\n",
        "\n",
        "This cookbook is a concise reference to the model linking API.\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import json\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model-Fit__\n",
        "\n",
        "We perform a quick model-fit, to create a `Result` object which has the attributes necessary to illustrate the model\n",
        "linking API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.ex.Gaussian, exponential=af.ex.Exponential)\n",
        "\n",
        "dataset_path = path.join(\"dataset\", \"example_1d\", \"gaussian_x1__exponential_x1\")\n",
        "\n",
        "data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "noise_map = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "dynesty = af.DynestyStatic(name=\"cookbook_5_model_linking\", nlive=50, sample=\"rwalk\")\n",
        "\n",
        "result = dynesty.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Instance & Model__\n",
        "\n",
        "The result object has two key attributes for model linking:\n",
        "\n",
        " - `instance`: The maximum log likelihood instance of the model-fit, where every parameter is therefore a float.\n",
        "\n",
        " - `model`: An attribute which represents how the result can be passed as a model-component to the next fit (the\n",
        " details of how its priors are passed are given in full below).\n",
        "\n",
        "Below, we create a new model using both of these attributes, where:\n",
        "\n",
        " - All of the `gaussian` model components parameters are passed via the `instance` attribute and therefore fixed to \n",
        " the inferred maximum log likelihood values (and are not free parameters in the model).\n",
        "\n",
        "  - All of the `exponential` model components parameters are passed via the `model` attribute and therefore are free\n",
        "  parameters in the model.\n",
        "\n",
        "The new model therefore has 3 free parameters and 3 fixed parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.ex.Gaussian, exponential=af.ex.Exponential)\n",
        "\n",
        "model.gaussian = result.instance.gaussian\n",
        "model.exponential = result.model.exponential"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `model.info` attribute shows that the parameter and prior passing has occurred as described above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can print the priors of the exponenital:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Exponential Model Priors \\n\")\n",
        "print(\"centre = \", model.exponential.centre)\n",
        "print(\"normalization = \", model.exponential.normalization)\n",
        "print(\"rate = \", model.exponential.rate)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How are the priors set via model linking? The full description is quite long, therefore it is attatched to the\n",
        "bottom of this script so that we can focus on the model linking API.\n",
        "\n",
        "__Component Specification__\n",
        "\n",
        "Model linking can be performed on any component of a model, for example to only pass specific parameters as \n",
        "an `instance` or `model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian = af.Model(af.ex.Gaussian)\n",
        "\n",
        "gaussian.centre = result.instance.gaussian.centre\n",
        "gaussian.normalization = result.model.gaussian.normalization\n",
        "gaussian.sigma = result.instance.gaussian.sigma\n",
        "\n",
        "exponential = af.Model(af.ex.Exponential)\n",
        "\n",
        "exponential.centre = result.model.exponential.centre\n",
        "exponential.normalization = result.model.exponential.normalization\n",
        "exponential.rate = result.instance.exponential.rate\n",
        "\n",
        "model = af.Collection(gaussian=gaussian, exponential=exponential)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `model.info` attribute shows that the parameter and prior passing has occurred on individual components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Take Attributes__\n",
        "\n",
        "The examples above linked models where the individual model components that were passed stayed the same.\n",
        "\n",
        "We can link two related models, where only a subset of parameters are shared, by using the `take_attributes()` method. \n",
        "\n",
        "For example, lets define a `GaussianKurtosis` which is a `Gaussian` with an extra parameter for its kurtosis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class GaussianKurtosis:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=30.0,  # <- **PyAutoFit** recognises these constructor arguments\n",
        "        normalization=1.0,  # <- are the Gaussian``s model parameters.\n",
        "        sigma=5.0,\n",
        "        kurtosis=1.0,\n",
        "    ):\n",
        "        self.centre = centre\n",
        "        self.normalization = normalization\n",
        "        self.sigma = sigma\n",
        "        self.kurtosis = kurtosis\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `take_attributes()` method takes a `source` model component, and inspects the names of all its parameters. \n",
        "\n",
        "For  the `Gaussian` model result input below, it finds the parameters `centre`, `normalization` and `sigma`.\n",
        "\n",
        "It then finds all parameters in the new `model` which have the same names, which for the `GaussianKurtosis` is\n",
        "`centre`, `normalization` and `sigma`.\n",
        "\n",
        "For all parameters which have the same name, the parameter is passed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.Model(GaussianKurtosis))\n",
        "model.kurtosis = af.UniformPrior(lower_limit=-1.0, upper_limit=1.0)\n",
        "\n",
        "model.gaussian.take_attributes(source=result.model.gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the result was passed using `model` we see the priors on the `GaussianKurtosis` `centre`, \n",
        "`normalization` and `sigma` have been updated, whereas its `kurtosis` has not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"GaussianKurtosis Model Priors After Take Attributes via Model \\n\")\n",
        "print(\"centre = \", model.gaussian.centre)\n",
        "print(\"normalization = \", model.gaussian.normalization)\n",
        "print(\"sigma = \", model.gaussian.sigma)\n",
        "print(\"kurtosis = \", model.gaussian.kurtosis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we pass `result.instance` to take_attributes the same name linking is used, however parameters are passed as\n",
        "floats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(gaussian=af.Model(GaussianKurtosis))\n",
        "model.kurtosis = af.UniformPrior(lower_limit=-1.0, upper_limit=1.0)\n",
        "\n",
        "model.gaussian.take_attributes(source=result.instance.gaussian)\n",
        "\n",
        "print(\"Gaussian Model Priors After Take Attributes via Instance \\n\")\n",
        "print(\"centre = \", model.gaussian.centre)\n",
        "print(\"normalization = \", model.gaussian.normalization)\n",
        "print(\"sigma = \", model.gaussian.sigma)\n",
        "print(\"kurtosis = \", model.gaussian.kurtosis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__As Model__\n",
        "\n",
        "A common problem is when we have an `instance` (e.g. from a previous fit where we fixed the parameters)\n",
        "but now wish to make its parameters free parameters again.\n",
        "\n",
        "Furthermore, we may want to do this for specific model components.\n",
        "\n",
        "The `as_model` method allows us to do this. Below, we pass the entire result (e.g. both the `gaussian` \n",
        "and `exponential` components), however we pass the `Gaussian` class to `as_model`, meaning that any model\n",
        "component in the `instance` which is a `Gaussian` will be converted to a model with free parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = result.instance.as_model((af.ex.Gaussian,))\n",
        "\n",
        "print(\"Gaussian Model Priors After via as_model: \\n\")\n",
        "print(\"centre = \", model.gaussian.centre)\n",
        "print(\"normalization = \", model.gaussian.normalization)\n",
        "print(\"sigma = \", model.gaussian.sigma)\n",
        "print(\"centre = \", model.exponential.centre)\n",
        "print(\"normalization = \", model.exponential.normalization)\n",
        "print(\"rate= \", model.exponential.rate)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `as_model()` method does not have too much utility for the simple model used in this cookbook. \n",
        "\n",
        "However, for multi-level models with many components, it is a powerful tool to compose custom models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class MultiLevelProfiles:\n",
        "    def __init__(\n",
        "        self,\n",
        "        higher_level_centre=50.0,  # This is the centre of all Gaussians in this multi level component.\n",
        "        profile_list=None,  # This will contain a list of model-components\n",
        "    ):\n",
        "        self.higher_level_centre = higher_level_centre\n",
        "\n",
        "        self.profile_list = profile_list\n",
        "\n",
        "\n",
        "group_level_0 = af.Model(\n",
        "    MultiLevelProfiles, profile_list=[af.ex.Gaussian, af.ex.Exponential, af.ex.Gaussian]\n",
        ")\n",
        "\n",
        "group_level_1 = af.Model(\n",
        "    MultiLevelProfiles,\n",
        "    profile_list=[af.ex.Gaussian, af.ex.Exponential, af.ex.Exponential],\n",
        ")\n",
        "\n",
        "model = af.Collection(group_level_0=group_level_0, group_level_1=group_level_1)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This means every `Gaussian` in the complex multi-level model above would  have parameters set via the result of our\n",
        "model-fit, if the model above was fitted such that it was contained in the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = result.instance.as_model((af.ex.Gaussian,))\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}