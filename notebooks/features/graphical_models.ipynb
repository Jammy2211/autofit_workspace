{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature: Graphical Models\n",
        "=========================\n",
        "\n",
        "NOTE: Graphical models are an in-development feature. This example serves to illustrate what we currently developing ,\n",
        "but the API is subject to change and many core features (all searches, database tools, etc.) are not\n",
        "fully supported. If you are interested in using graphical models I recommend you contact me directly\n",
        "(https://github.com/Jammy2211) so we can discuss when **PyAutoFit** will be ready for your use-case.\n",
        "\n",
        "The examples so far have focused on fitting one model to one dataset, for example fitting 1D profiles composed of\n",
        "Gaussians to noisy 1D data. When multiple datasets were available each was fitted individually and their results\n",
        "interpreted one-by-one using the `Result` object or **PyAutoFit**'s database tools.\n",
        "\n",
        "However, for many problems we may have a large dataset and not be interested in how well the model fits each dataset\n",
        "individually. Instead, we may wish to fit this model (or many similar models) to the full dataset and determine\n",
        "the 'global' trends of the model across the datasets.\n",
        "\n",
        "This can be done using graphical models, which compose and fit a model that has 'local' parameters specific to each\n",
        "individual dataset but also higher-level model components that fit 'global' parameters of the model across the whole\n",
        "dataset. This framework can be easily extended to fit datasets with different properties, models with different\n",
        "topologies and has the functionality to allow it to be generalized to models with thousands of parameters.\n",
        "\n",
        "In this example, we demonstrate the basic API for performing graphical modeling in **PyAutoFit** using the example of\n",
        "fitting noisy 1D Gaussians. However, graphical models are an extensive feature and at the end of this example we will\n",
        "discuss the many options available in **PyAutoFit** for composing a fitting a graphical model. The **HowToFit**\n",
        "tutorials contain a chapter dedicated to composing and complex graphical models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autofit as af\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to build a graphical model that fits three datasets. \n",
        "\n",
        "NOTE: In this example we explicitly write code for loading each dataset and store each dataset as their own Python\n",
        "variable (e.g. data_0, data_1, data_2, etc.). We do not use a for loop or a list to do this, even though this would \n",
        "be syntactically cleaner code. This is to make the API for setting up a graphical model in this example clear and \n",
        "explicit; in the **HowToFit** tutorials we we introduce the **PyAutoFit** API for setting up a graphical model for \n",
        "large datasets concisely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"example_1d\")\n",
        "\n",
        "dataset_0_path = path.join(dataset_path, \"gaussian_x1_0__low_snr\")\n",
        "data_0 = af.util.numpy_array_from_json(file_path=path.join(dataset_0_path, \"data.json\"))\n",
        "noise_map_0 = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_0_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "dataset_1_path = path.join(dataset_path, \"gaussian_x1_1__low_snr\")\n",
        "data_1 = af.util.numpy_array_from_json(file_path=path.join(dataset_1_path, \"data.json\"))\n",
        "noise_map_1 = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_1_path, \"noise_map.json\")\n",
        ")\n",
        "\n",
        "dataset_2_path = path.join(dataset_path, \"gaussian_x1_2__low_snr\")\n",
        "data_2 = af.util.numpy_array_from_json(file_path=path.join(dataset_2_path, \"data.json\"))\n",
        "noise_map_2 = af.util.numpy_array_from_json(\n",
        "    file_path=path.join(dataset_2_path, \"noise_map.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets plot the data, including their error bars. One should note that we are fitting much lower signal-to-noise\n",
        "datasets than usual.\n",
        "\n",
        "Note that all three of these `Gaussian`'s were simulated using the same `centre` value. To demonstrate graphical \n",
        "modeling we will therefore fit a model where the `centre` a shared parameter across the fit to the 3 `Gaussian`s, \n",
        "therefore making it a global parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xvalues = range(data_0.shape[0])\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=data_0,\n",
        "    yerr=noise_map_0,\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        ")\n",
        "plt.title(\"1D Gaussian Data #1.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile intensity\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "xvalues = range(data_1.shape[0])\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=data_1,\n",
        "    yerr=noise_map_1,\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        ")\n",
        "plt.title(\"1D Gaussian Data #2.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile intensity\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "xvalues = range(data_0.shape[0])\n",
        "\n",
        "plt.errorbar(\n",
        "    x=xvalues,\n",
        "    y=data_0,\n",
        "    yerr=noise_map_0,\n",
        "    color=\"k\",\n",
        "    ecolor=\"k\",\n",
        "    elinewidth=1,\n",
        "    capsize=2,\n",
        ")\n",
        "plt.title(\"1D Gaussian Data #3.\")\n",
        "plt.xlabel(\"x values of profile\")\n",
        "plt.ylabel(\"Profile intensity\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each dataset we now create a corresponding `Analysis` class (we use the `Analysis` class from `complex/fit`).\n",
        "\n",
        "By associating each dataset with an `Analysis` class we are therefore associating it with a unique \n",
        "`log_likelihood_function`. Because every dataset has its own `Analysis` class, it is straight forward to build a \n",
        "graphical model where datasets with different structures or formats are fitted. This is demonstrated in the graphical\n",
        "models chapter of **HowToFit**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import analysis as a\n",
        "\n",
        "analysis_0 = a.Analysis(data=data_0, noise_map=noise_map_0)\n",
        "analysis_1 = a.Analysis(data=data_1, noise_map=noise_map_1)\n",
        "analysis_2 = a.Analysis(data=data_2, noise_map=noise_map_2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now compose the graphical model we will fit using the `PriorModel` and `CollectionPriorModel` objects described in \n",
        "the `overview` examples and chapter 1 of **HowToFit**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit import graphical as g\n",
        "import model as m"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We begin by setting up a shared prior for `centre`, which is set up this up as a single `GaussianPrior` that is \n",
        "passed to a unique `PriorModel` for each `Gaussian`. This means all three `Gaussian`'s will be fitted wih the same \n",
        "value of `centre`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "centre_shared_prior = af.GaussianPrior(mean=50.0, sigma=30.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now set up three `CollectionPriorModel`'s, each of which contain a `Gaussian` that is used to fit each of the \n",
        "datasets we loaded above.\n",
        "\n",
        "All three of these `CollectionPriorModel`'s use the `centre_shared_prior`. This means all three `Gaussian` \n",
        "model-components use the same value of `centre` when model-fitting is performed reducing the dimensionality of\n",
        "parameter space from N=9 (e.g. 3 parameters per Gaussian) to N=7.\n",
        "\n",
        "Our graphical model therefore consists of three `Gaussians` with local parameters (a unique `intensity` and `sigma`)\n",
        "for each `Gaussian` and a global parameter for the graphical model (the `centre` of all three `Gaussians`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gaussian_0 = af.PriorModel(m.Gaussian)\n",
        "gaussian_0.centre = centre_shared_prior\n",
        "gaussian_0.intensity = af.GaussianPrior(mean=10.0, sigma=10.0)\n",
        "gaussian_0.sigma = af.GaussianPrior(\n",
        "    mean=10.0, sigma=10.0\n",
        ")  # This prior is used by all 3 Gaussians!\n",
        "\n",
        "prior_model_0 = af.CollectionPriorModel(gaussian=gaussian_0)\n",
        "\n",
        "gaussian_1 = af.PriorModel(m.Gaussian)\n",
        "gaussian_1.centre = centre_shared_prior\n",
        "gaussian_1.intensity = af.GaussianPrior(mean=10.0, sigma=10.0)\n",
        "gaussian_1.sigma = af.GaussianPrior(\n",
        "    mean=10.0, sigma=10.0\n",
        ")  # This prior is used by all 3 Gaussians!\n",
        "\n",
        "prior_model_1 = af.CollectionPriorModel(gaussian=gaussian_1)\n",
        "\n",
        "gaussian_2 = af.PriorModel(m.Gaussian)\n",
        "gaussian_2.centre = centre_shared_prior\n",
        "gaussian_2.intensity = af.GaussianPrior(mean=10.0, sigma=10.0)\n",
        "gaussian_2.sigma = af.GaussianPrior(\n",
        "    mean=10.0, sigma=10.0\n",
        ")  # This prior is used by all 3 Gaussians!\n",
        "\n",
        "prior_model_2 = af.CollectionPriorModel(gaussian=gaussian_2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we composed a model consisting of three `Gaussian`'s with a shared `centre` prior. We also loaded three \n",
        "datasets which we intend to fit with each of these `Gaussians`, setting up each in an `Analysis` class that defines \n",
        "how the model is used to fit the data.\n",
        "\n",
        "We now simply need to pair each model-component to each `Analysis` class, so that **PyAutoFit** knows that: \n",
        "\n",
        "- `prior_model_0` fits `data_0` via `analysis_0`.\n",
        "- `prior_model_1` fits `data_1` via `analysis_1`.\n",
        "- `prior_model_2` fits `data_2` via `analysis_2`.\n",
        "\n",
        "The point where a `PriorModel` and `Analysis` class meet is called a `ModelFactor`. \n",
        "\n",
        "This term is used to denote that we are composing a graphical model, which is commonly termed a 'factor graph'. A \n",
        "factor defines a node on this graph where we have some data, a model, and we fit the two together. The 'links' between \n",
        "these different nodes then define the global model we are fitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_factor_0 = g.ModelFactor(prior_model=prior_model_0, analysis=analysis_0)\n",
        "model_factor_1 = g.ModelFactor(prior_model=prior_model_1, analysis=analysis_1)\n",
        "model_factor_2 = g.ModelFactor(prior_model=prior_model_2, analysis=analysis_2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We combine our `ModelFactors` into one, to compose the factor graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = g.FactorGraphModel(model_factor_0, model_factor_1, model_factor_2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, what is a factor graph?\n",
        "\n",
        "A factor graph defines the graphical model we have composed. For example, it defines the different model components \n",
        "that make up our model (e.g. the three `Gaussian` classes) and how their parameters are linked or shared (e.g. that\n",
        "each `Gaussian` has its own unique `intensity` and `sigma`, but a shared `centre` parameter.\n",
        "\n",
        "This is what our factor graph looks like: \n",
        "\n",
        "The factor graph above is made up of two components:\n",
        "\n",
        "- Nodes: these are points on the graph where we have a unique set of data and a model that is made up of a subset of \n",
        "our overall graphical model. This is effectively the `ModelFactor` objects we created above. \n",
        "\n",
        "- Links: these define the model components and parameters that are shared across different nodes and thus retain the \n",
        "same values when fitting different datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.graphical import optimise\n",
        "\n",
        "laplace = optimise.LaplaceFactorOptimiser()\n",
        "collection = factor_graph.optimise(laplace)\n",
        "\n",
        "print(collection)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Road Map__\n",
        "\n",
        "The example above illustrates a simple graphical model where 3 datasets are fitted. Each fit has two local parameters\n",
        "and one global parameter. \n",
        "\n",
        "Graphical models are an in development feature and the following functionality is currently in development:\n",
        "\n",
        " - Searches: Currently a graphical model must be fitted using the `LaplaceOptimiser` class. They therefore do not \n",
        " support `NonLinearSearch`'s like `Emcee` and `Dynesty` nor do they support results output, visualization or database\n",
        " outputs. Graphical models will soon fully support all `NonLinearSearch`'s objects and therefore also outputs, \n",
        " visualization and database features.\n",
        " \n",
        " - Message passing: The graphical model above fitted all 7 parameters simultaneously. If the dataset was large the\n",
        " number of parameters would increase drastically, making it inefficient or impossible to fit the model (for example,\n",
        " with 1000 `Gaussian`'s our model would have had 2001 parameters!). Graphical models in **PyAutoFit** support the\n",
        " message passing framework below, which allows one to fit the local model to every dataset individually and pass \n",
        " messages 'up and down' the graph to infer the global parameters efficiently.\n",
        " \n",
        " https://arxiv.org/pdf/1412.4869.pdf\n",
        "\n",
        " - More advanced models: The model above fitted a single global parameter which was shared across the dataset. The\n",
        " framework will allow one to fit for the distributions from which parameters are draw or for trends between parameters.\n",
        " The graphical model framework itself can already do this, we simply need to write the **HowToFit** tutorials that will\n",
        " demonstrate how!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}