{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature: Graphical Models\n",
        "=========================\n",
        "\n",
        "The examples so far have focused on fitting one model to one dataset, for example fitting 1D profiles composed of\n",
        "Gaussians to noisy 1D data. When multiple datasets were available each is fitted individually and their results\n",
        "interpreted one-by-one.\n",
        "\n",
        "However, for many problems we may have a large dataset and not be interested in how well the model fits each dataset\n",
        "individually. Instead, we may wish to fit this model (or many similar models) to the full dataset and determine\n",
        "the 'global' trends of the model across the datasets.\n",
        "\n",
        "This can be done using graphical models, which compose and fit a model that has 'local' parameters specific to each\n",
        "individual dataset but also higher-level model components that fit 'global' parameters of the model across the whole\n",
        "dataset. This framework can be easily extended to fit datasets with different properties, complex models with different\n",
        "topologies and has the functionality to allow it to be generalized to models with thousands of parameters.\n",
        "\n",
        "In this example, we demonstrate the basic API for performing graphical modeling in **PyAutoFit** using the example of\n",
        "simultaneously fitting 3 noisy 1D Gaussians. However, graphical models are an extensive feature and at the end of\n",
        "this example we will discuss other options available in **PyAutoFit** for composing a fitting a graphical model.\n",
        "\n",
        "The **HowToFit** tutorials contain a chapter dedicated to composing and fitting graphical models.\n",
        "\n",
        "__Example Source Code (`af.ex`)__\n",
        "\n",
        "The **PyAutoFit** source code has the following example objects (accessed via `af.ex`) used in this tutorial:\n",
        "\n",
        " - `Analysis`: an analysis object which fits noisy 1D datasets, including `log_likelihood_function` and \n",
        " `visualize` functions.\n",
        " \n",
        " - `Gaussian`: a model component representing a 1D Gaussian profile.\n",
        "\n",
        "These are functionally identical to the `Analysis` and `Gaussian` objects you have seen elsewhere in the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "We are going to build a graphical model that fits three datasets. \n",
        "\n",
        "We begin by loading noisy 1D data containing 3 Gaussian's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "total_datasets = 3\n",
        "\n",
        "dataset_name_list = []\n",
        "data_list = []\n",
        "noise_map_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "\n",
        "    dataset_path = path.join(\n",
        "        \"dataset\", \"example_1d\", \"gaussian_x1__low_snr\", dataset_name\n",
        "    )\n",
        "\n",
        "    data = af.util.numpy_array_from_json(file_path=path.join(dataset_path, \"data.json\"))\n",
        "    noise_map = af.util.numpy_array_from_json(\n",
        "        file_path=path.join(dataset_path, \"noise_map.json\")\n",
        "    )\n",
        "\n",
        "    data_list.append(data)\n",
        "    noise_map_list.append(noise_map)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets plot the data, including their error bars. One should note that we are fitting much lower signal-to-noise\n",
        "datasets than usual.\n",
        "\n",
        "Note that all three of these `Gaussian`'s were simulated using the same `centre` value. To demonstrate graphical \n",
        "modeling we will therefore fit a model where the `centre` a shared parameter across the fit to the 3 `Gaussian`s, \n",
        "therefore making it a global parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_index in range(total_datasets):\n",
        "    xvalues = range(data_list[dataset_index].shape[0])\n",
        "\n",
        "    plt.errorbar(\n",
        "        x=xvalues,\n",
        "        y=data_list[dataset_index],\n",
        "        yerr=noise_map_list[dataset_index],\n",
        "        linestyle=\"\",\n",
        "        color=\"k\",\n",
        "        ecolor=\"k\",\n",
        "        elinewidth=1,\n",
        "        capsize=2,\n",
        "    )\n",
        "    plt.title(\"1D Gaussian Data #1.\")\n",
        "    plt.xlabel(\"x values of profile\")\n",
        "    plt.ylabel(\"Profile normalization\")\n",
        "    plt.show()\n",
        "    plt.close()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "They are much lower signal-to-noise than the Gaussian's in other examples. \n",
        "\n",
        "Graphical models extract a lot more information from lower quantity datasets, something we demonstrate explicitly \n",
        "in the **HowToFit** lectures on graphical models.\n",
        "\n",
        "For each dataset we now create a corresponding `Analysis` class. By associating each dataset with an `Analysis`\n",
        "class we are therefore associating it with a unique `log_likelihood_function`. \n",
        "\n",
        "If our dataset had many different formats which each required their own unique `log_likelihood_function`, it would \n",
        "be straight forward to write customized `Analysis` classes for each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for data, noise_map in zip(data_list, noise_map_list):\n",
        "    analysis = af.ex.Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We now compose the graphical model we will fit using the `Model` objects described in  the `overview` examples \n",
        "and chapter 1 of **HowToFit**.\n",
        "\n",
        "We begin by setting up a shared prior for `centre`, which is set up this up as a single `GaussianPrior` that is \n",
        "passed to a unique `Model` for each `Gaussian`. This means all three `Gaussian`'s will be fitted wih the same \n",
        "value of `centre`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "centre_shared_prior = af.GaussianPrior(mean=50.0, sigma=30.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now set up three `Model`'s, each of which contain a `Gaussian` that is used to fit each of the \n",
        "datasets we loaded above.\n",
        "\n",
        "All three of these `Model`'s use the `centre_shared_prior`. This means all three model-components use \n",
        "the same value of `centre` for every model composed and fitted by the `NonLinearSearch`, reducing the dimensionality \n",
        "of parameter space from N=9 (e.g. 3 parameters per Gaussian) to N=7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_list = []\n",
        "\n",
        "for model_index in range(len(data_list)):\n",
        "    gaussian = af.Model(af.ex.Gaussian)\n",
        "\n",
        "    gaussian.centre = centre_shared_prior  # This prior is used by all 3 Gaussians!\n",
        "    gaussian.normalization = af.LogUniformPrior(lower_limit=1e-6, upper_limit=1e6)\n",
        "    gaussian.sigma = af.UniformPrior(lower_limit=0.0, upper_limit=25.0)\n",
        "\n",
        "    model_list.append(gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factors__\n",
        "\n",
        "Above, we composed a model consisting of three `Gaussian`'s with a shared `centre` prior. We also loaded three \n",
        "datasets which we intend to fit with each of these `Gaussians`, setting up each in an `Analysis` class that defines \n",
        "how the model is used to fit the data.\n",
        "\n",
        "We now simply need to pair each model-component to each `Analysis` class, so that:\n",
        "\n",
        "- `prior_model_0` fits `data_0` via `analysis_0`.\n",
        "- `prior_model_1` fits `data_1` via `analysis_1`.\n",
        "- `prior_model_2` fits `data_2` via `analysis_2`.\n",
        "\n",
        "The point where a `Model` and `Analysis` class meet is called a `AnalysisFactor`. \n",
        "\n",
        "This term is used to denote that we are composing a graphical model, which is commonly termed a 'factor graph'. A \n",
        "factor defines a node on this graph where we have some data, a model, and we fit the two together. The 'links' between \n",
        "these different nodes then define the global model we are fitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "We combine our `AnalysisFactors` into one, to compose the factor graph.\n",
        "\n",
        "So, what is a factor graph?\n",
        "\n",
        "A factor graph defines the graphical model we have composed. For example, it defines the different model components \n",
        "that make up our model (e.g. the three `Gaussian` classes) and how their parameters are linked or shared (e.g. that\n",
        "each `Gaussian` has its own unique `normalization` and `sigma`, but a shared `centre` parameter.\n",
        "\n",
        "This is what our factor graph looks like: \n",
        "\n",
        "The factor graph above is made up of two components:\n",
        "\n",
        "- Nodes: these are points on the graph where we have a unique set of data and a model that is made up of a subset of \n",
        "our overall graphical model. This is effectively the `AnalysisFactor` objects we created above. \n",
        "\n",
        "- Links: these define the model components and parameters that are shared across different nodes and thus retain the \n",
        "same values when fitting different datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "We can now create a non-linear search and used it to the fit the factor graph, using its `global_prior_model` property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=\"features\", name=\"graphical_model\", sample=\"rwalk\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will fit the N=7 dimension parameter space where every Gaussian has a shared centre!\n",
        "\n",
        "This is all expanded upon in the **HowToFit** chapter on graphical models, where we will give a more detailed\n",
        "description of why this approach to model-fitting extracts a lot more information than fitting each dataset\n",
        "one-by-one.\n",
        "\n",
        "__Hierarchical Models__\n",
        "\n",
        "A specific type of graphical model is a hierarchical model, where the shared parameter(s) of a graph are assumed\n",
        "to be drawn from a common parent distribution. \n",
        "\n",
        "Fitting the datasets simultaneously enables better estimate of this global hierarchical distribution.\n",
        "\n",
        "__Expectation Propagation__\n",
        "\n",
        "For large datasets, a graphical model may have hundreds, thousands, or *hundreds of thousands* of parameters. The\n",
        "high dimensionality of such a parameter space can make it inefficient or impossible to fit the model.\n",
        "\n",
        "Fitting high dimensionality graphical models in **PyAutoFit** can use an Expectation Propagation (EP) framework to \n",
        "make scaling up feasible. This framework fits every dataset individually and pass messages throughout the graph to \n",
        "inform every fit the expected \n",
        "values of each parameter.\n",
        "\n",
        "The following paper describes the EP framework in formal Bayesian notation:\n",
        "\n",
        "https://arxiv.org/pdf/1412.4869.pdf\n",
        "\n",
        "Hierarchical models can also be scaled up to large datasets via Expectation Propagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}