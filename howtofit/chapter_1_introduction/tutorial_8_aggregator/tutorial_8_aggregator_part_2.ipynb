{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 8: Aggregator Part 2\n",
        "=============================\n",
        "\n",
        "In part 1 of tutorial 8, we fitted 3 datasets and used the aggregator to load their results. We focused on the\n",
        "results of the non-linear search, Emcee. In part 2, we'll look at how the way we designed our source code\n",
        "makes it easy to use these results to plot results and data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from autoconf import conf\n",
        "import autofit as af\n",
        "from autofit_workspace.howtofit.chapter_1_introduction.tutorial_8_aggregator import (\n",
        "    src as htf,\n",
        ")\n",
        "\n",
        "from pyprojroot import here\n",
        "\n",
        "workspace_path = str(here())\n",
        "print(\"Workspace Path: \", workspace_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup the configs as we did in the previous tutorial, as well as the output folder for our non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conf.instance = conf.Config(\n",
        "    config_path=f\"{workspace_path}/howtofit/config\",\n",
        "    output_path=f\"{workspace_path}/howtofit/output\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To load these results with the aggregator, we again point it to the path of the results we want it to inspect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "output_path = f\"{workspace_path}/howtofit/output\"\n",
        "\n",
        "agg = af.Aggregator(directory=str(output_path))\n",
        "phase_name = \"phase_t8\"\n",
        "agg_filter = agg.filter(agg.phase == phase_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the aggregator to load a generator of every fit's dataset, by changing the 'output' attribute to the \n",
        "'dataset' attribute at the end of the aggregator. We'll filter by phase name again to get datasets of only the fits \n",
        "performed for this tutorial.\n",
        "\n",
        "Note that we had to manually specify in the 'phase.py' for the dataset to be saved too hard-disk such that the \n",
        "aggregator can load it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_gen = agg_filter.values(\"dataset\")\n",
        "print(\"Datasets:\")\n",
        "print(list(dataset_gen), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is here the object-based design of our plot module comes into its own. We have the dataset objects loaded, meaning\n",
        "we can easily plot each dataset using the 'dataset_plot.py' module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset in agg_filter.values(\"dataset\"):\n",
        "    htf.plot.Dataset.data(dataset=dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset names are available, either as part of the dataset or via the aggregator's dataset_names method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset in agg_filter.values(\"dataset\"):\n",
        "    print(dataset.name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The info dictionary we input into the pipeline is also available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for info in agg_filter.values(\"info\"):\n",
        "    print(info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can repeat the same trick to get the mask of every fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask_gen = agg_filter.values(\"mask\")\n",
        "print(\"Masks:\")\n",
        "print(list(mask_gen), \"\\n\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're going to refer to our datasets using the best-fit model of each phase. To do this, we'll need each phase's masked\n",
        "dataset.\n",
        "\n",
        "(If you are unsure what the 'zip' is doing below, it essentially combines the'datasets' and 'masks' lists in such\n",
        "a way that we can iterate over the two simultaneously to create each MaskedDataset).\n",
        "\n",
        "The masked dataset may have been altered by the data_trim_ custom phase settings. We can load the meta_dataset via the \n",
        "aggregator to use these settings when we create the masked dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_gen = agg_filter.values(\"dataset\")\n",
        "mask_gen = agg_filter.values(\"mask\")\n",
        "\n",
        "masked_datasets = [\n",
        "    htf.MaskedDataset(dataset=dataset, mask=mask)\n",
        "    for dataset, mask in zip(dataset_gen, mask_gen)\n",
        "]\n",
        "\n",
        "masked_datasets = [\n",
        "    masked_dataset.with_left_trimmed(data_trim_left=meta_dataset.data_trim_left)\n",
        "    for masked_dataset, meta_dataset in zip(\n",
        "        masked_datasets, agg_filter.values(\"meta_dataset\")\n",
        "    )\n",
        "]\n",
        "masked_datasets = [\n",
        "    masked_dataset.with_right_trimmed(data_trim_right=meta_dataset.data_trim_right)\n",
        "    for masked_dataset, meta_dataset in zip(\n",
        "        masked_datasets, agg_filter.values(\"meta_dataset\")\n",
        "    )\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a problem with how we set up the masked datasets above, can you guess what it is?\n",
        "\n",
        "We used lists! If we had fit a large sample of data, the above object would store the masked dataset of all objects\n",
        "simultaneously in memory on our hard-disk, likely crashing our laptop! To avoid this, we must write functions that\n",
        "manipulate the aggregator generators as generator themselves. Below is an example function that performs the same\n",
        "task as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def masked_dataset_from_agg_obj(agg_obj):\n",
        "\n",
        "    dataset = agg_obj.dataset\n",
        "    mask = agg_obj.mask\n",
        "\n",
        "    masked_dataset = htf.MaskedDataset(dataset=dataset, mask=mask)\n",
        "\n",
        "    meta_dataset = agg_obj.meta_dataset\n",
        "\n",
        "    masked_dataset = masked_dataset.with_left_trimmed(\n",
        "        data_trim_left=meta_dataset.data_trim_left\n",
        "    )\n",
        "    masked_dataset = masked_dataset.with_right_trimmed(\n",
        "        data_trim_right=meta_dataset.data_trim_right\n",
        "    )\n",
        "\n",
        "    return masked_dataset\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To manipulate this function as a generator using the aggregator, we must apply it to the aggregator's map function.\n",
        "\n",
        "The masked_dataset_generator below ensures that we avoid representing all masked datasets simultaneously in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "masked_dataset_gen = agg_filter.map(func=masked_dataset_from_agg_obj)\n",
        "print(list(masked_dataset_gen))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets get the the maximum likelihood model instances, as we did in part 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instances = [\n",
        "    samps.max_log_likelihood_instance for samps in agg_filter.values(\"samples\")\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, we want to inspect the fit of each best-fit model. To do this, we reperform each fit.\n",
        "\n",
        "First, we need to create the model-data of every best-fit model instance. Lets begin by creating a list of profiles of\n",
        "every phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "profiles = [instance.profiles for instance in instances]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use these to create the model data of each set of profiles (Which in this case is just 1 Gaussian, but had\n",
        "we included more profiles in the model would consist of multiple Gaussians / Exponentials)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_datas = [\n",
        "    profile.gaussian.profile_from_xvalues(xvalues=dataset.xvalues)\n",
        "    for profile, dataset in zip(profiles, agg_filter.values(\"dataset\"))\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And, as we did in tutorial 2, we can combine the masked_datasets and model_datas in a Fit object to create the\n",
        "maximum likelihood fit of each phase!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fits = [\n",
        "    htf.FitDataset(masked_dataset=masked_dataset, model_data=model_data)\n",
        "    for masked_dataset, model_data in zip(masked_datasets, model_datas)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now plot different components of the fit (again benefiting from how we set up the 'fit_plots.py' module)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for fit in fits:\n",
        "    htf.plot.FitDataset.residual_map(fit=fit)\n",
        "    htf.plot.FitDataset.normalized_residual_map(fit=fit)\n",
        "    htf.plot.FitDataset.chi_squared_map(fit=fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, the code above does not use generators and could prove memory intensive for large datasets. Below is how we \n",
        "would perform the above task with generator functions, using the masked_dataset_generator above for the masked \n",
        "dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def model_data_from_agg_obj(agg_obj):\n",
        "    xvalues = agg_obj.dataset.xvalues\n",
        "    instance = agg_obj.samples.max_log_likelihood_instance\n",
        "    profiles = instance.profiles\n",
        "    model_data = sum(\n",
        "        [profile.profile_from_xvalues(xvalues=xvalues) for profile in profiles]\n",
        "    )\n",
        "\n",
        "    return model_data\n",
        "\n",
        "\n",
        "def fit_from_agg_obj(agg_obj):\n",
        "    masked_dataset = masked_dataset_from_agg_obj(agg_obj=agg_obj)\n",
        "    model_data = model_data_from_agg_obj(agg_obj=agg_obj)\n",
        "\n",
        "    return htf.FitDataset(masked_dataset=masked_dataset, model_data=model_data)\n",
        "\n",
        "\n",
        "fit_gen = agg_filter.map(func=fit_from_agg_obj)\n",
        "\n",
        "for fit in fit_gen:\n",
        "    htf.plot.FitDataset.residual_map(fit=fit)\n",
        "    htf.plot.FitDataset.normalized_residual_map(fit=fit)\n",
        "    htf.plot.FitDataset.chi_squared_map(fit=fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting up the above objects (the masked_datasets, model datas, fits) was a bit of work. It wasn't too many\n",
        "lines of code, but for something we'll likely want to do many times it'd be nice to have a short cut to setting\n",
        "them up, right?\n",
        "\n",
        "In 'aggregator.py' we've set up exactly such a short-cut. This module simply contains the generator functions above \n",
        "such that the generator can be created by passing the aggregator. This provides us with convenience methods for quickly \n",
        "creating the masked dataset, model data and fits using single lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from howtofit.chapter_1_introduction.tutorial_8_aggregator.src.phase import aggregator\n",
        "\n",
        "masked_dataset_gen = aggregator.masked_dataset_generator_from_aggregator(\n",
        "    aggregator=agg_filter\n",
        ")\n",
        "model_data_gen = aggregator.model_data_generator_from_aggregator(aggregator=agg_filter)\n",
        "fit_gen = aggregator.fit_generator_from_aggregator(aggregator=agg_filter)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For your model-fitting project, you'll need to update the 'aggregator.py' module in the same way. This is why we have \n",
        "emphasised the object-oriented design of our model-fitting project through. This design makes it very easy to inspect \n",
        "results via the aggregator later on!"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}