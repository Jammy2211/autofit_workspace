{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Aggregator Part 1__\n",
        "\n",
        "After fitting a large suite of data with the same pipeline, the aggregator allows us to load the results and\n",
        "manipulate / plot them using a Python script or Jupyter notebook.\n",
        "\n",
        "To begin, we need a set of results that we want to analyse using the aggregator. We'll create this by fitting 3\n",
        "different data-sets. Each dataset is a single Gaussian and we'll fit them using a single Gaussian model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autoconf import conf\n",
        "import autofit as af\n",
        "\n",
        "from howtofit.chapter_1_introduction.tutorial_8_aggregator.src.model import profiles\n",
        "from howtofit.chapter_1_introduction.tutorial_8_aggregator.src.dataset import (\n",
        "    dataset as ds,\n",
        ")\n",
        "from howtofit.chapter_1_introduction.tutorial_8_aggregator.src.phase import phase as ph\n",
        "from howtofit.chapter_1_introduction.tutorial_8_aggregator.src.phase.settings import (\n",
        "    PhaseSettings,\n",
        ")\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You need to change the path below to the workspace directory so we can load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "workspace_path = \"/home/jammy/PycharmProjects/PyAuto/autofit_workspace\"\n",
        "chapter_path = f\"{workspace_path}/howtofit/chapter_1_introduction\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup the configs as we did in the previous tutorial, as well as the output folder for our non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conf.instance = conf.Config(\n",
        "    config_path=f\"{workspace_path}/config\",\n",
        "    output_path=f\"{workspace_path}/output/howtofit\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, for each dataset we are going to set up the correct path, load it, create its mask and fit it with the phase above.\n",
        "\n",
        "We want our results to be in a folder specific to the dataset. We'll use the dataset_name string to do this. Lets\n",
        "create a list of all 3 of our dataset names.\n",
        "\n",
        "We'll also pass these names to the dataset when we create it - the name will be used by the aggregator to name the file\n",
        "the data is stored. More importantly, the name will be accessible to the aggregator, and we will use it to label \n",
        "figures we make via the aggregator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_names = [\"gaussian_x1_0\", \"gaussian_x1_1\", \"gaussian_x1_2\"]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also attach information to the model-fit, by setting up an info dictionary. \n",
        "\n",
        "Information about our model-fit (e.g. the dataset) that isn't part of the model-fit is made accessible to the \n",
        "aggregator. For example, below we write info on the dataset's data of observation and exposure time.\n",
        "\"\"\"\n",
        "info = {\"date_of_observation\": \"01-02-18\", \"exposure_time\": 1000.0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This for loop runs over every dataset, checkout the comments below for how we set up the path structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset_name in dataset_names:\n",
        "\n",
        "    # The code below loads the dataset and creates the mask as per usual.\n",
        "\n",
        "    dataset_path = f\"{chapter_path}/dataset/{dataset_name}\"\n",
        "\n",
        "    dataset = ds.Dataset.from_fits(\n",
        "        data_path=f\"{dataset_path}//data.fits\",\n",
        "        noise_map_path=f\"{dataset_path}/noise_map.fits\",\n",
        "        name=dataset_name,\n",
        "    )\n",
        "\n",
        "    mask = np.full(fill_value=False, shape=dataset.data.shape)\n",
        "\n",
        "    # Here, we create a phase as normal. However, we also include an input parameter 'folders'. The phase folders\n",
        "    # define the names of folders that the phase goes in. For example, if a phase goes to the path:\n",
        "\n",
        "    # '/path/to/autofit_workspace/output/phase_name/'\n",
        "\n",
        "    # A phase folder with the input 'phase_folder' edits this path to:\n",
        "\n",
        "    # '/path/to/autofit_workspace/output/phase_folder/phase_name/'\n",
        "\n",
        "    # You can input multiple phase folders, for example 'folders=['folder_0', 'folder_1'] would create the path:\n",
        "\n",
        "    # '/path/to/autofit_workspace/output/folder_0/folder_1/phase_name/'\n",
        "\n",
        "    # Below, we use the data_name, so our results go in a folder specific to the dataset, e.g:\n",
        "\n",
        "    # '/path/to/autofit_workspace/output/gaussian_x1_0/phase_t8/'\n",
        "\n",
        "    print(\n",
        "        \"Emcee has begun running - checkout the autofit_workspace/howtofit/chapter_1_introduction/output/\"\n",
        "        + dataset_name\n",
        "        + \"/phase_t8\"\n",
        "        \"folder for live output of the results.\"\n",
        "        \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
        "    )\n",
        "\n",
        "    phase = ph.Phase(\n",
        "        phase_name=\"phase_t8\",\n",
        "        folders=[dataset_name],\n",
        "        profiles=af.CollectionPriorModel(gaussian=profiles.Gaussian),\n",
        "        settings=PhaseSettings(),\n",
        "        search=af.Emcee(),\n",
        "    )\n",
        "\n",
        "    # Note that we pass the info to the phase when we run it, so that the aggregator can make it accessible.\n",
        "\n",
        "    phase.run(dataset=dataset, mask=mask, info=info)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkout the output folder - you should see three new sets of results corresponding to our 3 Gaussian datasets.\n",
        "Unlike previous tutorials, these folders in the output folder are named after the dataset and contain the folder\n",
        "with the phase's name, as opposed to just the phase-name folder.\n",
        "\n",
        "To load these results with the aggregator, we simply point it to the path of the results we want it to inspect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "output_path = f\"{chapter_path}/output\"\n",
        "\n",
        "agg = af.Aggregator(directory=str(output_path))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin, let me quickly explain what a generator is in Python, for those unaware. A generator is an object that \n",
        "iterates over a function when it is called. The aggregator creates all objects as generators, rather than lists, or \n",
        "dictionaries, or whatever.\n",
        "\n",
        "Why? Because lists and dictionaries store every entry in memory simultaneously. If you fit many datasets, you'll have \n",
        "lots of results and therefore use a lot of memory. This will crash your laptop! On the other hand, a generator only \n",
        "stores the object in memory when it runs the function; it is free to overwrite it afterwards. Thus, your laptop won't \n",
        "crash!\n",
        "\n",
        "There are two things to bare in mind with generators:\n",
        "\n",
        "1) A generator has no length, thus to determine how many entries of data it corresponds to you first must turn it to a \n",
        "list.\n",
        "\n",
        "2) Once we use a generator, we cannot use it again - we'll need to remake it. For this reason, we typically avoid \n",
        "   storing the generator as a variable and instead use the aggregator to create them on use.\n",
        "\n",
        "We can now create a 'samples' generator of every fit. An instance of the Samples class acts as an \n",
        "interface between the results of the non-linear fit on your hard-disk and Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_gen = agg.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we print this list of outputs you should see over 3 different MCMCSamples instances. There are more than 3\n",
        "because the aggregator has loaded the results of previous tutorial as well as the 3 fits we performed above!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Emcee Samples:\\n\")\n",
        "print(samples_gen)\n",
        "print(\"Total Samples Objects = \", len(list(samples_gen)), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To remove the fits of previous tutorials and just keep the MCMCSamples of the 3 datasets fitted in this tutorial we can \n",
        "use the aggregator's filter tool. The phase name 'phase_t8' used in this tutorial is unique to all 3 fits, so we can \n",
        "use it to filter our results are desired."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "phase_name = \"phase_t8\"\n",
        "agg_filter = agg.filter(agg.phase == phase_name)\n",
        "samples_gen = agg_filter.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, we can filter using strings, requiring that the string appears in the full path of the output\n",
        "results. This is useful if you fit a large sample of data where:\n",
        "\n",
        "    - Multiple results, corresponding to different phases and model-fits are stored in the same path.\n",
        "    - Different runs using different _PhaseSettings_ are in the same path.\n",
        "    - Fits using different non-linear searches, with different settings, are contained in the same path.\n",
        "\n",
        "The example below shows us using the contains filter to get the results of the fit to only the first dataset. \n",
        "\"\"\"\n",
        "agg_filter_contains = agg.filter(\n",
        "    agg.directory.contains(\"phase_t8\"),\n",
        "    agg.directory.contains(\"gaussian_x1_0\"),\n",
        ")\n",
        "print(\"Directory Contains Filtered NestedSampler Samples: \\n\")\n",
        "print(\"Total Samples Objects = \", len(list(agg_filter_contains.values(\"samples\"))), \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've encountered the Samples class in previous tutorials, specifically the Result object returned from a phase. In \n",
        "this tutorial we'll inspect the Sampels class in more detail. The Samples class contains all the accepted parameter \n",
        "samples of the non-linear search, which is a list of lists where:\n",
        "\n",
        " - The outer list is the size of the total number of samples.\n",
        " - The inner list is the size of the number of free parameters in the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg_filter.values(\"samples\"):\n",
        "    print(\"All parameters of the very first sample\")\n",
        "    print(samples.parameters[0])\n",
        "    print(\"The tenth sample's third parameter\")\n",
        "    print(samples.parameters[9][2])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Samples class also contains the log likelihood, log prior, log posterior and weights of every accepted sample, \n",
        "where:\n",
        "\n",
        "   - The log likelihood is the value evaluated from the likelihood function (e.g. -0.5 * chi_squared + the noise \n",
        "     normalized).\n",
        "\n",
        "   - The log prior encodes information on how the priors on the parameters maps the log likelihood value to the log\n",
        "     posterior value.\n",
        "\n",
        "   - The log posterior is log_likelihood + log_prior.\n",
        "\n",
        "   - The weight gives information on how samples should be combined to estimate the posterior. The weight values \n",
        "     depend on the sampler used, for MCMC samples they are all 1 (e.g. all weighted equally)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg_filter.values(\"samples\"):\n",
        "    print(\"log(likelihood), log(prior), log(posterior) and weight of the tenth sample.\")\n",
        "    print(samples.log_likelihoods[9])\n",
        "    print(samples.log_priors[9])\n",
        "    print(samples.log_posteriors[9])\n",
        "    print(samples.weights[9])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, this list now has only 3 MCMCSamples, one for each dataset we fitted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Phase Name Filtered Emcee Samples:\\n\")\n",
        "print(samples_gen)\n",
        "print(\"Total Samples Objects = \", len(list(samples_gen)), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the samples to create a list of the maximum log likelihood model of each fit to our three images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vector = [samps.max_log_likelihood_vector for samps in agg_filter.values(\"samples\")]\n",
        "print(\"Maximum Log Likelihood Parameter Lists:\\n\")\n",
        "print(vector, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This provides us with lists of all model parameters. However, this isn't that much use - which values correspond\n",
        "to which parameters?\n",
        "\n",
        "Its more useful to create the maximum log likelihood model instance of every fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instances = [\n",
        "    samps.max_log_likelihood_instance for samps in agg_filter.values(\"samples\")\n",
        "]\n",
        "print(\"Maximum Log Likelihood Model Instances:\\n\")\n",
        "print(instances, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A model instance contains all the model components of our fit - which for the fits above was a single gaussian\n",
        "profile (the word 'gaussian' comes from what we called it in the CollectionPriorModel when making the phase above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(instances[0].profiles.gaussian)\n",
        "print(instances[1].profiles.gaussian)\n",
        "print(instances[2].profiles.gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This, of course, gives us access to any individual parameter of our maximum log likelihood model. Below, we see that t\n",
        "he 3 Gaussians were simulated using sigma values of 1.0, 5.0 and 10.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(instances[0].profiles.gaussian.sigma)\n",
        "print(instances[1].profiles.gaussian.sigma)\n",
        "print(instances[2].profiles.gaussian.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also access the 'most probable' model, which is the model computed by binning all of the accepted Emcee sample\n",
        "parameters into a histogram, after removing the initial samples where the non-linear sampler is 'burning in' to the \n",
        "high likelihood regions of parameter space. \n",
        "\n",
        "The median of each 1D histogram (1 for each parameter) is then used to give the median PDF model. This process is \n",
        "called 'marginalization' and the hisograms which provide information on the probability estimates of each parameter \n",
        "are called the 'Probability Density Function' or PDF for short."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mp_vectors = [samps.median_pdf_vector for samps in agg_filter.values(\"samples\")]\n",
        "mp_instances = [samps.median_pdf_instance for samps in agg_filter.values(\"samples\")]\n",
        "\n",
        "print(\"Most Probable Model Parameter Lists:\\n\")\n",
        "print(mp_vectors, \"\\n\")\n",
        "print(\"Most probable Model Instances:\\n\")\n",
        "print(mp_instances, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute the upper and lower errors on each parameter at a given sigma limit. These are computed via \n",
        "marginalization, whereby instead of using the median of the histogram (e.g. the parameter value at the 50% mark of the\n",
        "histogram) the values at a specified sigma interval are used. For 3 sigma, these confidence intervals are at 0.3% and\n",
        "99.7%.\n",
        "\n",
        "# Here, I use \"ue3\" to signify this is an upper error at 3 sigma confidence,, and \"le3\" for the lower error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ue3_vectors = [\n",
        "    out.error_vector_at_upper_sigma(sigma=3.0) for out in agg_filter.values(\"samples\")\n",
        "]\n",
        "ue3_instances = [\n",
        "    out.error_instance_at_upper_sigma(sigma=3.0) for out in agg_filter.values(\"samples\")\n",
        "]\n",
        "le3_vectors = [\n",
        "    out.error_vector_at_lower_sigma(sigma=3.0) for out in agg_filter.values(\"samples\")\n",
        "]\n",
        "le3_instances = [\n",
        "    out.error_instance_at_lower_sigma(sigma=3.0) for out in agg_filter.values(\"samples\")\n",
        "]\n",
        "\n",
        "print(\"Errors Lists:\\n\")\n",
        "print(ue3_vectors, \"\\n\")\n",
        "print(le3_vectors, \"\\n\")\n",
        "print(\"Errors Instances:\\n\")\n",
        "print(ue3_instances, \"\\n\")\n",
        "print(le3_instances, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also print the \"model_results\" of all phases, which is string that summarizes every fit's model providing\n",
        "quick inspection of all results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = agg_filter.model_results\n",
        "print(\"Model Results Summary:\\n\")\n",
        "print(results, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets end part 1 with something more ambitious. Lets create a plot of the inferred sigma values vs intensity of each\n",
        "Gaussian profile, including error bars at 3 sigma confidence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mp_instances = [samps.median_pdf_instance for samps in agg_filter.values(\"samples\")]\n",
        "ue3_instances = [\n",
        "    out.error_instance_at_upper_sigma(sigma=3.0) for out in agg_filter.values(\"samples\")\n",
        "]\n",
        "le3_instances = [\n",
        "    out.error_instance_at_lower_sigma(sigma=3.0) for out in agg_filter.values(\"samples\")\n",
        "]\n",
        "\n",
        "mp_sigmas = [instance.profiles.gaussian.sigma for instance in mp_instances]\n",
        "ue3_sigmas = [instance.profiles.gaussian.sigma for instance in ue3_instances]\n",
        "le3_sigmas = [instance.profiles.gaussian.sigma for instance in le3_instances]\n",
        "mp_intensitys = [instance.profiles.gaussian.sigma for instance in mp_instances]\n",
        "ue3_intensitys = [instance.profiles.gaussian.sigma for instance in ue3_instances]\n",
        "le3_intensitys = [instance.profiles.gaussian.intensity for instance in le3_instances]\n",
        "\n",
        "plt.errorbar(\n",
        "    x=mp_sigmas,\n",
        "    y=mp_intensitys,\n",
        "    marker=\".\",\n",
        "    linestyle=\"\",\n",
        "    xerr=[le3_sigmas, ue3_sigmas],\n",
        "    yerr=[le3_intensitys, ue3_intensitys],\n",
        ")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}