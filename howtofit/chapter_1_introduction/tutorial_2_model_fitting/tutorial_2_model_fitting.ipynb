{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Fitting__\n",
    "\n",
    "In this tutorial, we'll fit the Gaussian model from the previous tutorial to the data we loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autofit as af\n",
    "import autoarray as aa\n",
    "import autoarray.plot as aplt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, lets load the dataset again.\n",
    "\n",
    "You need to change the path below to the chapter 1 directory so we can load the dataset#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_path = \"/home/jammy/PycharmProjects/PyAuto/autofit_workspace/howtofit/chapter_1_introduction/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These setup the configs as we did in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af.conf.instance = af.conf.Config(config_path=chapter_path + \"/config\")\n",
    "\n",
    "dataset_path = chapter_path + \"dataset/gaussian_x1/\"\n",
    "\n",
    "imaging = aa.imaging.from_fits(\n",
    "    image_path=dataset_path + \"image.fits\",\n",
    "    noise_map_path=dataset_path + \"noise_map.fits\",\n",
    "    psf_path=dataset_path + \"psf.fits\",\n",
    "    pixel_scales=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imaging subplot shows the imaging dataset's image and noise-map, which characterises the noise in every pixel of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aplt.imaging.subplot_imaging(imaging=imaging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we actually go about fitting our Gaussian model to this data? First, we need to be able to generate an image of our 2D Gaussian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from howtofit.chapter_1_introduction.tutorial_2_model_fitting import gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout the file:\n",
    "\n",
    "'autofit_workspace/howtofit/chapter_1_introduction/tutorial_2_model_fitting/gaussians.py'.\n",
    "\n",
    "Here, we've extended the Gaussian class to have a method \"image_from_grid\". Given an input grid of (y,x) coordinates this computes the intensity of the Gaussian at every point on that grid.\n",
    "\n",
    "We can use PyAutoArray to create such a grid, which we'll make the same dimensions as our data above.\n",
    "\n",
    "The \"pixel-scales\" define the conversion between pixels (which range from values of 0 to 100) and Gaussian coordinates (which define the length dimensions of its centre and sigma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = aa.grid.uniform(shape_2d=imaging.shape_2d, pixel_scales=imaging.pixel_scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grid is a uniform 2D array of coordinates in length units of our Gaussian profile.\n",
    "\n",
    "We can print each coordinate of this grid, revealing that it consists of coordinates where the spacing between each coordinate corresponds to the 'pixel_scale' of 1.0 we defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(y,x) pixel 0:\")\n",
    "print(grid.in_2d[0, 0])\n",
    "print(\"(y,x) pixel 1:\")\n",
    "print(grid.in_2d[0, 1])\n",
    "print(\"(y,x) pixel 2:\")\n",
    "print(grid.in_2d[0, 2])\n",
    "print(\"(y,x) pixel 100:\")\n",
    "print(grid.in_2d[1, 0])\n",
    "print(\"etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grids in PyAutoArray are stored as both 1D and 2D NumPy arrays, because different calculations benefit from us using the array in different formats. We can access both the 1D and 2D arrays automatically by specifying the input as a 1D or 2D NumPy index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(y,x) pixel 0 (accessed in 2D):\")\n",
    "print(grid.in_2d[0, 0])\n",
    "print(\"(y,x) pixel 0 (accessed in 1D):\")\n",
    "print(grid.in_1d[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the grid is also available in 1D and 2D, consisting of 625 (25 x 25) coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.shape_2d)\n",
    "print(grid.shape_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the entire grid in either 1D or 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.in_2d)\n",
    "print(grid.in_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass this grid to an instance of the Gaussian class, we can create an image of the gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = af.PriorModel(gaussians.Gaussian)\n",
    "\n",
    "gaussian = model.instance_from_vector(vector=[0.0, 0.0, 1.0, 1.0])\n",
    "\n",
    "model_image = gaussian.image_from_grid(grid=grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the grid, the arrays PyAutoArray computes are accessible in both 2D and 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_image.shape_2d)\n",
    "print(model_image.shape_1d)\n",
    "print(model_image.in_2d[0, 0])\n",
    "print(model_image.in_1d[0])\n",
    "print(model_image.in_2d)\n",
    "print(model_image.in_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyAutoArray has the tools we need to visualize the Gaussian's image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aplt.array(array=model_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different values of centre, intensity and sigma change the Gaussians apperance - have a go at editing some of the values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = model.instance_from_vector(vector=[1.0, 2.0, 3.0, 4.0])\n",
    "model_image = gaussian.image_from_grid(grid=grid)\n",
    "aplt.array(array=model_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so lets recap. We've defined a model which is a 2D Gaussian and given a set of parameters for that model (y, x, I, sigma) we can create a 'model_image' of the Gaussian. And, we have some data of a Gaussian we want to fit this model with. So how do we do that?\n",
    "\n",
    "Simple, we take the image from our data and our model_image of the Gaussian and subtract the two to get a 'residual-map'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_map = imaging.image - model_image\n",
    "aplt.array(array=residual_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this model isn't a good fit to the data - which was to be expected as they looked nothing alike!\n",
    "\n",
    "Next, we want to quantify how good (or bad) the fit actually was, via some goodness-of-fit measure. This measure needs to account for noise in the data - after all if we fit a pixel badly simply because it was very noisy we want our goodness-of-fit to account for that.\n",
    "\n",
    "To account for noise, we take our residual-map and divide it by the noise-map, to get the 'normalized residual-map'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_residual_map = residual_map / imaging.noise_map\n",
    "aplt.array(array=normalized_residual_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting close to a goodness-of-fit measure, but there is still a problem - we have negative and positive values in the normalized residual map. A value of -0.2 represents just as good of a fit as a value of 0.2, so we want them to both be the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we next define a 'chi-squared map', which is the normalized residual-map squared. This makes negative and positive values both positive and thus defined on a common overall scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared_map = (normalized_residual_map) ** 2\n",
    "aplt.array(array=chi_squared_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, even when looking at a chi-squared map its clear that our model gives a rubbish fit to the data.\n",
    "\n",
    "Finally, we want to reduce all the information in our chi-squared map into a single goodness-of-fit measure. To do this we define the 'chi-squared', which is the sum of all values on the chi-squared map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared = np.sum(chi_squared_map)\n",
    "print(\"Chi-squared = \", chi_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the lower our chi-squared, the fewer residuals in the fit between our model and the data and therefore the better our fit!\n",
    "\n",
    "From the chi-squared we can then define our final goodness-of-fit measure, the 'likelihood', which is the chi-squared value times -0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = -0.5 * chi_squared\n",
    "print(\"Likelihood = \", likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the likelihood the chi-squared times -0.5? Lets not worry about. This is simply the standard definition of a likelihood in statistics (it relates to the noise-properties of our data-set). For now, just accept that this is what a likelihood is and if we want to fit a model to data our goal is to thus find the combination of model parameters that maximizes our likelihood.\n",
    "\n",
    "There is a second quantity that enters the likelihood, called the 'noise-normalization'. This is the log sum of all noise values squared in our data-set (give the noise-map doesn't change the noise_normalization is the same value for all models that we fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_normalization = np.sum(np.log(2 * np.pi * imaging.noise_map ** 2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, like the definition of a likelihood, lets not worry about why a noise normalization is defined in this way or why its in our goodness-of-fit. Lets just accept for now that this is how it is in statistics.\n",
    "\n",
    "Thus, we now have the definition of a likelihood that we'll use hereafter in all PyAutoFit tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = -0.5 * chi_squared + noise_normalization\n",
    "print(\"Likelihood = \", likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with model-fitting, you'll have probably heard of terms like 'residuals', 'chi-squared' and 'likelihood' before. These are the standard metrics by which a model-fit's quality is measured. They are used for model fitting in general, so not just when your data is an image but when its 1D data (e.g a line), 3D data (e.g. a datacube) or something else entirely!\n",
    "\n",
    "If you haven't performed model fitting before and these terms are new to you, make sure you are clear on exactly what they all mean as they are at the core of all model-fitting performed in PyAutoFit!\n",
    "\n",
    "Before fitting data, we may want mask it, perhaps masking out regions of the data where we know it is defective or removing regions of an image where there is no signal to save on computational run time.\n",
    "\n",
    "For our Gaussian data there is no need to apply a mask, but PyAutoArray still requires that a mask is specified, so we'll create a mask which is \"unmasked\" - that is it consists entirely of False entries signifying every data-point in our image to perform a fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = aa.mask.unmasked(shape_2d=imaging.shape_2d, pixel_scales=imaging.pixel_scales)\n",
    "masked_imaging = aa.masked.imaging(imaging=imaging, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was a lot of code performing the fits above and creating our residuals, chi-squareds and likelihoods. From here on we'll use PyAutoArray's built-in fitting tools which fit the imaging data in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = aa.fit(masked_dataset=masked_imaging, model_data=model_image)\n",
    "\n",
    "print(\"Fit: \\n\")\n",
    "print(fit)\n",
    "print(\"Model-Image:\\n\")\n",
    "print(fit.model_image.in_2d)\n",
    "print(fit.model_image.in_1d)\n",
    "print()\n",
    "print(\"Residual Maps:\\n\")\n",
    "print(fit.residual_map.in_2d)\n",
    "print(fit.residual_map.in_1d)\n",
    "print()\n",
    "print(\"Chi-Squareds Maps:\\n\")\n",
    "print(fit.chi_squared_map.in_2d)\n",
    "print(fit.chi_squared_map.in_1d)\n",
    "print(\"Likelihood:\")\n",
    "print(fit.likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, PyAutoArray also provides the tools we need to visualize a fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aplt.fit_imaging.subplot_fit_imaging(fit=fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to recap the previous tutorial and this one:\n",
    "\n",
    "- We can define a model components in PyAutoFit, like our Gaussian, using Python classes that follow a certain format.\n",
    "- The model component's parameters each have priors, which given a unit vecto can be mapped to an instance of the Gaussian class.\n",
    "- We can use this model-instance to create a model-image of our Gaussian and compare it to data and quantify the goodness-of-fit via a likelihood.\n",
    "\n",
    "Thus we have everything we need to fit our model to our data! So, how do we go about finding the best-fit model? That is, the model which maximizes the likelihood.\n",
    "\n",
    "The most simple thing we can do is guess parameters, and when we guess parameters that give a good fit, guess another set of parameters near those values. We can then repeat this process, over and over, until we find a really good model!\n",
    "\n",
    "For our Gaussian this works pretty well, below I've fitted 5 diferent Gaussian models and ended up landing on the best-fit model (the model I used to create the dataset in the first place!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = model.instance_from_vector(vector=[0.0, 0.5, 3.0, 3.0])\n",
    "model_image = gaussian.image_from_grid(grid=grid)\n",
    "fit = aa.fit(masked_dataset=masked_imaging, model_data=model_image)\n",
    "aplt.fit_imaging.subplot_fit_imaging(fit=fit)\n",
    "print(\"Likelihood:\")\n",
    "print(fit.likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = model.instance_from_vector(vector=[0.0, 0.0, 3.0, 3.0])\n",
    "model_image = gaussian.image_from_grid(grid=grid)\n",
    "fit = aa.fit(masked_dataset=masked_imaging, model_data=model_image)\n",
    "aplt.fit_imaging.subplot_fit_imaging(fit=fit)\n",
    "print(\"Likelihood:\")\n",
    "print(fit.likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = model.instance_from_vector(vector=[0.0, 0.0, 10.0, 3.0])\n",
    "model_image = gaussian.image_from_grid(grid=grid)\n",
    "fit = aa.fit(masked_dataset=masked_imaging, model_data=model_image)\n",
    "aplt.fit_imaging.subplot_fit_imaging(fit=fit)\n",
    "print(\"Likelihood:\")\n",
    "print(fit.likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = model.instance_from_vector(vector=[0.0, 0.0, 10.0, 1.0])\n",
    "model_image = gaussian.image_from_grid(grid=grid)\n",
    "fit = aa.fit(masked_dataset=masked_imaging, model_data=model_image)\n",
    "aplt.fit_imaging.subplot_fit_imaging(fit=fit)\n",
    "print(\"Likelihood:\")\n",
    "print(fit.likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = model.instance_from_vector(vector=[0.0, 0.0, 10.0, 0.5])\n",
    "model_image = gaussian.image_from_grid(grid=grid)\n",
    "fit = aa.fit(masked_dataset=masked_imaging, model_data=model_image)\n",
    "aplt.fit_imaging.subplot_fit_imaging(fit=fit)\n",
    "print(\"Likelihood:\")\n",
    "print(fit.likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now perform model-fitting with PyAutoFit! All we have to do is guess lots of parameters, over and over and over again, until we hit a model with a high likelihood. Yay!\n",
    "\n",
    "Of course, you're probably thinking, is that really it? Should we really be guessing models to find the best-fit?\n",
    "\n",
    "Obviously, the answer is no. Imagine our model was more complex, that it had many more parameters than just 4. Our approach of guessing parameters won't work - it could take days, maybe years, to find models with a high likelihood, and how could you even be sure they ware the best-fit models? Maybe a set of parameters you never tried provide an even better fit?\n",
    "\n",
    "Of course, there is a much better way to perform model-fitting, and in the next tutorial we'll take you through how to do such fitting in PyAutoFit, using whats called a 'non-linear search'.\n",
    "\n",
    "To end, its worth quickly thinking about the model you ultimately want to fit with PyAutoFit. In this example, we extended the Gaussian class to contain the function we needed to generate an image of the Gaussian and thus generate the model-image we need to fit our data. For your model fitting problem can you do something similar? Or is your model-fitting task a bit more complicated than this? Maybe there are more model component you want to combine or there is an inter-dependency between models?\n",
    "\n",
    "PyAutoFit provides a lot of flexibility in how you ultimate use your model instances, so whatever your problem you should find that it is straight forward to find a solution. But, whatever you need to do at its core your modeling problem will break down into the tasks we did in this tutorial:\n",
    "\n",
    "1) Use your model to create some model data.\n",
    "2) Subtract it from the data to create residuals.\n",
    "3) Use these residuals in conjunction with your noise-map to define a likelihood.\n",
    "4) Find the highest likelihood models.\n",
    "\n",
    "So, get thinking about how these steps woould be performed for your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
