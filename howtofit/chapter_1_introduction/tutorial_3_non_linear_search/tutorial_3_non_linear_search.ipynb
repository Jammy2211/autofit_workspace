{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#%%\n",
        "\"\"\"\n",
        "__Non-linear Search__\n",
        "\n",
        "Okay, so its finally time to take our model and fit it to data, hurrah!\n",
        "\n",
        "So, how do we infer the parameters for the 1D Gaussian that give a good fit to our data?  In the last tutorial, we\n",
        "tried a very basic approach, randomly guessing models until we found one that gave a good fit and high log_likelihood.\n",
        "\n",
        "We discussed that this wasn't really a viable strategy for more complex models, and it isn't. However, this is the\n",
        "basis of how model fitting actually works! Basically, our model-fitting algorithm guesses lots of models, tracking\n",
        "the log likelihood of these models. As the algorithm progresses, it begins to guess more models using parameter\n",
        "combinations that gave higher log_likelihood solutions previously. If a set of parameters provided a good fit to the\n",
        "data previously, a model with similar values probably will too.\n",
        "\n",
        "This is called a 'non-linear search' and its a fairly common problem faced by scientists. We're going to use a\n",
        "non-linear search algorithm called 'Emcee', which for those familiar with statistic inference is a Markov Chain Monte\n",
        "Carlo (MCMC) method. For now, lets not worry about the details of how Emcee actually works. Instead, just picture that\n",
        "a non-linear search in PyAutoFit operates as follows:\n",
        "\n",
        "    1) Randomly guess a model, mapping their parameters via the priors to instances of the model, in this case a\n",
        "       Gaussian.\n",
        "\n",
        "    2) Use this model instance to generate model data and compare this model data to the data to compute a log\n",
        "       likelihood.\n",
        "\n",
        "    3) Repeat this many times, choosing models whose parameter values are near those of the model which currently has\n",
        "       the highest log likelihood value. If the new model's log likelihood is higher than the previous model, new\n",
        "       models will thus be chosen with parameters nearer this model.\n",
        "\n",
        "The idea is that if we keep guessing models with higher log-likelihood values, we'll inevitably 'climb' up the gradient\n",
        "of the log likelihood in parameter space until we eventually hit the highest log likelihood models.\n",
        "\n",
        "To be clear, this overly simplfied description of an MCMC algorithm is not how the *non-linear search* really works and\n",
        "omits the details of how our priors inpact our inference or why an MCMC algorithm can provide reliable errors on our\n",
        "parameter estimates.\n",
        "\n",
        "A detailed understanding of *non-linear sampling* is not required in chapter 1, however interested users will find\n",
        "a complete description of *non-linear searches* in chapter 2 of the HowToFit lectures.\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autoconf import conf\n",
        "import autofit as af\n",
        "import matplotlib.pyplot as plt\n",
        "from astropy.io import fits\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You need to change the path below to the workspace directory so we can load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "workspace_path = \"/home/jammy/PycharmProjects/PyAuto/autofit_workspace\"\n",
        "chapter_path = f\"{workspace_path}/howtofit/chapter_1_introduction\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The line conf.instance is now used to set up a second property of the configuration:\n",
        "\n",
        "    - The path to the PyAutoFit output folder, which is where the results of the non-linear search are written to \n",
        "      on your hard-disk, alongside visualization and other properties of the fit \n",
        "      (e.g. '/path/to/autolens_workspace/output/howtolens')\n",
        "\n",
        "(These will work autommatically if the WORKSPACE environment variable was set up correctly during installation. \n",
        "Nevertheless, setting the paths explicitly within the code is good practise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conf.instance = conf.Config(\n",
        "    config_path=f\"{workspace_path}/config\",\n",
        "    output_path=f\"{workspace_path}/output/howtofit\",  # <- This sets up where the non-linear search's outputs go.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets load the data and noise-map we'll use for our fits, which is the same data we used in tutorial 2.\n",
        "\"\"\"\n",
        "dataset_path = f\"{chapter_path}/dataset/gaussian_x1/\"\n",
        "\n",
        "data_hdu_list = fits.open(f\"{dataset_path}/data.fits\")\n",
        "data = np.array(data_hdu_list[0].data)\n",
        "\n",
        "noise_map_hdu_list = fits.open(f\"{dataset_path}/noise_map.fits\")\n",
        "noise_map = np.array(noise_map_hdu_list[0].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets remind ourselves what the data looks like, using the plot_lint convenience method fom the previous tutorial.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def plot_line(xvalues, line, ylabel=None):\n",
        "\n",
        "    plt.plot(xvalues, line)\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "xvalues = np.arange(data.shape[0])\n",
        "\n",
        "plot_line(xvalues=xvalues, line=data, ylabel=\"Data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets remake the Gaussian class for this tutorial, which is the model we will fit using the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Gaussian:\n",
        "    def __init__(\n",
        "        self,\n",
        "        centre=0.0,  # <- PyAutoFit recognises these constructor arguments\n",
        "        intensity=0.1,  # <- are the Gaussian's model parameters.\n",
        "        sigma=0.01,\n",
        "    ):\n",
        "        self.centre = centre\n",
        "        self.intensity = intensity\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def line_from_xvalues(self, xvalues):\n",
        "        \"\"\"\n",
        "        Calculate the intensity of the light profile on a line of Cartesian x coordinates.\n",
        "\n",
        "        The input xvalues are translated to a coordinate system centred on the Gaussian, using its centre.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xvalues : ndarray\n",
        "            The x coordinates in the original reference frame of the data.\n",
        "        \"\"\"\n",
        "        transformed_xvalues = np.subtract(xvalues, self.centre)\n",
        "        return np.multiply(\n",
        "            np.divide(self.intensity, self.sigma * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_xvalues, self.sigma))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The non-linear search requires an *Analysis* class, which:\n",
        "\n",
        "    - Receives the data to be fitted and prepares it so the model can fit it.\n",
        "    - Defines the 'log_likelihood_function' used to compute the log likelihood from a model instance. \n",
        "    - Passes this log likelihood to the non-linear search so that it can determine parameter values for the the next model \n",
        "      that it samples.\n",
        "\n",
        "For our 1D Gaussian model-fitting example, here is our *Analysis* class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data, noise_map):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "\n",
        "    def log_likelihood_function(self, instance):\n",
        "\n",
        "        # The 'instance' that comes into this method is an instance of the Gaussian class above, with the parameters\n",
        "        # set to values chosen by the non-linear search. (These are commented out to pretent excessive print statements\n",
        "        # when we run the non-linear search).\n",
        "\n",
        "        # This instance's parameter values are chosen by the non-linear search based on the previous model with the\n",
        "        # highest likelihood result.\n",
        "\n",
        "        # print(\"Gaussian Instance:\")\n",
        "        # print(\"Centre = \", instance.centre)\n",
        "        # print(\"Intensity = \", instance.intensity)\n",
        "        # print(\"Sigma = \", instance.sigma)\n",
        "\n",
        "        # Below, we fit the data with the Gaussian instance, using its \"line_from_xvalues\" function to create the\n",
        "        # model data.\n",
        "\n",
        "        xvalues = np.arange(self.data.shape[0])\n",
        "\n",
        "        model_data = instance.line_from_xvalues(xvalues=xvalues)\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * noise_map ** 2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform the non-linear search using Emcee, we simply compose our model using a PriorModel, instantiate the Analysis\n",
        "class and pass them to an instance of the Emcee class.\n",
        "\n",
        "We manually set the priors on the model, in the next tutorial we'll cover how this can be performed automatically.\n",
        "\"\"\"\n",
        "\n",
        "model = af.PriorModel(Gaussian)\n",
        "model.centre = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.intensity = af.UniformPrior(lower_limit=0.0, upper_limit=1e2)\n",
        "model.sigma = af.UniformPrior(lower_limit=0.0, upper_limit=50.0)\n",
        "\n",
        "analysis = Analysis(data=data, noise_map=noise_map)\n",
        "\n",
        "emcee = af.Emcee()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We begin the non-linear search by calling its fit method, it  will take a minute or so to run (which is very fast for a \n",
        "model-fit). Whilst you're waiting, checkout the folder:\n",
        "\n",
        "'autofit_workspace/howtofit/chapter_1_introduction/output/emcee'\n",
        "\n",
        "Here, the results of the model-fit are output to your hard-disk (on-the-fly) and you can inspect them as the non-linear\n",
        "search runs. In particular, you'll fild:\n",
        "\n",
        "    - model.info: A file listing every model component, parameter and prior in your model-fit.\n",
        "    - model.results: A file giving the latest best-fit model, parameter estimates and errors of the fit.\n",
        "    - search: A folder containing the Emcee output in hdf5 format.txt (you'll probably never need to look at these, but\n",
        "              its good to know what they are).\n",
        "    - Other metadata which you can ignore for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = emcee.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running - checkout the autofit_workspace/howtofit/chapter_1_introduction/output/phase_t3\"\n",
        "    \"folder for live output of the results.\"\n",
        "    \"This Jupyter notebook cell with progress once Emcee has completed - this could take a few minutes!\"\n",
        ")\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once completed, the non-linear search returns a Result object, which contains lots of information about the non-linear. \n",
        "A full description of the *Results* object can be found at:\n",
        " \n",
        "'autofit_workspace/examples/simple/results'\n",
        "'autofit_workspace/examples/complex/results'. \n",
        "\n",
        "In this tutorial, lets use it to inspect the maximum likelihood model instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Likelihood Model:\\n\")\n",
        "max_log_likelihood_instance = result.samples.max_log_likelihood_instance\n",
        "print(\"Centre = \", max_log_likelihood_instance.centre)\n",
        "print(\"Intensity = \", max_log_likelihood_instance.intensity)\n",
        "print(\"Sigma = \", max_log_likelihood_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this to plot the maximum log likelihood fit over the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_data = result.max_log_likelihood_instance.line_from_xvalues(\n",
        "    xvalues=np.arange(data.shape[0])\n",
        ")\n",
        "plt.plot(range(data.shape[0]), data)\n",
        "plt.plot(range(data.shape[0]), model_data)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Intensity\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we used the results 'Samples' property, which in this case is a MCMCSamples object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This object acts as an interface between the Emcee output results on your hard-disk and this Python code. For\n",
        "example, we can use it to get the parameters and log likelihood of every accepted emcee sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.samples.parameters)\n",
        "print(result.samples.log_likelihoods)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also use it to get a model instance of the \"most probable\" model, which is the model where each parameter is\n",
        "the value estimated from the probability distribution of parameter space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mp_instance = result.samples.median_pdf_instance\n",
        "print()\n",
        "print(\"Most Probable Model:\\n\")\n",
        "print(\"Centre = \", mp_instance.centre)\n",
        "print(\"Intensity = \", mp_instance.intensity)\n",
        "print(\"Sigma = \", mp_instance.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll come back to look at Samples objects in more detail tutorial 8!"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}